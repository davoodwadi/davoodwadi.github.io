{
  "hash": "79d222581118995c2f2028efadafe524",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\n\n### Linear least squares\n\nLinear least squares is a technique used for regression problems, where we aim to predict continuous numerical values. However, it can also be used for classification tasks by transforming the problem into a binary classification problem.\n\nIn linear least squares for classification, we use a linear model to classify data into two classes. We assign class labels of -1 and 1 to the two classes. The goal is to find a linear boundary that best separates the two classes, minimizing the sum of squared distances between the data points and the decision boundary.\n\nLet's see how we can do this in Python:\n\nFirst, we need to import the required libraries: numpy and matplotlib.\n\n::: {#cbd790de .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nNext, let's generate some synthetic data with two classes. We will use the `make_classification` function from the `sklearn.datasets` module to create a random dataset.\n\n::: {#2b17f3cc .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\n```\n:::\n\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\n\nNow, let's visualize the data using a scatter plot:\n\n::: {#622fda3d .cell execution_count=3}\n``` {.python .cell-code}\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_classification_files/figure-html/cell-4-output-1.png){width=592 height=429}\n:::\n:::\n\n\nWe have plotted the data points for each class on a scatter plot.\n\nTo apply linear least squares for classification, we need to add a column of ones to our feature matrix X to incorporate the bias term in the linear equation.\n\n::: {#76eaa673 .cell execution_count=4}\n``` {.python .cell-code}\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n```\n:::\n\n\nNow, let's define our linear model and solve for the optimal parameters using linear least squares.\n\n::: {#9e308d58 .cell execution_count=5}\n``` {.python .cell-code}\ntheta = np.linalg.inv(X.T @ X) @ X.T @ y\n```\n:::\n\n\nHere, `theta` is the vector of parameters that defines our linear model. The equation used to solve for `theta` is:\n\n\n$\\theta = (X^T X)^{-1} X^T y$\n\nFinally, let's visualize the decision boundary of our linear model along with the data points.\n\n::: {#a1647a1f .cell execution_count=6}\n``` {.python .cell-code}\nplt.scatter(X[y == 1][:,1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0][:,1], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\n\n# Plotting the decision boundary\nx_boundary = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n# x2_boundary = -(theta[0] + theta[1]*x1_boundary) / theta[2]\ny_boundary = x_boundary * theta[1] +  theta[0]\nplt.plot(x_boundary, y_boundary, color='black', linewidth=2)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Linear_classification_files/figure-html/cell-7-output-1.png){width=604 height=429}\n:::\n:::\n\n\nWe have plotted the decision boundary determined by our linear model, which separates the two classes.\n\nLinear least squares for classification is a simple technique for linearly separable datasets. Note that this approach assumes the data points are linearly separable and does not work well for nonlinear classification problems.\n\n",
    "supporting": [
      "Linear_classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}