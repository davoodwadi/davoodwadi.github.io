{
  "hash": "fe3ae93ddcef2883b11e3dc373e5baf5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\n\n\nTo demonstrate the implementation of Multilayer Perceptron (MLP) using Pytorch, we will use the classic Iris dataset. MLP is a type of artificial neural network that is widely used in machine learning for classification tasks. In this tutorial, we will build a simple MLP model to classify the Iris flowers into different species.\n\nBut first, let's import the necessary libraries:\n\n::: {#1e7ab46a .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/var/folders/jv/ppbxly7j7vzgcr8sdv78s2hr0000gn/T/ipykernel_51247/678647940.py\", line 1, in <module>\n    import torch\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n    from .functional import *  # noqa: F403\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n    import torch.nn.functional as F\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n    from .modules import *  # noqa: F403\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning:\n\nFailed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n\n```\n:::\n:::\n\n\nNow let's load the Iris dataset and split it into training and testing sets:\n\n::: {#23a30b3c .cell execution_count=2}\n``` {.python .cell-code}\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\nIn the code above, we first import the required libraries: `torch` for building and training the MLP model, `torch.nn` for defining the network architecture, `torch.optim` for optimizing the model parameters, `sklearn.datasets` for loading the Iris dataset, and `sklearn.model_selection` for splitting the dataset into training and testing sets.\n\nNext, we load the Iris dataset using `load_iris()` function and assign the feature data to `X` and the target labels to `y`. Then, we split the dataset into training and testing sets using `train_test_split()` function, where we specify the test size as 0.2 (20% of the data) and set the random state for reproducibility.\n\nNow, let's define our MLP model:\n\n::: {#2a86634d .cell execution_count=3}\n``` {.python .cell-code}\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n```\n:::\n\n\nIn the code above, we define a class `MLP` inheriting from `nn.Module`. The constructor (`__init__()`) takes three arguments: `input_size` which is the number of input features, `hidden_size` which is the number of neurons in the hidden layer, and `num_classes` which is the number of output classes. Inside the constructor, we define the layers of our MLP model using `nn.Linear` (fully connected) and `nn.ReLU` (rectified linear unit) activation function.\n\nThe `forward()` method defines the forward pass of the model. It takes an input `x`, passes it through the layers, and returns the output.\n\nNow let's create an instance of our MLP model:\n\n::: {#f31d4544 .cell execution_count=4}\n``` {.python .cell-code}\ninput_size = X.shape[1]\nhidden_size = 64\nnum_classes = len(set(y))\n\nmodel = MLP(input_size, hidden_size, num_classes)\n```\n:::\n\n\nIn the code above, we set `input_size` as the number of features in the input data (`X.shape[1]`), `hidden_size` as 64 (you can change this value to experiment), and `num_classes` as the number of unique classes in the target labels (`len(set(y))`).\n\nNow let's define the loss function and optimizer:\n\n::: {#c7b88327 .cell execution_count=5}\n``` {.python .cell-code}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n:::\n\n\nWe use `nn.CrossEntropyLoss()` as our loss function, which is suitable for multi-class classification problems. We also define the optimizer using `optim.Adam` and pass the model parameters (`model.parameters()`) along with the learning rate (`lr=0.001`).\n\nNext, let's train the MLP model:\n\n::: {#8ae6f80a .cell execution_count=6}\n``` {.python .cell-code}\nnum_epochs = 100\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    for i in range(0, X_train.shape[0], batch_size):\n        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n        batch_y = torch.LongTensor(y_train[i:i+batch_size])\n        \n        # Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [10/100], Loss: 0.4976\nEpoch [20/100], Loss: 0.3189\nEpoch [30/100], Loss: 0.2236\nEpoch [40/100], Loss: 0.1617\nEpoch [50/100], Loss: 0.1213\nEpoch [60/100], Loss: 0.0943\nEpoch [70/100], Loss: 0.0759\nEpoch [80/100], Loss: 0.0628\nEpoch [90/100], Loss: 0.0534\nEpoch [100/100], Loss: 0.0463\n```\n:::\n:::\n\n\nIn the code above, we define the number of epochs (`num_epochs`) and the batch size (`batch_size`) for training. We iterate over each epoch and split the training data into batches. For each batch, we convert the numpy arrays to tensors using `torch.FloatTensor` and `torch.LongTensor` for the input features and target labels respectively.\n\nIn the forward pass, we pass the input batch (`batch_X`) through the model and compute the predicted outputs. We then calculate the loss between the predicted outputs and the target labels using the defined loss function.\n\nIn the backward pass, we zero the gradients with `optimizer.zero_grad()`, compute the gradients of the loss with respect to the model parameters using `loss.backward()`, and update the model parameters using the optimizer with `optimizer.step()`.\n\nFinally, we print the loss value every 10 epochs for tracking the training progress.\n\nAfter training, let's evaluate the MLP model on the test set:\n\n::: {#66d33e29 .cell execution_count=7}\n``` {.python .cell-code}\nwith torch.no_grad():\n    outputs = model(torch.FloatTensor(X_test))\n    _, predicted = torch.max(outputs.data, 1)\n\naccuracy = (predicted == torch.LongTensor(y_test)).sum().item() / len(y_test)\nprint(f'Accuracy: {accuracy:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0000\n```\n:::\n:::\n\n\nIn the code above, we use `torch.no_grad()` to turn off gradient calculation and save memory. We pass the test features (`X_test`) through the trained model and obtain the predicted outputs. We then use `torch.max()` to get the class labels with the highest probability and compare them with the ground truth labels (`y_test`) to calculate the accuracy.\n\nThat's it! You have successfully implemented an MLP model using Pytorch for classification on the Iris dataset.\n\n",
    "supporting": [
      "MLP_pytorch_files"
    ],
    "filters": [],
    "includes": {}
  }
}