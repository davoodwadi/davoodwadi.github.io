{
  "hash": "29595f70c821ee97f1976012bba61052",
  "result": {
    "markdown": "---\ntitle: L2 Regularization for Multiple Linear Regression with Design Matrix X\nformat:\n  html:\n    code-fold: false\nexecute:\n  enabled: true\n---\n\n\n\n\n\nIn multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\n\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\n\nwhere:\n\n- $y$ is the dependent variable (target variable)\n- $X$ is the design matrix that consists of independent variables (features)\n- $\\beta$ is the vector of coefficients (slopes) for each feature in X\n- $\\epsilon$ is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients $\\beta$ that minimize the sum of squared residuals.\n\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\n\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter $\\lambda$.\n\nThe loss function for multiple linear regression with L2 regularization is given by:\n\n\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\n\nwhere:\n\n- $N$ is the number of samples in the dataset\n- $y_i$ is the target value for the i-th sample\n- $X_i$ is the feature vector for the i-th sample\n- $\\|\\beta\\|^2$ represents the squared L2 norm of the coefficient vector $\\beta$\n\nThe goal is to find the value of $\\beta$ that minimizes this loss function.\n\nLet's now implement L2 regularization for multiple linear regression using Python:\n\n#### Step 1: Importing the Required Libraries\n\nWe will start by importing the necessary libraries for our implementation.\n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n```\n:::\n\n\n#### Step 2: Generating Sample Data\n\nTo demonstrate the L2 regularization for multiple linear regression, we will generate some sample data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n```\n:::\n\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n#### Step 3: Fitting the Multiple Linear Regression Model\n\nWe will fit the multiple linear regression model using the OLS method.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n```\n:::\n:::\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n#### Step 4: Fitting the Ridge Regression Model with L2 Regularization\n\nNow, let's fit the Ridge regression model with L2 regularization to the data.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 regularization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n```\n:::\n:::\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n#### Step 5: Comparing Results\n\nFinally, let's compare the coefficients and MSE of the OLS and Ridge models.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n```\n:::\n:::\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 regularization on the coefficients and model performance.\n\nBy introducing L2 regularization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\n\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\n\nNote that L2 regularization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 regularization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term.\n\n",
    "supporting": [
      "L2 reg_files"
    ],
    "filters": [],
    "includes": {}
  }
}