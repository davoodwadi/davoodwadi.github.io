{
  "hash": "7eee57860893f792c2a5e755d810cb53",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multiple Linear Regression - Part 2\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\n\n\nMultiple linear regression is a powerful technique used for predicting a continuous outcome variable based on multiple predictor variables. In this tutorial, we will learn how to perform multiple linear regression using a design matrix in Python. \n\nFirst, let's define the problem. In multiple linear regression, we have a dependent variable (also called the response or target variable) and several independent variables (also called features, input variables, or predictors). The goal is to find the best linear relationship between the predictors and the target variable.\n\nThe general equation for a multiple linear regression model with 'p' predictors is given by:\n\nY = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε\n\nWhere:\n- Y is the target variable\n- X₁, X₂, ..., Xₚ are the predictor variables\n- β₀, β₁, β₂, ..., βₚ are the coefficients (or weights) of the predictors\n- ε is the error term, representing the randomness or noise in the relationship\n\nTo estimate the coefficients (β₀, β₁, β₂, ..., βₚ), we need to minimize the sum of squared residuals, which measures the differences between the actual values of the target variable (Y) and the predicted values from the regression model.\n\nIn multiple linear regression, the predictors are often organized into a design matrix (X), where each row represents an observation and each column represents a predictor variable.\n\nNow let's see how to perform multiple linear regression using a design matrix in Python.\n\n::: {#081b0734 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\n# Define the design matrix X\nX = np.array([[3,  3, -3],\n              [-4, 5,  6],\n              [7, -8,  9]])\n\n# Define the target variable Y\nY = np.array([10, 20, 30])\n\n# Calculate beta coefficients using the normal equation\nbeta = np.linalg.inv(X.T @ X) @ X.T @ Y\n\nprint('Beta coefficients:', beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBeta coefficients: [3.56321839 2.98850575 3.2183908 ]\n```\n:::\n:::\n\n\nIn the above code, we first import the necessary libraries. Then, we define the design matrix X as a 2-dimensional numpy array that contains the predictor variables. Each row represents an observation, and each column represents a predictor variable. We also define the target variable Y as a 1-dimensional numpy array.\n\nNext, we use the normal equation to calculate the beta coefficients. The normal equation is given by:\n\n$$β = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y$$\n\n- $X^T$ is the transpose of X\n- $(X^T \\cdot X)^{-1}$ is the inverse of the matrix product of $X^T$ and $X$\n- $X^T \\cdot Y$ is the matrix product of $X^T$ and $Y$\n\nFinally, we print the beta coefficients, which represent the weights or coefficients of the predictors in the multiple linear regression model.\n\n",
    "supporting": [
      "multip2_files"
    ],
    "filters": [],
    "includes": {}
  }
}