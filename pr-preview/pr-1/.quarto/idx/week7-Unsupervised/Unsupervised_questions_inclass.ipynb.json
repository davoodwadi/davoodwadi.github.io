{"title":"MATH60629A","markdown":{"headingText":"MATH60629A","containsRefs":false,"markdown":"\n<a href=\"https://colab.research.google.com/github/davoodwadi/davoodwadi.github.io/blob/main/week7-Unsupervised/Unsupervised_questions_inclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Week \\#7 - Unsupervised Learning - Exercises\n\nThis practical session focuses on two tasks which used unsupervised learning, namely clustering (Sections 7.2 and 7.3) and dimensionality reduction (Section 7.4). The goal of this tutorial is to develop basic intuition behind some classic algorithms used for unsupervised learning (k-means, GMMs, and autoencoders).\n\n## 7.1 Data generation\n\nLet's first look at [Gaussian mixtures](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model), a relatively simple model, which we will generate with the [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs) function.\n\n**Remark**: In order to properly run Section 7.2.1, do not change the attributes of the `make_blobs` function in the code snippet below.\n\n## 7.2 Clustering: K-means Algorithm\n\n\n### 7.2.1 Implementation with numpy\n\nThe objective of this section is to implement and understand the procedures associated with the k-means algorithm. First, we will therefore implement the k-means algorithm with numpy.\n\n**Question 7.1**\n\nAccording to the pseudo code presented on [Slide 18 of the course](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_unsupervised.pdf), complete the `fit` function of the `k_means` class below.\n\n## update the centroids and cluster assignments\n\n## k means - updates\n\nWe can now call the above `k_means` class, estimate the centroids and visualize the associated clusters!\n\n### 7.2.2 Exploration of the K-means Algorithm with Scikit Learn\n\nOnce the algorithm has been coded, we are going to make our life easier and simply use the [Scikit Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) (-_-).  First, let's check that everything is running fine.\n\n### measure of error and Silhouette Score\n\n### probabilistic k-means\n\n### 7.2.3 Choosing the optimal number of clusters\n\nIn our first experiment, we knew the actual number of subpopulations (parameterized by the variable $ k $) associated with the simulated data. On the other hand, with non simulated datasets, we typically do not know the correct number of clusters. It is therefore important to develop methodologies in order to clearly define the number of clusters required.\n\n**Questions 7.2**\n\n1. Find a simple way to determine the optimal number of clusters. (hint: using a validation set.)\n2. Implement it. (see below for an initial implementation)\n3. How many clusters would you choose?\n\n**Questions 7.3**\n\n   1. Are you disappointed by the behaviour of the curve associated to the validation set?\n   2. Considering the results obtained here, could you imagine a better way to know the optimal number of clusters?\n\nThe previous exercise was based on a relatively simple dataset. Indeed, there was a large number of observations ($n = 1000$) for a relatively small variable space ($\\bf{X} \\in \\mathbb{R}^2$) and small number of clusters ($k = 2$). In order to validate the relevance of the cross-validation procedure to fix the number of clusters, let's now simulate a slightly more complex dataset.\n\nLet's now look how the loss associated to the validation set behave on a more complex dataset.\n\n**Questions 7.4**\n\n   1. Are you surprised by the behavior of the curve associated to the training set?\n   2. Are you disappointed by the behavior of the curve associated to the validation set?\n\n## 7.3 Clustering - The Gaussian Mixture Model\n\nWe now consider the [Expectation Maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) (EM) in order to estimate mixtures of Gaussians. As presented on [Slides 26 to 41 of the course](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_unsupervised.pdf), the idea is simply to associate each observation with a probability of belonging to one or the other of the distributions.\n\nCoding the EM algorithm in numpy can be a tedious exercise, so we'll just use the GMM implementation provided by sklearn: [sklearn GMMs](https://scikit-learn.org/0.16/modules/generated/sklearn.mixture.GMM.html)\n\n**Questions 7.5**\n\n1. After a quick glance at the figure above, what do you notice that is different from the k-means algorithm?\n2. What would have happened if we had set the parameter associated with the variance of the sub-populations (`cluster_std`) to 2?\n\n### 7.3.1 Choosing the optimal number of clusters for a GMM\n\n**Questions 7.6**\n\n1. How do we find the right number of clusters?\n2. Do the train/validation curves behave similarly to those of the k-means algorithm (e.g. in Section 7.2.3)? Does it make sense?\n\n<i>Additional information:</i> To evaluate the performance of an algorith, in machine learning we tend to stick with the validation set. However, we could use a model selection criterium from the statistics literature such as the [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) or the [Bayesian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) which penalized nonparsimonious models.\n\n## 7.4 Dimensionality Reduction: Autoencoder\n\n### 7.4.1 Model\n\nAutoencoders (AE) are a class of neural networks that allow unsupervised learning of the latent characteristics of the data being studied (see capsule 4 of week 7 or [Chapter 14](http://www.deeplearningbook.org/contents/autoencoders.html) of the [Deep Learning book](http://www.deeplearningbook.org/)). To do this, the AE will attempt to predict, or copy, the input observations using (multiple) transformations (hidden layer). In its simplest form, the architecture of an AE can be summarized in the diagram below.\n\n![title](https://github.com/lcharlin/80-629/blob/master/week7-Unsupervised/Images/AE.png?raw=1)\n\nLooking more closely, the AE consists of an encoder, the function $h(\\cdot)$ defined by:\n\n$$\n\\begin{align}\n    h(\\mathbf{x}) = \\frac{1}{1+ \\exp(-\\mathbf{W} \\mathbf{x})}.\n\\end{align}\n$$\n\nThis function takes as input the observations and will consist of recoding it as a hidden layer so as to reduce their size (fewer neurons). Afterwards, an encoder defined by:\n\n$$\n\\begin{align}\n    f(h(\\mathbf{x})) = \\mathbf{W}^\\top h(\\mathbf{x})\n\\end{align}\n$$\n\nwill attempt <i>to reconstruct </i> the input observations from the hidden layer. In this sense, the AE tries to estimate the observations used as input.\n\n### 7.4.3 Data simulation\n\nLet's first simulate more complex Gaussian mixtures (look at the number of clusters and the dimension of the data) in order to understand the behaviour of the AEs.\n\n**!! Remark !!**\n\nThere's no specific class for autoencoders in `sklearn`, but since AEs are a type of feed-forward network we can simply reuse the `MLP` classes (e.g., `MLPRegressor`) to build an AE.\n\nNote that there exists un library called `scikit-neuralnetwork` which offers [AE models](https://scikit-neuralnetwork.readthedocs.io/en/latest/guide_sklearn.html#unsupervised-pre-training) directly using an sklearn-like interface. You could of course also use PyTorch.\n\n**Question 7.7**\n\nWhy should the dimension of the hidden layer be smaller than the dimension of the input layer?\n\n### 7.4.5 A representation study\n\nNow that the AE is trained, we can look at the latent representation given by the hidden layer. Since we want to visualize the hidden states, we can simply compare the two by two representations from a small amount of data.\n\n**Questions 7.8**\n\n1. Does the plot behave as you expected? How so?\n2. Run the same experiment again without changing the hyperparameters. What do you notice? Does the latent representation seems to have changed?\n3. Would the plot have been different if we had simulated the data from 2 clusters instead of 4? Try it!\n4. How could you use the information represented in the hidden layers?\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"Unsupervised_questions_inclass.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}