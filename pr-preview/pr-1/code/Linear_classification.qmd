---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  cache: true
---

### Linear least squares

Linear least squares is a technique used for regression problems, where we aim to predict continuous numerical values. However, it can also be used for classification tasks by transforming the problem into a binary classification problem.

In linear least squares for classification, we use a linear model to classify data into two classes. We assign class labels of -1 and 1 to the two classes. The goal is to find a linear boundary that best separates the two classes, minimizing the sum of squared distances between the data points and the decision boundary.

Let's see how we can do this in Python:

First, we need to import the required libraries: numpy and matplotlib.

```{python}
import numpy as np
import matplotlib.pyplot as plt
```

Next, let's generate some synthetic data with two classes. We will use the `make_classification` function from the `sklearn.datasets` module to create a random dataset.

```{python}
from sklearn.datasets import make_classification

np.random.seed(0)
X, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)
```

We have created a dataset with 100 samples, 1 feature, and 1 informative features.

Now, let's visualize the data using a scatter plot:

```{python}
plt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')
plt.scatter(X[y == 0], y[y == 0], color='r', label='Class 0')
plt.xlabel('Feature')
plt.ylabel('Label')
plt.legend()
plt.show()
```

We have plotted the data points for each class on a scatter plot.

To apply linear least squares for classification, we need to add a column of ones to our feature matrix X to incorporate the bias term in the linear equation.

```{python}
X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
```

Now, let's define our linear model and solve for the optimal parameters using linear least squares.

```{python}
theta = np.linalg.inv(X.T @ X) @ X.T @ y
```

Here, `theta` is the vector of parameters that defines our linear model. The equation used to solve for `theta` is:


$\theta = (X^T X)^{-1} X^T y$

Finally, let's visualize the decision boundary of our linear model along with the data points.

```{python}
plt.scatter(X[y == 1][:,1], y[y == 1], color='b', label='Class 1')
plt.scatter(X[y == 0][:,1], y[y == 0], color='r', label='Class 0')
plt.xlabel('Feature')
plt.ylabel('Label')
plt.legend()

# Plotting the decision boundary
x_boundary = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
# x2_boundary = -(theta[0] + theta[1]*x1_boundary) / theta[2]
y_boundary = x_boundary * theta[1] +  theta[0]
plt.plot(x_boundary, y_boundary, color='black', linewidth=2)

plt.show()
```



We have plotted the decision boundary determined by our linear model, which separates the two classes.

Linear least squares for classification is a simple technique for linearly separable datasets. Note that this approach assumes the data points are linearly separable and does not work well for nonlinear classification problems.