
---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  cache: true
eval: false
---


The Value Iteration Algorithm is a method used in reinforcement learning to find the optimal value function for a given Markov Decision Process (MDP). It is an iterative algorithm that aims to converge to the optimal value function by iteratively updating the values of states based on the Bellman Optimality Equation.

The Bellman Optimality Equation for a state s in an MDP is given by:

$V^*(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]$

where:
- $V^*(s)$ is the optimal value function for state s.
- $\max_a$ represents the selection of the action that maximizes the equation.
- $P(s'|s,a)$ is the probability of transitioning to state s' from state s given action a.
- $R(s,a,s')$ is the reward for transitioning from state s to state s' using action a.
- $\gamma$ is the discount factor that determines the importance of future rewards.

The algorithm starts with an initial estimate of the optimal value function and iteratively updates the values of states until convergence. The value of each state is updated by selecting the action that maximizes the equation.

Now, let's implement the Value Iteration Algorithm in Python:

```{python}
import numpy as np

def value_iteration(env, gamma, epsilon):
    # Initialize the value function
    V = np.zeros(env.n_states)
    
    # Iterate until convergence
    while True:
        delta = 0
        # Update the value of each state
        for s in range(env.n_states):
            # Find the maximum Q-value for the state
            max_value = -np.inf
            for a in range(env.n_actions):
                q_value = 0
                # Calculate the Q-value for each action
                for s_prime in range(env.n_states):
                    transition_prob = env.transition_probabilities[s,a,s_prime]
                    reward = env.rewards[s,a,s_prime]
                    q_value += transition_prob * (reward + gamma * V[s_prime])
                max_value = max(max_value, q_value)
            # Update the value function
            delta = max(delta, np.abs(max_value - V[s]))
            V[s] = max_value
        
        # Check for convergence
        if delta < epsilon:
            break
    
    return V
```

Let's go through the code step by step:

1. The code starts by importing the necessary libraries, including numpy for numerical operations.

2. The `value_iteration` function takes three arguments: `env` (the environment), `gamma` (the discount factor), and `epsilon` (the convergence threshold).

3. We initialize the value function `V` as an array of zeros with the same size as the number of states in the environment.

4. The algorithm enters a loop that continues until convergence.

5. Inside the loop, we set the `delta` variable to 0. `delta` is used to keep track of the maximum change in the value function during an iteration.

6. We iterate over all the states in the environment.

7. For each state, we iterate over all the possible actions.

8. For each action, we calculate the Q-value using nested loops. The Q-value is calculated as the sum of the transition probabilities multiplied by the sum of the rewards and the discounted value of the next state.

9. We update the `max_value` variable if the current Q-value is greater than the current maximum value.

10. After all actions have been considered, we update the value of the state in the value function array.

11. We update the `delta` variable with the maximum change in any state during the iteration.

12. We check if the `delta` is less than the convergence threshold `epsilon`. If it is, the algorithm breaks out of the loop and returns the optimal value function.

13. If the convergence condition is not met, the algorithm continues to the next iteration.

That's it! The `value_iteration` function returns the optimal value function for the given MDP. This value function can be used to determine the optimal policy for taking actions in the environment.