{
  "hash": "323cfbca9d6deca34a046ba45a04a528",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Generating Synthetic Data\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute:\n  cache: true\n---\n\n\n\n\n\n\n\n# Bias-Variance Trade-off\n\nThe bias-variance trade-off is a fundamental concept in machine learning that helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. In supervised learning problems, we aim to find a model, usually a mathematical function, that can accurately predict the target variable based on the input features.\n\n## Bias and Variance\n\nBefore diving into the bias-variance trade-off, let's briefly explain two important concepts: **bias** and **variance**.\n\n- **Bias** measures how much our modelâ€™s predictions deviate from the true values. A model with high bias oversimplifies the underlying relationship between the features and the target variable. This can lead to underfitting, where the model fails to capture the patterns and relationships in the data.\n\n- **Variance** measures the variability of model predictions for different training sets. A model with high variance is too sensitive to the specific training examples and does not generalize well to new, unseen data. This can lead to overfitting, where the model fits the training data too well but performs poorly on new data.\n\nThe aim is to find a good balance between bias and variance, where the model captures the underlying patterns in the training data without overfitting.\n\n## Bias-Variance Trade-off\n\nThe bias-variance trade-off states that as we decrease the bias of a model (increasing complexity), we tend to increase its variance, and vice versa. This trade-off occurs because model complexity allows for a better fit to the training data, but at the risk of poor performance on new data.\n\nTo illustrate this concept, let's consider a regression problem where we can adjust the complexity of a model by changing the degree of the polynomial used for fitting the data.\n\n## Example\n\n### Importing Required Libraries\n\nWe start by importing the necessary libraries: NumPy for numerical operations and matplotlib for visualization.\n\n\n::: {#2b7ffc67 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# To plot the graphs inline in Jupyter Notebook\n%matplotlib inline\n```\n:::\n\n\n\nNext, we generate some synthetic data with a nonlinear relationship between the input features and the target variable using the `numpy` library.\n\n\n::: {#db49b5b4 .cell execution_count=2}\n``` {.python .cell-code}\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Generate input features\nX = np.linspace(-5, 5, 100).reshape(-1, 1)\n\n# Generate target variable with noise\nY_true = X**3 - X**2 + X + np.random.randn(100, 1)\n```\n:::\n\n\nHere, we generate 100 samples of input features `X` ranging from -5 to 5. The target variable `Y_true` is generated using a cubic relationship with some random Gaussian noise.\n\n### Fitting Polynomial Models\n\nWe will now fit polynomial models with different degrees to the synthetic data and observe the effect of model complexity on bias and variance.\n\n::: {#d805e522 .cell execution_count=3}\n``` {.python .cell-code}\n# Create a function to fit polynomial models and visualize the results\ndef fit_polynomial(X, Y_true, degree):\n    # Fit polynomial regression model\n    poly_features = np.polynomial.Polynomial.fit(X.flatten(), Y_true.flatten(), degree)\n    Y_pred = poly_features(X.flatten())\n    \n    # Compute bias and variance\n    bias = np.mean(np.abs(Y_true - Y_pred))\n    variance = np.var(Y_pred)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X, Y_true, label='True Data', color='b')\n    plt.plot(X, Y_pred, label='Predicted', color='r')\n    plt.title(f'Polynomial Regression (Degree = {degree})\\nBias = {bias:.2f}, Variance = {variance:.2f}')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n# Fit polynomial models with degrees 1, 2, 3, 5, 10\nfit_polynomial(X, Y_true, degree=1)\nfit_polynomial(X, Y_true, degree=2)\nfit_polynomial(X, Y_true, degree=3)\nfit_polynomial(X, Y_true, degree=5)\nfit_polynomial(X, Y_true, degree=10)\n```\n\n::: {.cell-output .cell-output-display}\n![](bias v_files/figure-html/cell-4-output-1.png){width=827 height=541}\n:::\n\n::: {.cell-output .cell-output-display}\n![](bias v_files/figure-html/cell-4-output-2.png){width=827 height=541}\n:::\n\n::: {.cell-output .cell-output-display}\n![](bias v_files/figure-html/cell-4-output-3.png){width=827 height=541}\n:::\n\n::: {.cell-output .cell-output-display}\n![](bias v_files/figure-html/cell-4-output-4.png){width=827 height=541}\n:::\n\n::: {.cell-output .cell-output-display}\n![](bias v_files/figure-html/cell-4-output-5.png){width=827 height=541}\n:::\n:::\n\n\nIn this code block, we define a function `fit_polynomial` that takes the input features `X`, true target variable `Y_true`, and the degree of the polynomial model to be fitted as arguments. Inside the function, we use the `numpy.polynomial.Polynomial.fit` function to fit a polynomial regression model with the desired degree.\n\nFor each degree of the polynomial model, we compute the bias and variance using the mean absolute error and variance of the predicted values. Then, we plot the true data points, the predicted curve, and display the bias and variance in the title of the plot.\n\n### Analysis and Observations\n\nBy running the code above, we get a series of plots showing the true data points and the predicted curves for polynomial regression models with different degrees. Each plot also displays the corresponding bias and variance values.\n\n- For a linear model (degree=1), the model is too simple to capture the underlying cubic relationship in the data. Hence, it has a high bias and performs poorly in terms of fitting the data.\n\n- As the degree of the polynomial model increases, the model can fit the data more accurately, resulting in reduced bias. However, as the complexity increases (degree=5 and 10), we observe that the models start to capture the random fluctuations in the data, resulting in higher variance. These models may fit the training data very well but are likely to perform poorly on unseen data.\n\n- The model with a degree of 3 strikes a good balance between bias and variance, as it captures the underlying cubic relationship while avoiding overfitting.\n\n## Conclusion\n\nThe bias-variance trade-off is a fundamental concept in machine learning. It helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. By finding the right balance between bias and variance, we can develop models that accurately represent the patterns in the data without overfitting or oversimplifying the relationships.\n\n",
    "supporting": [
      "bias v_files"
    ],
    "filters": [],
    "includes": {}
  }
}