{
  "hash": "52322553b512b4d2515c67479930100a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multiple Linear Regression - Part 1\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\nIn machine learning and statistics, multiple linear regression is a commonly used technique to model the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the independent variables and the dependent variable.\n\nThe general form of multiple linear regression can be written as:\n\n$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon$$\n\nWhere:\n\n- $y$ is the dependent variable,\n- $x_1, x_2, ..., x_p$ are the independent variables,\n- $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p$ are the coefficients,\n- $\\epsilon$ is the noise term.\n\nTo perform multiple linear regression, we can use the Ordinary Least Squares (OLS) method, which aims to find the coefficients that minimize the sum of the squared differences between the actual and predicted values.\n\n## Design Matrix\n\nIn multiple linear regression, we represent the independent variables as a matrix called the design matrix. The design matrix, denoted by $X$, has one row for each observation and one column for each independent variable.\n\nFor example, if we have n observations and p independent variables, the design matrix $X$ will be an n x p matrix.\n\n## Proof of OLS\n\nThe OLS method minimizes the sum of squared errors (SSE) between the actual dependent variable values and the predicted values. The SSE can be written as:\n\n$$SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n\nWhere:\n\n- $y_i$ is the actual dependent variable value for the i-th observation,\n- $\\hat{y}_i$ is the predicted dependent variable value for the i-th observation.\n\nTo find the coefficients that minimize SSE, we differentiate SSE with respect to each coefficient and set the derivatives to zero.\n\nLet's derive the formula for OLS step by step.\n\n1. The predicted value of the dependent variable can be written as:\n\n$$\\hat{y} = X\\beta$$\n\nWhere:\n\n- $\\hat{y}$ is an n x 1 vector of predicted values,\n- $X$ is the design matrix,\n- $\\beta$ is a vector of coefficients.\n\n2. The SSE can be expressed in matrix form as:\n\n$$SSE = (\\mathbf{y} - \\mathbf{\\hat{y}})^T (\\mathbf{y} - \\mathbf{\\hat{y}})$$\n\nWhere:\n\n- $\\mathbf{y}$ is an n x 1 vector of actual dependent variable values,\n- $\\mathbf{\\hat{y}}$ is an n x 1 vector of predicted dependent variable values.\n\n3. Expanding the above equation, we get:\n\n$$SSE = (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)$$\n\n4. Expanding the squared term, we get:\n\n$$SSE = \\mathbf{y}^T\\mathbf{y} - 2\\beta^TX^T\\mathbf{y} + \\beta^TX^TX\\beta$$\n\n5. To minimize SSE with respect to $\\beta$, we differentiate SSE with respect to $\\beta$ and set the derivative to zero:\n\n$$\\frac{\\partial SSE}{\\partial \\beta} = -2X^T\\mathbf{y} + 2X^TX\\beta = 0$$\n\n6. Solving for $\\beta$, we get:\n\n$$X^TX\\beta = X^T\\mathbf{y}$$\n\n$$\\beta = (X^TX)^{-1}X^T\\mathbf{y}$$\n\nThe above formula gives us the optimal values for $\\beta$ that minimize SSE.\n\n",
    "supporting": [
      "multip1_files"
    ],
    "filters": [],
    "includes": {}
  }
}