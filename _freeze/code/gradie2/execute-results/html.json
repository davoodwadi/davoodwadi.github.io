{
  "hash": "23dad2046e65e8138ac7815761c4a69b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\n\n\n# Gradient Descent with PyTorch\n\nIn this tutorial, we will learn how to implement the Gradient Descent algorithm using PyTorch. Gradient Descent is an optimization algorithm commonly used in machine learning to find the optimal parameters of a model by iteratively updating them based on the gradient of a cost function.\n\nWe will start by explaining the theory behind Gradient Descent, then move on to implementing it in PyTorch.\n\n## Theory\n\nGradient Descent works by iteratively updating the parameters of a model in the opposite direction of the gradient of a cost function. The goal is to find the minimum of the cost function, which corresponds to the optimal parameters for the model.\n\nThe update rule for Gradient Descent is given by:\n\n\n$\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)$\n\n\nwhere:\n- $\\theta$ represents the current parameters of the model,\n- $\\alpha$ is the learning rate (step size),\n- $J(\\theta)$ is the cost function, and\n- $\\nabla J(\\theta)$ is the gradient of the cost function with respect to $\\theta$.\n\nThe learning rate $\\alpha$ determines the size of the steps taken in each iteration. Too small of a learning rate may cause the algorithm to converge slowly, while too large of a learning rate may cause it to overshoot the minimum.\n\n## Implementation\n\nNow let's implement Gradient Descent using PyTorch. We will start by generating some sample data and defining a linear regression model. Then, we will define the cost function and gradient descent function.\n\n### Step 1: Generate Sample Data\n\n::: {#bb0c555f .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Generate some dummy data\nX = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\ny = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\nprint(f\"X is {X}\")\nprint(f\"y is {y}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/var/folders/jv/ppbxly7j7vzgcr8sdv78s2hr0000gn/T/ipykernel_51131/3338659258.py\", line 1, in <module>\n    import torch\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n    from .functional import *  # noqa: F403\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n    import torch.nn.functional as F\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n    from .modules import *  # noqa: F403\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\n  File \"/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n/Users/davoodwadi/pyth/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning:\n\nFailed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nX is tensor([[1.],\n        [2.],\n        [3.],\n        [4.]])\ny is tensor([[2.],\n        [4.],\n        [6.],\n        [8.]])\n```\n:::\n:::\n\n\nIn this step, we import the `torch` module, set the random seed for reproducibility, and generate some dummy data. Here, `X` represents the input features and `y` represents the corresponding output values.\n\n### Step 2: Define Linear Regression Model\n\n::: {#d634040b .cell execution_count=2}\n``` {.python .cell-code}\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super(LinearRegression, self).__init__()\n        self.linear = torch.nn.Linear(1, 1)\n        \n    def forward(self, x):\n        y_pred = self.linear(x)\n        return y_pred\n    \nmodel = LinearRegression()\nprint(f\"weight: {model.linear.weight.data}\")\nprint(f\"bias: {model.linear.bias.data}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweight: tensor([[-0.0075]])\nbias: tensor([0.5364])\n```\n:::\n:::\n\n\nIn this step, we define a linear regression model using PyTorch's `torch.nn.Module` class. The model consists of a single linear layer `self.linear`, which maps the input features to the output values. The `forward` method is used to define the forward pass of the model.\n\n### Step 3: Define Cost Function\n\n::: {#91b8c674 .cell execution_count=3}\n``` {.python .cell-code}\ncriterion = torch.nn.MSELoss()\n```\n:::\n\n\nIn this step, we define the Mean Squared Error (MSE) loss function using PyTorch's `torch.nn.MSELoss` class. The loss function measures the difference between the predicted values and the actual values.\n\n### Step 4: Define Gradient Descent Function\n\n::: {#ff78c653 .cell execution_count=4}\n``` {.python .cell-code}\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\ndef gradient_descent(X, y, model, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        # Forward pass\n        y_pred = model(X)\n        \n        # Compute loss\n        loss = criterion(y_pred, y)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch+1) % 200 == 0:\n            print(f'Epoch: {epoch+1}, Loss: {loss.item():.3f}, Weight: {model.linear.weight.data.item():.3f}, bias: {model.linear.bias.data.item():.3f}')\n```\n:::\n\n\nIn this step, we define the gradient descent function `gradient_descent`. The function takes the input features `X`, target values `y`, model, criterion (loss function), optimizer, and the number of epochs as input.\n\nIn each epoch, we perform the following steps:\n1. Compute the forward pass of the model to obtain the predicted values.\n2. Compute the loss between the predicted values and the actual values.\n3. Perform backpropagation to compute the gradients.\n4. Update the parameters of the model using the optimizer.\n\nWe also print the loss value every 10 epochs to track the progress of the algorithm.\n\n### Step 5: Run Gradient Descent\n\n::: {#8a4cbe44 .cell execution_count=5}\n``` {.python .cell-code}\nnum_epochs = 2000\ngradient_descent(X, y, model, criterion, optimizer, num_epochs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch: 200, Loss: 0.060, Weight: 1.796, bias: 0.600\nEpoch: 400, Loss: 0.018, Weight: 1.888, bias: 0.329\nEpoch: 600, Loss: 0.005, Weight: 1.939, bias: 0.181\nEpoch: 800, Loss: 0.002, Weight: 1.966, bias: 0.099\nEpoch: 1000, Loss: 0.000, Weight: 1.981, bias: 0.054\nEpoch: 1200, Loss: 0.000, Weight: 1.990, bias: 0.030\nEpoch: 1400, Loss: 0.000, Weight: 1.994, bias: 0.016\nEpoch: 1600, Loss: 0.000, Weight: 1.997, bias: 0.009\nEpoch: 1800, Loss: 0.000, Weight: 1.998, bias: 0.005\nEpoch: 2000, Loss: 0.000, Weight: 1.999, bias: 0.003\n```\n:::\n:::\n\n\nFinally, we run the gradient descent function with the specified number of epochs. The function will update the parameters of the model in each epoch to minimize the loss.\n\n## Conclusion\n\nIn this tutorial, we learned how to implement the Gradient Descent algorithm using PyTorch. Gradient Descent is a fundamental optimization algorithm used to find the optimal parameters of a model. By iteratively updating the parameters in the direction of the negative gradient, we can find the minimum of a cost function.\n\n",
    "supporting": [
      "gradie2_files"
    ],
    "filters": [],
    "includes": {}
  }
}