{
  "hash": "524e736961886278a0c5c6ca9f87f295",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\n\n\nTo work with custom datasets in PyTorch, we need to create a custom dataset class and then use a data loader to load the data in mini-batches during training or evaluation. In this tutorial, we will create a custom randomly generated dataset and load it using a data loader.\n\nFirst, let's install the necessary packages and import the required libraries:\n\n::: {#57d1f01f .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n```\n:::\n\n\nNext, we will create a custom dataset class by subclassing the `Dataset` class provided by PyTorch. In this example, let's create a dataset of randomly generated 2D points with corresponding labels:\n\n::: {#bb171183 .cell execution_count=2}\n``` {.python .cell-code}\nclass CustomDataset(Dataset):\n    def __init__(self, num_samples):\n        self.num_samples = num_samples\n        # data is from 0 to num_samples-1\n        self.data = torch.arange(num_samples)\n        self.labels = (self.data%2==0).long()\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n        return sample, label\n```\n:::\n\n\nIn the `__init__` method of our `CustomDataset` class, we initialize the number of samples and number of features. We then generate random data points and labels using the `torch.randn` and `torch.randint` functions, respectively.\n\nThe `__len__` method is used to return the total number of samples in the dataset.\n\nThe `__getitem__` method is used to access a particular sample and its label given an index. We retrieve the corresponding data point and label from the pre-generated data and labels tensors, and return them as a tuple.\n\nNow, let's create an instance of our custom dataset:\n\n::: {#876465a4 .cell execution_count=3}\n``` {.python .cell-code}\ndataset = CustomDataset(num_samples=20)\nprint(dataset[0])\nprint(dataset[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(tensor(0), tensor(1))\n(tensor(1), tensor(0))\n```\n:::\n:::\n\n\nLet's visualize the dataset.\n\n::: {#a169452c .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nplt.scatter(dataset.data, dataset.labels)\nplt.xlabel('inputs')\nplt.ylabel('labels')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0, 0.5, 'labels')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dataloader_pytorch_files/figure-html/cell-5-output-2.png){width=589 height=429}\n:::\n:::\n\n\nWe have created a dataset with 20 samples.\n\nFinally, we can create a data loader to load our custom dataset in mini-batches:\n\n::: {#a0a9c07b .cell execution_count=5}\n``` {.python .cell-code}\ndataloader = DataLoader(dataset, batch_size=8, shuffle=False, drop_last=False)\n```\n:::\n\n\nThe `DataLoader` class takes our custom dataset as input along with the desired batch size. \n\n::: {#c6818b57 .cell execution_count=6}\n``` {.python .cell-code}\nfor batch in dataloader:\n    inputs, labels = batch\n    print(f'inputs: {inputs}')\n    print(f'labels: {labels}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs: tensor([0, 1, 2, 3, 4, 5, 6, 7])\nlabels: tensor([1, 0, 1, 0, 1, 0, 1, 0])\n\ninputs: tensor([ 8,  9, 10, 11, 12, 13, 14, 15])\nlabels: tensor([1, 0, 1, 0, 1, 0, 1, 0])\n\ninputs: tensor([16, 17, 18, 19])\nlabels: tensor([1, 0, 1, 0])\n\n```\n:::\n:::\n\n\nIn each iteration, the data loader returns a batch of samples, where `inputs` contains the features of the samples and `labels` contains their corresponding labels. We can now perform any required operations on the mini-batch, such as passing it through a model for training or evaluation.\n\nYou can see that by default, the `Dataloader` class returns the dataset in mini-batches without changing the order of the data. \n\nWe can set the `shuffle` parameter to `True` if we want to shuffle the data before each epoch. Suffling the training data can prevent the model from relying on the order of the data points to predict the labels.\n\n::: {#e375713f .cell execution_count=7}\n``` {.python .cell-code}\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=False)\nfor batch in dataloader:\n    inputs, labels = batch\n    print(f'inputs: {inputs}')\n    print(f'labels: {labels}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs: tensor([15,  4,  5, 16, 19, 18,  2,  7])\nlabels: tensor([0, 1, 0, 1, 0, 1, 1, 0])\n\ninputs: tensor([11, 12,  3,  9,  1,  6,  0, 17])\nlabels: tensor([0, 1, 0, 0, 0, 1, 1, 0])\n\ninputs: tensor([13,  8, 10, 14])\nlabels: tensor([0, 1, 1, 1])\n\n```\n:::\n:::\n\n\nFor specific models that rely on the `batch_size` to construct their parameters (see [RNNs](RNN_timeseries.qmd)), we need to set `drop_last` to `True` to prevent batches having different shapes.\n\n::: {#92d4ffe8 .cell execution_count=8}\n``` {.python .cell-code}\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)\nfor batch in dataloader:\n    inputs, labels = batch\n    print(f'inputs: {inputs}')\n    print(f'labels: {labels}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs: tensor([10,  2, 18,  9, 16, 19,  4,  5])\nlabels: tensor([1, 1, 1, 0, 1, 0, 1, 0])\n\ninputs: tensor([ 6,  8,  1,  3,  7, 14, 17, 13])\nlabels: tensor([1, 1, 0, 0, 0, 1, 0, 0])\n\n```\n:::\n:::\n\n\nYou can see that by setting `drop_last=True`, the final mini-batch of size $4$ is dropped.\n\n\nThat's it! You have learned how to create a custom dataset and load it using a data loader in PyTorch.\n\n",
    "supporting": [
      "dataloader_pytorch_files"
    ],
    "filters": [],
    "includes": {}
  }
}