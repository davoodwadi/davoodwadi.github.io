{
  "hash": "2511ce1fc256497ac78f4ead7ebbfc5f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\neval: false\n---\n\n\n\n\n\nTo train the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`, we need to perform the following steps:\n\n1. Import the necessary libraries:\n\n::: {#e49258c4 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nimport torch.multiprocessing as mp\nimport torchvision\nimport torchvision.transforms as transforms\n```\n:::\n\n\n2. Define the model architecture:\n\n::: {#38bfe665 .cell execution_count=2}\n``` {.python .cell-code}\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 16 * 16, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(-1, 32 * 16 * 16)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CNN()\n```\n:::\n\n\n3. Define the training function:\n\n::: {#3f91baf9 .cell execution_count=3}\n``` {.python .cell-code}\ndef train(rank, world_size):\n    torch.manual_seed(0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize distributed training\n    dist.init_process_group(backend='nccl', init_method='env://', rank=rank, world_size=world_size)\n\n    # Load CIFAR-10 dataset\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n                                        transform=transforms.ToTensor())\n    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset, num_replicas=world_size, rank=rank)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=False, sampler=train_sampler)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    # Transfer model to GPU\n    model.to(device)\n\n    # Create DistributedDataParallel model\n    model = DistributedDataParallel(model)\n\n    # Training loop\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        train_correct = 0\n        total = 0\n\n        for inputs, labels in trainloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Compute metrics\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            train_loss += loss.item()\n\n        # Print epoch results\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {(train_correct/total)*100:.2f}%\")\n\n    # Cleanup\n    dist.destroy_process_group()\n```\n:::\n\n\n4. Define the main function for multi-processing:\n\n::: {#b24e007f .cell execution_count=4}\n``` {.python .cell-code}\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == '__main__':\n    main()\n```\n:::\n\n\n5. Run the main function to start the training process:\n\n::: {#5986f424 .cell execution_count=5}\n``` {.python .cell-code}\npython train_cifar10_distributed.py\n```\n:::\n\n\nThis code will train a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`. The `DistributedDataParallel` module enables us to train models on multiple GPUs or machines.\n\nThe CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat, etc.). The dataset is divided into 50,000 training images and 10,000 testing images.\n\nIn the code above, we define a CNN model with two convolutional layers and two fully connected layers. We then load the CIFAR-10 dataset, define the loss function and optimizer, and transfer the model to the GPU if available.\n\nWe create a `DistributedDataParallel` model from the base model to enable distributed training. The `DistributedDataParallel` wrapper will automatically scatter the input data to the available GPUs and gather the gradients during the backward pass.\n\nNext, we define the training loop, where we iterate over the batches of the training dataset. For each batch, we compute the forward pass, the loss, and perform the backward pass and optimization. We also keep track of the training loss and accuracy.\n\nAfter each epoch, we print the epoch's training loss and accuracy. Finally, we clean up the distributed training environment and destroy the process group.\n\nTo run the code, we use the `mp.spawn()` function to parallelize the training process across multiple processes, where each process trains on a separate GPU. The `world_size` parameter is set to the number of available GPUs.\n\n",
    "supporting": [
      "DistributedDataParallel_files"
    ],
    "filters": [],
    "includes": {}
  }
}