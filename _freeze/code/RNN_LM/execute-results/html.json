{
  "hash": "02a596e6184c8920a1c379acb8f0c641",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 2. Preparing the Dataset\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute:\n  cache: true\neval: false\n---\n\n\n\n\n\n# Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch\n\nIn this tutorial, we will explore how to use Recurrent Neural Networks (RNNs) for language modeling using PyTorch. Language modeling is the task of predicting the probability of a sequence of words occurring in a given language. RNNs are particularly well-suited for this task, as they can capture the sequential nature of language.\n\n## 1. Setting up the Environment\n\nBefore we begin, make sure you have PyTorch installed. You can install PyTorch by following the instructions on the official website ([pytorch.org](https://pytorch.org/)).\n\nNext, let's import the required libraries and set the random seed for reproducibility.\n\n\n::: {#4b36d965 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport random\n\n# Set the random seed for reproducibility\nrandom.seed(42)\n```\n:::\n\n\n\nTo train our language model, we need a dataset consisting of sentences or sequences of words. In this tutorial, we will use a simplified version of the [WikiText-2](https://metatext.io/datasets/wikitext-103-&-2). *Created by Merity et al. at 2016, the WikiText-103 & 2 Dataset contains word and character level tokens extracted from Wikipedia, in English language. Containing 100M+ in TOKENS file format.*\n\n\n::: {#ce2e7068 .cell execution_count=2}\n``` {.python .cell-code}\npath = '../wikitext-2/'\ntrain_path = path + 'wiki.train.tokens'\nvalid_path = path + 'wiki.valid.tokens'\ntest_path = path + 'wiki.test.tokens'\n\ndef read_file(path):\n    # Open the file in read mode\n    with open(file_path, 'r') as file:\n        # Read the contents of the file\n        file_contents = file.read()\n    return file_contents\n\ntrain_string = read_file(train_path)\nvalid_string = read_file(valid_path)\ntest_string = read_file(test_path)\nprint(train_string[:100])\n```\n:::\n\n\nNow, we need to preprocess our dataset. We will take care of tokenization, converting words to numerical indices, and creating batches of sequences for training.\n\n## Tokenization\n\nWe first need to turn our very large string into a list of strings.\n\n::: {#e27f1bcf .cell execution_count=3}\n``` {.python .cell-code}\ntrain_list = train_string.split(\"\\n\")\nvalid_list = valid_string.split(\"\\n\")\ntest_list = test_string.split(\"\\n\")\ntrain_list[:5]\n```\n:::\n\n\n::: {#ec6b1755 .cell execution_count=4}\n``` {.python .cell-code}\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp('''\"Let's go to N.Y.!\"''')\n\nfor token in doc:\n    print(token.text)\n\nnlp.vocab.strings['Let']\n```\n:::\n\n\n## 3. Defining the RNN Language Model\n\nNext, we define our language model using an RNN. Here, we will use a simple LSTM (Long Short-Term Memory) architecture. The LSTM cell has been shown to be particularly effective in capturing long-range dependencies in sequential data.\n\n::: {#9f5c3d0e .cell execution_count=5}\n``` {.python .cell-code}\nclass RNNLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(RNNLanguageModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.lstm(embedded)\n        output = self.fc(output.reshape(-1, output.size(2)))\n        return output\n```\n:::\n\n\nIn the `__init__` method, we define the components of our model:\n- `embedding`: an embedding layer to convert word indices to dense word representations.\n- `lstm`: an LSTM layer to process the word embeddings, capturing the sequential dependencies.\n- `fc`: a fully connected linear layer to map the LSTM output to the vocabulary size, predicting the next word in the sequence.\n\nIn the `forward` method, we pass the input sequence through the layers. The output of the LSTM layer is reshaped and passed through the fully connected layer to obtain the predicted probabilities for each word in the vocabulary.\n\n## 4. Training the RNN Language Model\n\nNow, let's define a function to train our RNN Language Model.\n\n::: {#bf42f220 .cell execution_count=6}\n``` {.python .cell-code}\ndef train_model(model, train_iter, val_iter, num_epochs, lr):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        \n        for batch in train_iter:\n            optimizer.zero_grad()\n            \n            x = batch.text[:, :-1]\n            y = batch.text[:, 1:].flatten()\n            \n            output = model(x)\n            loss = criterion(output, y)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        train_loss /= len(train_iter)\n        \n        model.eval()\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in val_iter:\n                x = batch.text[:, :-1]\n                y = batch.text[:, 1:].flatten()\n                \n                output = model(x)\n                loss = criterion(output, y)\n                \n                val_loss += loss.item()\n        \n        val_loss /= len(val_iter)\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'language_model.pt')\n        \n        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n```\n:::\n\n\nIn the `train_model` function, we define the training loop:\n- We initialize the optimizer and the loss function.\n- For each epoch, we iterate over the training data.\n- For each batch, we compute the forward pass, calculate the loss, compute the gradients, and update the model's parameters using the optimizer.\n- After each epoch, we evaluate the model on the validation data and save the model if the validation loss improves.\n- Finally, we print the epoch number, training loss, and validation loss.\n\n## 5. Generating Text with the RNN Language Model\n\nLastly, let's define a function to generate text using our trained language model.\n\n::: {#40418eb3 .cell execution_count=7}\n``` {.python .cell-code}\ndef generate_text(model, seed_text, max_length):\n    model.eval()\n    \n    with torch.no_grad():\n        tokens = seed_text.split()\n        current_length = len(tokens)\n        \n        while current_length < max_length:\n            x = torch.tensor([[TEXT.vocab.stoi[token] for token in tokens]]).to(device)\n            \n            output = model(x)\n            last_word_logits = output[0, -1]\n            \n            probabilities = F.softmax(last_word_logits, dim=0).numpy()\n            predicted_index = np.random.choice(len(probabilities), p=probabilities)\n            predicted_word = TEXT.vocab.itos[predicted_index]\n            \n            tokens.append(predicted_word)\n            current_length += 1\n            \n    generated_text = ' '.join(tokens)\n    return generated_text\n```\n:::\n\n\nIn the `generate_text` function, we generate text of a given maximum length using the trained language model:\n- We initialize the model in evaluation mode.\n- We tokenize the seed text into a list of words.\n- We repeatedly generate the next word in the sequence until we reach the maximum length.\n- For each step, we pass the input sequence through the model to obtain the logits for the next word.\n- We apply softmax to the logits to obtain word probabilities and sample the next word using `np.random.choice()`.\n- We append the predicted word to the list of tokens and update the current length accordingly.\n- Finally, we concatenate the tokens and return the generated text.\n\n## Putting It All Together\n\nNow, let's put everything together and train our RNN language model.\n\n::: {#13e4609e .cell execution_count=8}\n``` {.python .cell-code}\n# Set the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Preprocess the dataset\ntrain_iter, val_iter, test_iter = preprocess_dataset()\n\n# Define the model\nvocab_size = len(train_iter.dataset.fields['text'].vocab)\nembedding_dim = 100\nhidden_dim = 128\nnum_layers = 2\nmodel = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n\n# Train the model\nnum_epochs = 10\nlearning_rate = 0.001\ntrain_model(model, train_iter, val_iter, num_epochs, learning_rate)\n\n# Generate text using the trained model\nseed_text = \"The weather is\"\nmax_length = 20\ngenerated_text = generate_text(model, seed_text, max_length)\nprint(generated_text)\n```\n:::\n\n\nIn this code snippet, we perform the following steps:\n- We set the device to CUDA if available, otherwise fallback to CPU.\n- We preprocess the dataset using the `preprocess_dataset()` function, which returns iterators for training, validation, and testing.\n- We define the model architecture using the `RNNLanguageModel` class.\n- We train the model using the `train_model()` function.\n- We generate text using the `generate_text()` function, providing a seed text and maximum length.\n\nThat's it! You have now learned how to use RNNs for language modeling in PyTorch. You can further experiment by changing the model architecture, hyperparameters, or using different datasets. Happy coding!\n\n",
    "supporting": [
      "RNN_LM_files"
    ],
    "filters": [],
    "includes": {}
  }
}