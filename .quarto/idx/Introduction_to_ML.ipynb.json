{"title":"MATH60629A Fall 2022","markdown":{"yaml":{"title":"MATH60629A Fall 2022"},"headingText":"Tutorial: An Introduction to Practical Machine Learning","containsRefs":false,"markdown":"\n\n\n\n\n\n\nThis tutorial provides a short introduction to the practice of machine learning.\n\nWe assume that the user already has an understanding of the basic concepts that underlie the field. We review both methodological concepts like supervised learning and also use software libraries such as `scikit-learn`, `pandas`, and `numpy`.\n\nIn particular, we will:\n1. load some data,\n2. fit different supervised models on variations of the data,\n3. and compare results.\n\nThis tutorial is not meant to be exhaustive (references are provided throughout, and links to extra material are provided at the end).  \n\n### Authors:\n- Laurent Charlin <lcharlin@gmail.com>\n\n### Table of Content\n\n- [Section 0. Introduction](#introduction)\n- [Section 1. Data Pre-Processing](#pre-processing)\n- [Section 2. Modelling](#modelling)\n- [Section 3. Concluding Remarks](#concluding-remarks)\n\n\n\n<a id='introduction'></a>\n### Section 0. Introduction\nWe will use the example of a recommender system, i.e., a system which must recommend movies of interests to its users (e.g., Netflix). We will model user-movie preferences from a popular publicly available dataset (Movielens 1M). We will learn, from past user-movie ratings, to predict (missing/future) user-movie ratings from user socio-demographics and movie-tags data.\n\nMathematically, we are interested in learning the (parameters of the) following function:\n\n$$ r_{um} = f_\\theta(x_u, x_m)$$\nwhere\n- $u$ indexes users\n- $m$ indexes items\n- $r_{um}$ is u's rating for m (that user's preference) -- the dependent variable\n- $f_\\theta$ is some model parametrized by $\\theta$. For example, a linear regression with coefficients $\\theta$\n- $x_u$ are user u's covariates (e.g., age and occupation of this user)\n- $x_m$ are movie m's covariates (e.g., tags associated with the movie)\n\nThe function $f$ can take several forms (in other words, we can use a variety of models for this task). In today's tutorial we will assume that the problem is a regression one and we will experiment with several models ranging from a simple linear regression model to a more complicated two-hidden layer neural network.\n\n\n### Machine Learning terminology\n\nIt can be useful to think of machine learning as comprising three elements:\n1. Task (T)\n2. Experience (E)\n3. Performance measure (P).\n\n(a good description of these concepts is provided in [Ch. 5 of the Deep Learning Book](https://www.deeplearningbook.org/contents/ml.html))\n\nThe intuition is that the task (T) is \"the type of problem you are trying the solve\" (e.g., classification, regression, anomaly detection), the experience (E) is \"how your data comes about\" (e.g., does it come with labels or not, do you observe it all at once or as a stream), and the performance (P) is \"how well your model does\". Standard performance measures include accuracy and mean-squared error.  \n\nNote that the above terminology does not define the model used to learn (fit) the data nor does it define the fitting procedure (e.g., gradient descent).\n\nRelationship to the problem of rating prediction:\n- Task: Our task is to predict user-movie ratings. It can be modelled in different ways (more on this during week 11), but here we will model it as a regression problem.\n- Experience: The experience is a supervised learning one because we are predicting some dependent variable (rating) from a set of independent variables\n- Performance measure: We will be using the mean-squared error (MSE).\n\n### Setting up the data\n\nFor supervised learning, it is customary, to construct two data matrix $X$ and $Y$. The former, $X$, contains the covariates (features). It is a matrix of size $n \\times p$ with $n$ the number of examples and $p$ the dimensionality of each example (in other words the number of covariates associated with each example).  They are the input to the function.\n\n$$X = \\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\n\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\\nx_{n1} & x_{12} & \\ldots & x_{np} \\\\\n\\end{bmatrix}\n$$\n\nThe latter, $Y$, is a (column) vector of length $n$ which contains the labels (here ratings). $Y_1$ corresponds to the rating of $X_1$ (row  contains the labels (here ratings).\n\n$$\nY = \\begin{bmatrix}\nr_1 \\\\\nr_2 \\\\\n\\vdots \\\\\nr_n\n\\end{bmatrix}$$\n\nOf course, in a real problem we will differentiate the `train` and `test` sets, e.g., with $X_\\text{train}$ and $X_\\text{test}$. Same for the labels using, e.g., $Y_\\text{train}$ and $Y_\\text{test}$.\n\n## Start\n\nFollowing this brief introduction, we now dive into the problem.\n\nWe begin by importing the packages that we will need:\n- `reduce` function will come in handy to iteratively process data\n- `os` standard packages for performing system operations (e.g., opening files)\n- `re` package for regex\n- `sys` package to deal with system-level operations (here used to change the search path)\n- `time` package we will use to measure the duration of certain operations\n\n\n\n- `matplotlib` for plotting\n- `numpy` for linear-algebra computations\n- `pandas` for data wrangling\n- `sklearn` (scikit-learn) for machine learning models and useful machine learning related routines\n\n<a id='pre-processing'></a>\n# Section 1: Data Pre-Processing\n\nIn the following we load data from several csv files and pre-process it.\n\nWhile this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\n\n**I suggest that you read this section, but that you spend most of your time on the other sections. If you have time at the end, you can come back and do this more thoroughly.**\n\n#### Details\n\nWe will use the publically available [movielens dataset](https://grouplens.org/datasets/movielens/). The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the [ML-1M data](https://grouplens.org/datasets/movielens/1m/) (it contains 1M ratings) but we will also use movie tags from the [ML-20M dataset](https://grouplens.org/datasets/movielens/20m/) (20M ratings).\n\nExcept for downloading the dataset (to save you some time), I have not processed nor modified the data in any way.\n\n#### Load Movie Names/Genres\n\nWe begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format `MovieID::Name::Genres`.\n\nAfter loading into a pandas `dataFrame` structure, we will have movie names (`mName`), IDs (`mid`), and movie genres (`mGenres`)\n\nUsing pandas we can also search for movies by `mid` or by their name:\n\n#### Load Ratings\n\nUsing a similar routine as above, we load the ratings data which is in this format `UserID::MovieID::Rating::Timestamp`, and we will name the column UserID with `uid`, the column MovieID with `mid`, the rating with `rating`, and the time of the rating with `timestamp`.\n\n#### Load User Socio-Demographics Information\n\nThe file is in this format `UserID::Gender::Age::Occupation::Zip-code`, which we will load in a `dataFrame` with the following column names `uid,gender,age,occupation,zip`.\n\n\n\nFurther we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and >3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature.\n\n#### Load Movie Tags\n\nThe remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\n\nWe will load the csv data `movieId,tagId,relevance` into a `dataFrame` with the columns `mid,tid,relevance`.\n\nFrom above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12.\n\nWe also load the tag names. This will be useful for exploration purposes.\n\nSince we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies  in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (`mid`) are the same across these two datasets (i.e., not need for messy string match).\n\n(Note: Pandas' functionalities allow you to do operations similar to what you would do in SQL for relational databases.)\n\nWe lost a few movies compared to the original count of 3706 but we can live with that.\n\nNext, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie.\n\n#### Explore tags\n\nLet's get some understanding of how these tags are used. To help, we first build a `dataFrame` that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names).\n\nPandas' `merge` function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)\n\nSimilarly as above we can search for top movies according to a particular tag:\n\nThis next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data.\n\nIn the current dataset, every movie-tag id pair is a separate entry (row of the `dataFrame`). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example.\n\nTo do so, we re-encode `tid`s using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., \"cats\" and \"dogs\") which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag `scary` is id `882`) but tags cannot be compared using their numbers (e.g., tag `882` is not \"bigger\" than tag `880` or smaller than tag `900`). 1-of-K encoding deals with this by encoding each tag as a binary vector of length $K$ with a single non-zero value which corresponds to the tag. In the present case, $K=968$ tags, and tag `scary` would have a `1` at position `882`.\n\nBelow we see that our data now has 971 columns: 968 for tag ids, 1 for `mid`, and 1 for `relevance`, and 1 for the pandas index.\n\nWith this data we can then explore the distribution of movies per tag (and tags per movie below).\n\nIn this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (<5). On the other hand, the most popular tag was used to tag 210 movies.\n\nWe note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies.\n\nUsing the same recipe, we can do something similar for movies instead of tags\n\nIn this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags.\n\n#### Question 1\nWhat is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags?\n\nIn the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features.\n\nFor the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes.\n\nWe can have a look at our current dataset.\n\nNotice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags).\n\nWhile we used pandas to create dummies, `scikit-learn` has similar capacities. The `preprocessing` module is detailed [here](https://scikit-learn.org/stable/modules/preprocessing.html). You can also checkout the section on [Categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features).\n\n\nWe are ready to construct our first dataset. We will first use a subset of the columns (not including tags).\n\nBelow you will also note that we split our data into train and test using `train_test_split` from scikit-learn.\n\n*Recommender Systems note:* We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items).\n\n*Recommender Systems note #2:* Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings).\n\n<a id=\"modelling\"></a>\n# Section 2: Modelling\n\nRatings ([likert scale](https://en.wikipedia.org/wiki/Likert_scale)) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n\n$$ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2$$\n\nThe MSE can be understood as the average square distance between the predictor $f(x_i)$ and the target $y_i$. MSE returns a non-negative quantity and the perfect predictor has an MSE of $0$. If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\n\n*Train/Test:* Recall that while we estimate the parameters of the model using the **train** data, we evaluate the quality of the model (its performance) using **test** data.\n\n### 2.1 A first model: mean predictor\n\nIt is often helpful to use a very simple benchmark to compare against the performance of our models.\n\nOur initial benchmark is a model which simply predicts the mean (train) rating.\n\n*Recommender Systems note:* We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean.\n\nThe train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance).\n\nIn terms of absolute values these indicate that, on average, our predictions are 1.3 units ($\\sqrt{1.6}$) away from the true rating. This indicates that you shouldn't be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods.\n\n### 2.2 Linear regression\n\nFor our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users' gender, age, zip, and occupation. We fit this model\n$$\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n$$\n\n$\\theta_{1:6}$ are the parameters, $\\text{gender}_u$ stands for the gender of user $u$ and similarly for other covariates. Also, $x_{\\text{uid}_u}$ represents the identity of the user and similarly for $x_{\\text{mid}_i}$ and movies.\n\nNote that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so $\\theta_{\\text{zip}}$ has 100 dimensions.\n\nTraining this model involves minimizing the train MSE, this is exactly what the `LinearRegression` class does. (This is a [least-squares problem](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) and it can be solved in closed form.)\n\nWe note that train error $<<$ test error ($<<$ stands for \"much smaller\"). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it's a low bias and high variance model).\n\nDifferent methods can help prevent the model from overfitting, this is often referred to as *regularizing* the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: $$  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 $$\n\nInstead of the previous $ \\text{loss} := \\text{MSE} $.\n\nNote:\n- $||\\cdot||_2$ stands for the 2-Norm. That is, the square root of the sum of the operand's squared elements.\n- $\\alpha$ is a hyper-parameter which denotes the strength of the regularizer (if $\\alpha=0$ the regularizer vanishes and if $\\alpha=\\infty$ all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning $\\alpha$ along with the $\\theta$s would lead to a $\\alpha=0$).\n\nDuring learning the model must then tradeoff performance (MSE) and complexity (high $\\theta$s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the `Ridge` class from the `linear_model` package to fit regularized linear regression models.\n\n**Question 3** Explain why there are 7,184 parameters.\n\n**Hint:** Don't forget the intercept/bias term\n\nCompared to above, we see that with $\\alpha=10$ the train and test errors are much closer (i.e., there's less overfitting). Presumably different values of $\\alpha$ would yield different generalizations.\n\n\n**Question 4** How do you find the \"best\" value of $\\alpha$ for a given model and dataset?\n\n**Hint:** Have a look at the [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV), a cross-validation enabled version of Ridge.\n\n**Answer:** See below.\n\nTechnical remark: since the optimization is often done in log space, it's typical for the set of $\\alpha$'s to be powers of 10.\n\nThe advantage of doing cross validation (for example using `RidgeCV`) is clear. It automatically searches for the best values of hyperparameters (here $\\alpha$) from a given set (here $\\{ 1, 10, 100 \\}$).\n\nA validation set (or cross validation) should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement validation. In such cases you will likely need to split a separate validation set from your training data--in `sklearn` you can use the `train_test_split` function. It is typical for the validation set to be the same size as the test set.\n\nYou should also remember to **never select model hyperparameters based on performance on test set**, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models.\n\n***\n\nArmed with a good model we can explore the learned model including some of its predictions\n\nAbove we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n\n- This empirical distribution looks symmetrical so there doesn't seem to be a bias toward lower or higher predictions\n- The prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a \"triangle\" pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters ($\\theta_{\\text{mid}}$). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process.\n\n### 2.3 Linear regression with tag features/covariates\n\nWe use a linear regression model as above but also model movie tags:\n\n$$\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n$$\n\nThe last term on the right-hand side (bolded) is the only difference wrt to our previous model.  \n\n**Question 5:** How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance?\n\n**Hint:** One model is a special case of another.\n\nRemarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n\n***\n\n### 2.4 Fitting a non-linear model\n\nSo far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant.\n\nHere we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\n\n**Neural network basics:** (We will discuss these models in some depth over the next two weeks.)\n- A neural network is made up of interconnected *neurons*. Each neuron computes a few simple operations.\n- In a *feed-forward network*, neurons are organized into sets called *layers*.\n - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer.\n - The first layer is called the *input layer* it provides data to the next layer. The last layer is the *output layer* it provides a prediction $\\hat{y}$.\n - Layers in between the input layer and the output layer are called *hidden layers*. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (*activation function*): $f(x) = \\sum_i x_i \\theta_i$.\n  - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions).\n  - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers.\n\nMathematically for a regression task (with a single output), a one-hidden layer neural net is:\n$$\nf(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) )\n$$\nwhere\n- $\\theta_{ij}$ are the parameters of input $i$ and neuron $j$ in the hidden layer.\n- $f_h$ is the activation function of the hidden layer\n- $\\theta'_{j}$ are the parameters that connect the neuron $j$ in the hidden layer to the output layer.\n- $f_o$ is the activation function of the output layer\n\nAn intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:\n\nMuch like previous models we can regularize a neural net to combat overfitting:\n- Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by $\\alpha$.\n- In addition, we use a second regularizer called `early-stopping`. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In `scikit-learn`, the MLPRegressor class with `early_stopping=True` automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters.\n\n**Question 6:** Why does this model come with the possibility to set the random seed (i.e., `random_state`) while linear regression did not?\n\nHere is our updated table of results\n\n| Model        | Test MSE           |\n| ------------- |:-------------:|\n| 2.2 (Linear Reg. w. basic features)     | 1.031 | 0.865  |\n| 2.3 (2.2 + movie tags)     | 0.991 | 0.857 |\n| 2.4 (Neural Network w. features from 2.3) | 1.029 | 0.865 |\n\nAlthough neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better.\n\n<a id=\"concluding-remarks\"></a>\n### Section 3. Concluding Remarks\n\nThe goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it's coming from and how to model it.\n\nHere are a few more parting thoughts:\n\n#### Machine Learning\n\nAs you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n\n\n#### Scikit-learn\n`scikit-learn` is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\n\nModel Selection, i.e., which model should I use for a particular dataset/task can be daunting. [This page](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) provides some tips particular to `scikit-learn`. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\n\nNote that `scikit-learn` does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n\n#### Other packages\n\nSoftware is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. `scikit-learn` is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more.\n\n\n#### Food for thought\n - In our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don't have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be:\n\n$$\nf(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n$$\n\n\n- **Question 7:** What's wrong with the above model? Try to think about it for a minute or two before looking at the answer.\n\n\n\n- As we will see during week 11 (on recommender systems), many models take ratings as output **and** as input. For example, one could take a user's previous ratings and try to predict one's future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist).\n\n## References\n\n\nScikit-learn\n- [Documentation](https://scikit-learn.org/stable/documentation.html)\n- [Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n- [Help for model selection](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n","srcMarkdownNoYaml":"\n\n\n\n\nTutorial: An Introduction to Practical Machine Learning\n=============\n\n\nThis tutorial provides a short introduction to the practice of machine learning.\n\nWe assume that the user already has an understanding of the basic concepts that underlie the field. We review both methodological concepts like supervised learning and also use software libraries such as `scikit-learn`, `pandas`, and `numpy`.\n\nIn particular, we will:\n1. load some data,\n2. fit different supervised models on variations of the data,\n3. and compare results.\n\nThis tutorial is not meant to be exhaustive (references are provided throughout, and links to extra material are provided at the end).  \n\n### Authors:\n- Laurent Charlin <lcharlin@gmail.com>\n\n### Table of Content\n\n- [Section 0. Introduction](#introduction)\n- [Section 1. Data Pre-Processing](#pre-processing)\n- [Section 2. Modelling](#modelling)\n- [Section 3. Concluding Remarks](#concluding-remarks)\n\n\n\n<a id='introduction'></a>\n### Section 0. Introduction\nWe will use the example of a recommender system, i.e., a system which must recommend movies of interests to its users (e.g., Netflix). We will model user-movie preferences from a popular publicly available dataset (Movielens 1M). We will learn, from past user-movie ratings, to predict (missing/future) user-movie ratings from user socio-demographics and movie-tags data.\n\nMathematically, we are interested in learning the (parameters of the) following function:\n\n$$ r_{um} = f_\\theta(x_u, x_m)$$\nwhere\n- $u$ indexes users\n- $m$ indexes items\n- $r_{um}$ is u's rating for m (that user's preference) -- the dependent variable\n- $f_\\theta$ is some model parametrized by $\\theta$. For example, a linear regression with coefficients $\\theta$\n- $x_u$ are user u's covariates (e.g., age and occupation of this user)\n- $x_m$ are movie m's covariates (e.g., tags associated with the movie)\n\nThe function $f$ can take several forms (in other words, we can use a variety of models for this task). In today's tutorial we will assume that the problem is a regression one and we will experiment with several models ranging from a simple linear regression model to a more complicated two-hidden layer neural network.\n\n\n### Machine Learning terminology\n\nIt can be useful to think of machine learning as comprising three elements:\n1. Task (T)\n2. Experience (E)\n3. Performance measure (P).\n\n(a good description of these concepts is provided in [Ch. 5 of the Deep Learning Book](https://www.deeplearningbook.org/contents/ml.html))\n\nThe intuition is that the task (T) is \"the type of problem you are trying the solve\" (e.g., classification, regression, anomaly detection), the experience (E) is \"how your data comes about\" (e.g., does it come with labels or not, do you observe it all at once or as a stream), and the performance (P) is \"how well your model does\". Standard performance measures include accuracy and mean-squared error.  \n\nNote that the above terminology does not define the model used to learn (fit) the data nor does it define the fitting procedure (e.g., gradient descent).\n\nRelationship to the problem of rating prediction:\n- Task: Our task is to predict user-movie ratings. It can be modelled in different ways (more on this during week 11), but here we will model it as a regression problem.\n- Experience: The experience is a supervised learning one because we are predicting some dependent variable (rating) from a set of independent variables\n- Performance measure: We will be using the mean-squared error (MSE).\n\n### Setting up the data\n\nFor supervised learning, it is customary, to construct two data matrix $X$ and $Y$. The former, $X$, contains the covariates (features). It is a matrix of size $n \\times p$ with $n$ the number of examples and $p$ the dimensionality of each example (in other words the number of covariates associated with each example).  They are the input to the function.\n\n$$X = \\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\n\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\\nx_{n1} & x_{12} & \\ldots & x_{np} \\\\\n\\end{bmatrix}\n$$\n\nThe latter, $Y$, is a (column) vector of length $n$ which contains the labels (here ratings). $Y_1$ corresponds to the rating of $X_1$ (row  contains the labels (here ratings).\n\n$$\nY = \\begin{bmatrix}\nr_1 \\\\\nr_2 \\\\\n\\vdots \\\\\nr_n\n\\end{bmatrix}$$\n\nOf course, in a real problem we will differentiate the `train` and `test` sets, e.g., with $X_\\text{train}$ and $X_\\text{test}$. Same for the labels using, e.g., $Y_\\text{train}$ and $Y_\\text{test}$.\n\n## Start\n\nFollowing this brief introduction, we now dive into the problem.\n\nWe begin by importing the packages that we will need:\n- `reduce` function will come in handy to iteratively process data\n- `os` standard packages for performing system operations (e.g., opening files)\n- `re` package for regex\n- `sys` package to deal with system-level operations (here used to change the search path)\n- `time` package we will use to measure the duration of certain operations\n\n\n\n- `matplotlib` for plotting\n- `numpy` for linear-algebra computations\n- `pandas` for data wrangling\n- `sklearn` (scikit-learn) for machine learning models and useful machine learning related routines\n\n<a id='pre-processing'></a>\n# Section 1: Data Pre-Processing\n\nIn the following we load data from several csv files and pre-process it.\n\nWhile this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\n\n**I suggest that you read this section, but that you spend most of your time on the other sections. If you have time at the end, you can come back and do this more thoroughly.**\n\n#### Details\n\nWe will use the publically available [movielens dataset](https://grouplens.org/datasets/movielens/). The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the [ML-1M data](https://grouplens.org/datasets/movielens/1m/) (it contains 1M ratings) but we will also use movie tags from the [ML-20M dataset](https://grouplens.org/datasets/movielens/20m/) (20M ratings).\n\nExcept for downloading the dataset (to save you some time), I have not processed nor modified the data in any way.\n\n#### Load Movie Names/Genres\n\nWe begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format `MovieID::Name::Genres`.\n\nAfter loading into a pandas `dataFrame` structure, we will have movie names (`mName`), IDs (`mid`), and movie genres (`mGenres`)\n\nUsing pandas we can also search for movies by `mid` or by their name:\n\n#### Load Ratings\n\nUsing a similar routine as above, we load the ratings data which is in this format `UserID::MovieID::Rating::Timestamp`, and we will name the column UserID with `uid`, the column MovieID with `mid`, the rating with `rating`, and the time of the rating with `timestamp`.\n\n#### Load User Socio-Demographics Information\n\nThe file is in this format `UserID::Gender::Age::Occupation::Zip-code`, which we will load in a `dataFrame` with the following column names `uid,gender,age,occupation,zip`.\n\n\n\nFurther we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and >3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature.\n\n#### Load Movie Tags\n\nThe remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\n\nWe will load the csv data `movieId,tagId,relevance` into a `dataFrame` with the columns `mid,tid,relevance`.\n\nFrom above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12.\n\nWe also load the tag names. This will be useful for exploration purposes.\n\nSince we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies  in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (`mid`) are the same across these two datasets (i.e., not need for messy string match).\n\n(Note: Pandas' functionalities allow you to do operations similar to what you would do in SQL for relational databases.)\n\nWe lost a few movies compared to the original count of 3706 but we can live with that.\n\nNext, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie.\n\n#### Explore tags\n\nLet's get some understanding of how these tags are used. To help, we first build a `dataFrame` that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names).\n\nPandas' `merge` function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)\n\nSimilarly as above we can search for top movies according to a particular tag:\n\nThis next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data.\n\nIn the current dataset, every movie-tag id pair is a separate entry (row of the `dataFrame`). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example.\n\nTo do so, we re-encode `tid`s using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., \"cats\" and \"dogs\") which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag `scary` is id `882`) but tags cannot be compared using their numbers (e.g., tag `882` is not \"bigger\" than tag `880` or smaller than tag `900`). 1-of-K encoding deals with this by encoding each tag as a binary vector of length $K$ with a single non-zero value which corresponds to the tag. In the present case, $K=968$ tags, and tag `scary` would have a `1` at position `882`.\n\nBelow we see that our data now has 971 columns: 968 for tag ids, 1 for `mid`, and 1 for `relevance`, and 1 for the pandas index.\n\nWith this data we can then explore the distribution of movies per tag (and tags per movie below).\n\nIn this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (<5). On the other hand, the most popular tag was used to tag 210 movies.\n\nWe note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies.\n\nUsing the same recipe, we can do something similar for movies instead of tags\n\nIn this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags.\n\n#### Question 1\nWhat is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags?\n\nIn the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features.\n\nFor the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes.\n\nWe can have a look at our current dataset.\n\nNotice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags).\n\nWhile we used pandas to create dummies, `scikit-learn` has similar capacities. The `preprocessing` module is detailed [here](https://scikit-learn.org/stable/modules/preprocessing.html). You can also checkout the section on [Categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features).\n\n\nWe are ready to construct our first dataset. We will first use a subset of the columns (not including tags).\n\nBelow you will also note that we split our data into train and test using `train_test_split` from scikit-learn.\n\n*Recommender Systems note:* We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items).\n\n*Recommender Systems note #2:* Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings).\n\n<a id=\"modelling\"></a>\n# Section 2: Modelling\n\nRatings ([likert scale](https://en.wikipedia.org/wiki/Likert_scale)) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n\n$$ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2$$\n\nThe MSE can be understood as the average square distance between the predictor $f(x_i)$ and the target $y_i$. MSE returns a non-negative quantity and the perfect predictor has an MSE of $0$. If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\n\n*Train/Test:* Recall that while we estimate the parameters of the model using the **train** data, we evaluate the quality of the model (its performance) using **test** data.\n\n### 2.1 A first model: mean predictor\n\nIt is often helpful to use a very simple benchmark to compare against the performance of our models.\n\nOur initial benchmark is a model which simply predicts the mean (train) rating.\n\n*Recommender Systems note:* We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean.\n\nThe train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance).\n\nIn terms of absolute values these indicate that, on average, our predictions are 1.3 units ($\\sqrt{1.6}$) away from the true rating. This indicates that you shouldn't be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods.\n\n### 2.2 Linear regression\n\nFor our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users' gender, age, zip, and occupation. We fit this model\n$$\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n$$\n\n$\\theta_{1:6}$ are the parameters, $\\text{gender}_u$ stands for the gender of user $u$ and similarly for other covariates. Also, $x_{\\text{uid}_u}$ represents the identity of the user and similarly for $x_{\\text{mid}_i}$ and movies.\n\nNote that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so $\\theta_{\\text{zip}}$ has 100 dimensions.\n\nTraining this model involves minimizing the train MSE, this is exactly what the `LinearRegression` class does. (This is a [least-squares problem](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) and it can be solved in closed form.)\n\nWe note that train error $<<$ test error ($<<$ stands for \"much smaller\"). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it's a low bias and high variance model).\n\nDifferent methods can help prevent the model from overfitting, this is often referred to as *regularizing* the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: $$  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 $$\n\nInstead of the previous $ \\text{loss} := \\text{MSE} $.\n\nNote:\n- $||\\cdot||_2$ stands for the 2-Norm. That is, the square root of the sum of the operand's squared elements.\n- $\\alpha$ is a hyper-parameter which denotes the strength of the regularizer (if $\\alpha=0$ the regularizer vanishes and if $\\alpha=\\infty$ all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning $\\alpha$ along with the $\\theta$s would lead to a $\\alpha=0$).\n\nDuring learning the model must then tradeoff performance (MSE) and complexity (high $\\theta$s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the `Ridge` class from the `linear_model` package to fit regularized linear regression models.\n\n**Question 3** Explain why there are 7,184 parameters.\n\n**Hint:** Don't forget the intercept/bias term\n\nCompared to above, we see that with $\\alpha=10$ the train and test errors are much closer (i.e., there's less overfitting). Presumably different values of $\\alpha$ would yield different generalizations.\n\n\n**Question 4** How do you find the \"best\" value of $\\alpha$ for a given model and dataset?\n\n**Hint:** Have a look at the [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV), a cross-validation enabled version of Ridge.\n\n**Answer:** See below.\n\nTechnical remark: since the optimization is often done in log space, it's typical for the set of $\\alpha$'s to be powers of 10.\n\nThe advantage of doing cross validation (for example using `RidgeCV`) is clear. It automatically searches for the best values of hyperparameters (here $\\alpha$) from a given set (here $\\{ 1, 10, 100 \\}$).\n\nA validation set (or cross validation) should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement validation. In such cases you will likely need to split a separate validation set from your training data--in `sklearn` you can use the `train_test_split` function. It is typical for the validation set to be the same size as the test set.\n\nYou should also remember to **never select model hyperparameters based on performance on test set**, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models.\n\n***\n\nArmed with a good model we can explore the learned model including some of its predictions\n\nAbove we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n\n- This empirical distribution looks symmetrical so there doesn't seem to be a bias toward lower or higher predictions\n- The prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a \"triangle\" pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters ($\\theta_{\\text{mid}}$). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process.\n\n### 2.3 Linear regression with tag features/covariates\n\nWe use a linear regression model as above but also model movie tags:\n\n$$\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n$$\n\nThe last term on the right-hand side (bolded) is the only difference wrt to our previous model.  \n\n**Question 5:** How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance?\n\n**Hint:** One model is a special case of another.\n\nRemarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n\n***\n\n### 2.4 Fitting a non-linear model\n\nSo far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant.\n\nHere we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\n\n**Neural network basics:** (We will discuss these models in some depth over the next two weeks.)\n- A neural network is made up of interconnected *neurons*. Each neuron computes a few simple operations.\n- In a *feed-forward network*, neurons are organized into sets called *layers*.\n - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer.\n - The first layer is called the *input layer* it provides data to the next layer. The last layer is the *output layer* it provides a prediction $\\hat{y}$.\n - Layers in between the input layer and the output layer are called *hidden layers*. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (*activation function*): $f(x) = \\sum_i x_i \\theta_i$.\n  - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions).\n  - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers.\n\nMathematically for a regression task (with a single output), a one-hidden layer neural net is:\n$$\nf(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) )\n$$\nwhere\n- $\\theta_{ij}$ are the parameters of input $i$ and neuron $j$ in the hidden layer.\n- $f_h$ is the activation function of the hidden layer\n- $\\theta'_{j}$ are the parameters that connect the neuron $j$ in the hidden layer to the output layer.\n- $f_o$ is the activation function of the output layer\n\nAn intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:\n\nMuch like previous models we can regularize a neural net to combat overfitting:\n- Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by $\\alpha$.\n- In addition, we use a second regularizer called `early-stopping`. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In `scikit-learn`, the MLPRegressor class with `early_stopping=True` automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters.\n\n**Question 6:** Why does this model come with the possibility to set the random seed (i.e., `random_state`) while linear regression did not?\n\nHere is our updated table of results\n\n| Model        | Test MSE           |\n| ------------- |:-------------:|\n| 2.2 (Linear Reg. w. basic features)     | 1.031 | 0.865  |\n| 2.3 (2.2 + movie tags)     | 0.991 | 0.857 |\n| 2.4 (Neural Network w. features from 2.3) | 1.029 | 0.865 |\n\nAlthough neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better.\n\n<a id=\"concluding-remarks\"></a>\n### Section 3. Concluding Remarks\n\nThe goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it's coming from and how to model it.\n\nHere are a few more parting thoughts:\n\n#### Machine Learning\n\nAs you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n\n\n#### Scikit-learn\n`scikit-learn` is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\n\nModel Selection, i.e., which model should I use for a particular dataset/task can be daunting. [This page](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) provides some tips particular to `scikit-learn`. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\n\nNote that `scikit-learn` does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n\n#### Other packages\n\nSoftware is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. `scikit-learn` is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more.\n\n\n#### Food for thought\n - In our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don't have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be:\n\n$$\nf(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n$$\n\n\n- **Question 7:** What's wrong with the above model? Try to think about it for a minute or two before looking at the answer.\n\n\n\n- As we will see during week 11 (on recommender systems), many models take ratings as output **and** as input. For example, one could take a user's previous ratings and try to predict one's future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist).\n\n## References\n\n\nScikit-learn\n- [Documentation](https://scikit-learn.org/stable/documentation.html)\n- [Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n- [Help for model selection](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Introduction_to_ML.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"MATH60629A Fall 2022"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}