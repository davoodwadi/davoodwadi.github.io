{"title":"MATH60629A","markdown":{"yaml":{"title":"MATH60629A"},"headingText":"Week \\#5 - Neural Networks - Exercices","containsRefs":false,"markdown":"\n\n\n\n\nThis tutorial explores neural networks.\n\n\n\n## A tiny neural network classifier\n\nIn order to classify the examples, we will use the following simple neural network:\n\n<img src=\"https://github.com/lcharlin/80-629/blob/master/week5-NeuralNetworks/images/nn.png?raw=1\" width=\"700\">\n\nwhere $\\sigma$ is the sigmoid function defined as:\n\n$$\n    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n$$\n\n### Question 1\n\nAssume that the parameters of the neural network are as follows:\n\n\\begin{aligned}\n& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n& b_1 = 25 & b_2 = 40 && b_3 = -30\n\\end{aligned}\n\nWhat would be the predicted label for the following data points:\n\n | x1 | x2 | o | label |\n |-------|-------|-----|-------|\n | 4     | -4    |     |       |\n |-4     | 4     |     |       |\n | -4    | -4    |     |       |\n | 4     | 4     |     |       |\n\nYou can use the following piece of code to evaluate the output of the network:\n\n## Finding good parameters for our network\n\nLet's move to a slightly more realistic example. Here we focus on the task of (binary) classification. As always, we first load the data that we want to classify:\n\nYou can plot the data using the helper function `plot_data`:\n\nAs you can see, this data is not linearly separable. In other words, the positive and negative examples cannot be separated using a linear classifier. Our goal for the rest of this notebook-session is to learn the parameters of a neural-network model which can separate the positives from the negative examples.\n\nWhat do we mean by *learning the parameters*? Remember that our neural network has 9 parameters including three biases ($w_1, \\ldots, w_6, b_1, b_2, b_3$). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best.\n\nLet's see how different choices of parameters changes the classifier. For a given set of parameters, the function `plot_boundaries` shows the regions of positive prediction (coloured blue) and negative prediction (coloured red):\n\nNow let's project the plot of data on these decision boundaries:\n\nIt appears that the classifier obtained using the above set of parameters does not match our data. (Of course, this is to be expected. This classifier with fixed weights a priori has a high bias and a low variance.)\n\n### Question 2\nTry the alternatives below and see which one is a better match for our data:\n\nObviously, we need a better way than trial and error to find the best parameters. The way that we do this is by *minimizing a loss function*.\n\n## Loss function\n\nA *loss function* evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the *binary cross-entropy* loss. Let's represent our training data by the set $\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}$ and our neural network function by $f$. Then the binary cross-entropy loss function will be defined as:\n\n\\begin{equation}\n    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n\\end{equation}\n\nThe binary cross-entropy relates to the Bernoulli distribution (maximizing the Bernoulli likelihood is equivalent to minimizing the binary cross-entropy). **It is the loss function that should be used for binary classification problems. It can be generalized to multiclass classification problems, see [cross entropy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html).**\n\n### Question 3\nLet's see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of $X, f(X), y$ of those four examples are as follows:\n\n|X|f(X)|y|\n|:---|:---|:---|\n|(5.4, 1.6)|1|1|\n|(1.4, -0.5)|0.3679|1|\n|(3.5, -3)|0.8647|0|\n|(-3.5, 1.1)|0|0|\n\nCalculate the loss function using the equation above. You can calculate the *log* using this function:\n\nIt is important to remember that the loss function $l$ is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n\n\\begin{equation}\n    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n\\end{equation}\n\nIn principle, we want to find the set of parameters $\\mathbf{w}, \\mathbf{b}$ for which $\\ell(\\mathbf{w}, \\mathbf{b})$ has the smallest value. We will use *gradient descent* to find these values.\n\n### Minimization by gradient descent\n\nThe plot below shows the function $f(x_1, x_2) = x_1^2 + x_2^2$:\n\n<img src=\"https://github.com/lcharlin/80-629/blob/master/week5-NeuralNetworks/images/descent.png?raw=1\" width=\"700\">\n\n\n### Question 4\nPoint A on the plot has coordinates $(1, 1, 3)$. The blue vector AB shows the direction $(-1, -1)$, and the green vector AC shows the direction $(0, -1)$.\nAssume that we are at initial point $(1, 1)$ and we want to move in a direction that minimizes the function $f$. Which of these two directions moves faster towards the minimum: $(-1, -1)$ or $(0, -1)$?\n\n### Question 5\nCalculate the gradient of function $f$ in the point $(1, 1)$. How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?\n\n## Training the neural network\n\nWe now understand the theory of training neural networks. But how do we do this in practice? We will now develop our practical skills using the *scikit-learn* library to train our tiny network. Let's first define the network:\n\nThe argument `hidden_layer_sizes=(2,)` states that we only have one hidden layer with two neurons, and the argument `activation='logistic'` shows that we use the sigmoid activation function (Let's ignore the other arguments for now).\n\nWe will now train the network using our training data:\n\nOnce the network is trained, use the helper function `tiny_net_parameters` to get the parameters of the trained network (`tiny_net_parameters` is a wrapper around `clf.coefs_` and `clf.intercepts_`):\n\nThe learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data).\n\n---\n\nIn addition to the decision boundaries in the original data space, we can also visualize how the data are transformed through the neural networks. Since we use a hidden layer with two neurons, we can visualize its \"output\" in two dimensions.\n\nEn plus des frontières de décisions dans l'espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions.\n\n(for better visibility, we changed the color of the yellow class to blue.)\n\n## Tensorflow Playground questions\n\nWe will now investigate a few properties of neural networks using [tensorflow playground](https://playground.tensorflow.org/). Take a few minutes to familiarize yourself with the playground:\n\n- Change the number of hidden layers to one\n- Change the data distribution to *exclusive OR*\n- Push the *run* button and see how the network is trained\n- Stop training after epoch 500 (each epoch involves doing gradient descent using the complete dataset)\n- Hover over the neurons in the hidden layer and see the vizualization of their outputs.\n\n### Learning rate\n\nOpen [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=3&regularizationRate=0&noise=35&networkShape=1&seed=0.68448&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) example on tensorflow playground.\n\n- Push the *run* button and see the learning process for 500 epochs. What do you observe?\n- Stop training and press the *restart* button. Change the learning rate from 3 to 0.1, and press the *run* button again. What is different from the previous run?\n- Try these steps using three learning rates: 0.3, 0.03, and 0.003:\n + Press the *reset* button\n + Change the learning rate\n + Press the *step* button (located at the right of *run* button) a few times, and observe how the training/test loss changes in each step.\n\nWhich of those three rates would you use?\n\n### Regularization\n\nOpen [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=50&networkShape=4,4&seed=0.64895&showTestData=false&discretize=false&percTrainData=10&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) example on playground.\n\nLet's first observe a few things about this example. Check the box titled *Show test data*. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting.\n- Press the *run* button and let the training proceed for 500 epochs, then pause the training.\n- What do you think about the decision boundary of the classifier?\n- What causes the difference between the training error and test error? (Check the *Show test data* box again)\n- Write down the test error\n\nWe will now see how we can avoid overfitting using $L_2$ regularization.\n- Press the *restart* button\n- Change *regularization* from *None* to *L2*\n- Change *Regularization rate* from 0 to 0.3\n- Press the *run* button and run the model for 500 epochs\n- What is different from the previous setting?\n- Write down the test error\n\nJust like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003:\n- Press the *restart* button\n- Change *Regularization rate*\n- Press the *run* button and run the model for 500 epochs\n- Write down the test error\n\nWhich of these regularization rates would you use?\n","srcMarkdownNoYaml":"\n\n\n\n# Week \\#5 - Neural Networks - Exercices\n\nThis tutorial explores neural networks.\n\n\n\n## A tiny neural network classifier\n\nIn order to classify the examples, we will use the following simple neural network:\n\n<img src=\"https://github.com/lcharlin/80-629/blob/master/week5-NeuralNetworks/images/nn.png?raw=1\" width=\"700\">\n\nwhere $\\sigma$ is the sigmoid function defined as:\n\n$$\n    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n$$\n\n### Question 1\n\nAssume that the parameters of the neural network are as follows:\n\n\\begin{aligned}\n& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n& b_1 = 25 & b_2 = 40 && b_3 = -30\n\\end{aligned}\n\nWhat would be the predicted label for the following data points:\n\n | x1 | x2 | o | label |\n |-------|-------|-----|-------|\n | 4     | -4    |     |       |\n |-4     | 4     |     |       |\n | -4    | -4    |     |       |\n | 4     | 4     |     |       |\n\nYou can use the following piece of code to evaluate the output of the network:\n\n## Finding good parameters for our network\n\nLet's move to a slightly more realistic example. Here we focus on the task of (binary) classification. As always, we first load the data that we want to classify:\n\nYou can plot the data using the helper function `plot_data`:\n\nAs you can see, this data is not linearly separable. In other words, the positive and negative examples cannot be separated using a linear classifier. Our goal for the rest of this notebook-session is to learn the parameters of a neural-network model which can separate the positives from the negative examples.\n\nWhat do we mean by *learning the parameters*? Remember that our neural network has 9 parameters including three biases ($w_1, \\ldots, w_6, b_1, b_2, b_3$). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best.\n\nLet's see how different choices of parameters changes the classifier. For a given set of parameters, the function `plot_boundaries` shows the regions of positive prediction (coloured blue) and negative prediction (coloured red):\n\nNow let's project the plot of data on these decision boundaries:\n\nIt appears that the classifier obtained using the above set of parameters does not match our data. (Of course, this is to be expected. This classifier with fixed weights a priori has a high bias and a low variance.)\n\n### Question 2\nTry the alternatives below and see which one is a better match for our data:\n\nObviously, we need a better way than trial and error to find the best parameters. The way that we do this is by *minimizing a loss function*.\n\n## Loss function\n\nA *loss function* evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the *binary cross-entropy* loss. Let's represent our training data by the set $\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}$ and our neural network function by $f$. Then the binary cross-entropy loss function will be defined as:\n\n\\begin{equation}\n    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n\\end{equation}\n\nThe binary cross-entropy relates to the Bernoulli distribution (maximizing the Bernoulli likelihood is equivalent to minimizing the binary cross-entropy). **It is the loss function that should be used for binary classification problems. It can be generalized to multiclass classification problems, see [cross entropy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html).**\n\n### Question 3\nLet's see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of $X, f(X), y$ of those four examples are as follows:\n\n|X|f(X)|y|\n|:---|:---|:---|\n|(5.4, 1.6)|1|1|\n|(1.4, -0.5)|0.3679|1|\n|(3.5, -3)|0.8647|0|\n|(-3.5, 1.1)|0|0|\n\nCalculate the loss function using the equation above. You can calculate the *log* using this function:\n\nIt is important to remember that the loss function $l$ is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n\n\\begin{equation}\n    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n\\end{equation}\n\nIn principle, we want to find the set of parameters $\\mathbf{w}, \\mathbf{b}$ for which $\\ell(\\mathbf{w}, \\mathbf{b})$ has the smallest value. We will use *gradient descent* to find these values.\n\n### Minimization by gradient descent\n\nThe plot below shows the function $f(x_1, x_2) = x_1^2 + x_2^2$:\n\n<img src=\"https://github.com/lcharlin/80-629/blob/master/week5-NeuralNetworks/images/descent.png?raw=1\" width=\"700\">\n\n\n### Question 4\nPoint A on the plot has coordinates $(1, 1, 3)$. The blue vector AB shows the direction $(-1, -1)$, and the green vector AC shows the direction $(0, -1)$.\nAssume that we are at initial point $(1, 1)$ and we want to move in a direction that minimizes the function $f$. Which of these two directions moves faster towards the minimum: $(-1, -1)$ or $(0, -1)$?\n\n### Question 5\nCalculate the gradient of function $f$ in the point $(1, 1)$. How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?\n\n## Training the neural network\n\nWe now understand the theory of training neural networks. But how do we do this in practice? We will now develop our practical skills using the *scikit-learn* library to train our tiny network. Let's first define the network:\n\nThe argument `hidden_layer_sizes=(2,)` states that we only have one hidden layer with two neurons, and the argument `activation='logistic'` shows that we use the sigmoid activation function (Let's ignore the other arguments for now).\n\nWe will now train the network using our training data:\n\nOnce the network is trained, use the helper function `tiny_net_parameters` to get the parameters of the trained network (`tiny_net_parameters` is a wrapper around `clf.coefs_` and `clf.intercepts_`):\n\nThe learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data).\n\n---\n\nIn addition to the decision boundaries in the original data space, we can also visualize how the data are transformed through the neural networks. Since we use a hidden layer with two neurons, we can visualize its \"output\" in two dimensions.\n\nEn plus des frontières de décisions dans l'espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions.\n\n(for better visibility, we changed the color of the yellow class to blue.)\n\n## Tensorflow Playground questions\n\nWe will now investigate a few properties of neural networks using [tensorflow playground](https://playground.tensorflow.org/). Take a few minutes to familiarize yourself with the playground:\n\n- Change the number of hidden layers to one\n- Change the data distribution to *exclusive OR*\n- Push the *run* button and see how the network is trained\n- Stop training after epoch 500 (each epoch involves doing gradient descent using the complete dataset)\n- Hover over the neurons in the hidden layer and see the vizualization of their outputs.\n\n### Learning rate\n\nOpen [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=3&regularizationRate=0&noise=35&networkShape=1&seed=0.68448&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) example on tensorflow playground.\n\n- Push the *run* button and see the learning process for 500 epochs. What do you observe?\n- Stop training and press the *restart* button. Change the learning rate from 3 to 0.1, and press the *run* button again. What is different from the previous run?\n- Try these steps using three learning rates: 0.3, 0.03, and 0.003:\n + Press the *reset* button\n + Change the learning rate\n + Press the *step* button (located at the right of *run* button) a few times, and observe how the training/test loss changes in each step.\n\nWhich of those three rates would you use?\n\n### Regularization\n\nOpen [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=50&networkShape=4,4&seed=0.64895&showTestData=false&discretize=false&percTrainData=10&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) example on playground.\n\nLet's first observe a few things about this example. Check the box titled *Show test data*. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting.\n- Press the *run* button and let the training proceed for 500 epochs, then pause the training.\n- What do you think about the decision boundary of the classifier?\n- What causes the difference between the training error and test error? (Check the *Show test data* box again)\n- Write down the test error\n\nWe will now see how we can avoid overfitting using $L_2$ regularization.\n- Press the *restart* button\n- Change *regularization* from *None* to *L2*\n- Change *Regularization rate* from 0 to 0.3\n- Press the *run* button and run the model for 500 epochs\n- What is different from the previous setting?\n- Write down the test error\n\nJust like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003:\n- Press the *restart* button\n- Change *Regularization rate*\n- Press the *run* button and run the model for 500 epochs\n- Write down the test error\n\nWhich of these regularization rates would you use?\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Neural_Networks_questions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"MATH60629A"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}