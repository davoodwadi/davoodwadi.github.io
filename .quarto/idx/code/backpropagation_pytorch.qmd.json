{"title":"Backpropagation with PyTorch","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true},"eval":false},"headingText":"Backpropagation with PyTorch","containsRefs":false,"markdown":"\n\n\n\nIn this tutorial, we will learn about backpropagation and how to implement it using PyTorch. Backpropagation is a key algorithm for training neural networks. It allows us to compute gradients of the loss function with respect to the network's parameters, which in turn allows us to update the parameters and improve the model's performance.\n\n## What is Backpropagation?\n\nBackpropagation is a technique for training neural networks by computing the gradient of the loss function with respect to the parameters of the network. It is based on the chain rule of calculus, which allows us to compute the gradient of a function composed of multiple nested functions.\n\nLet's denote a neural network with parameters $\\theta$, and a loss function $J(\\theta)$. The backpropagation algorithm computes the gradient $\\nabla J(\\theta)$ with respect to the parameters by applying the chain rule, as follows:\n\n$$\n\\nabla J(\\theta) = \\frac{{\\partial J(\\theta)}}{{\\partial \\theta}} = \\frac{{\\partial J(\\theta)}}{{\\partial \\hat{y}}} \\cdot \\frac{{\\partial \\hat{y}}}{{\\partial h}} \\cdot \\frac{{\\partial h}}{{\\partial \\theta}}\n$$\n\nwhere $\\hat{y}$ is the predicted output of the neural network, $h$ is the pre-activation (weighted sum) of the final layer, and $\\theta$ represents the weights and biases of all the layers.\n\nTo update the parameters of the network, we typically use an optimization algorithm such as Stochastic Gradient Descent (SGD) or Adam, which leverages the computed gradients to perform parameter updates in the direction of minimizing the loss.\n\n## Implementing Backpropagation with PyTorch\n\nPyTorch is a popular deep learning library that provides an automatic differentiation engine, which can compute gradients of any function with respect to its input. This makes implementing backpropagation much easier.\n\nHere are the steps we will follow to implement backpropagation with PyTorch:\n\n1. Define the neural network structure.\n2. Define the loss function.\n3. Create an optimizer object.\n4. Write the training loop.\n\nLet's dive into the code implementation.\n\n## Step 1: Define the Neural Network Structure\n\nFirst, we need to define the structure of our neural network using the `torch.nn` module in PyTorch. We will create a simple feedforward neural network with two hidden layers.\n\n```{python}\nimport torch\nimport torch.nn as nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        return out\n```\n\nIn this code, we define a class `NeuralNetwork` which inherits from `nn.Module`. This allows us to use all of PyTorch's built-in capabilities for neural networks.\n\nWe define three fully connected layers (`nn.Linear`) with ReLU activation between them. ReLU is a commonly used activation function in neural networks.\n\nThe `forward` method defines the forward pass, which computes the output of the neural network given an input `x`.\n\n## Step 2: Define the Loss Function\n\nNext, we need to define the loss function that we will use to measure how well our neural network is performing. In this example, we will use the Mean Squared Error (MSE) loss.\n\n```{python}\ncriterion = nn.MSELoss()\n```\n\n## Step 3: Create an Optimizer Object\n\nTo update the parameters of the neural network during training, we need to use an optimizer. PyTorch provides various optimizers, such as SGD, Adam, and RMSprop.\n\nHere, we will use the Adam optimizer.\n\n```{python}\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```\n\n## Step 4: Write the Training Loop\n\nFinally, we can write the training loop. The training loop consists of the following steps:\n\n1. Compute the predicted output of the neural network.\n2. Compute the loss between the predicted output and the target output.\n3. Compute the gradients of the loss with respect to the network's parameters using the `backward` method.\n4. Update the parameters of the network using the optimizer's `step` method.\n5. Reset the gradients to zero.\n\n```{python}\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute loss\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\nIn this code snippet, `inputs` and `targets` represent the input and target output for each training example, respectively. We compute the predicted outputs using `model(inputs)`, and then compute the loss using `criterion(outputs, targets)`.\n\nWe zero out the gradients using `optimizer.zero_grad()` to avoid accumulating gradients from previous iterations. Then, we call `loss.backward()` to compute the gradients using backpropagation.\n\nFinally, we update the parameters of the network using `optimizer.step()` and repeat this process for each epoch.\n\nThat's it! You have now implemented backpropagation using PyTorch. By running this code and providing appropriate inputs and targets, you can train your neural network on your specific problem.\n\nRemember to preprocess your data, normalize it if necessary, split it into training and validation sets, and track the training and validation loss to monitor the progress of your neural network.\n\nI hope this tutorial was helpful in understanding backpropagation and its implementation with PyTorch. Happy learning!","srcMarkdownNoYaml":"\n\n\n# Backpropagation with PyTorch\n\nIn this tutorial, we will learn about backpropagation and how to implement it using PyTorch. Backpropagation is a key algorithm for training neural networks. It allows us to compute gradients of the loss function with respect to the network's parameters, which in turn allows us to update the parameters and improve the model's performance.\n\n## What is Backpropagation?\n\nBackpropagation is a technique for training neural networks by computing the gradient of the loss function with respect to the parameters of the network. It is based on the chain rule of calculus, which allows us to compute the gradient of a function composed of multiple nested functions.\n\nLet's denote a neural network with parameters $\\theta$, and a loss function $J(\\theta)$. The backpropagation algorithm computes the gradient $\\nabla J(\\theta)$ with respect to the parameters by applying the chain rule, as follows:\n\n$$\n\\nabla J(\\theta) = \\frac{{\\partial J(\\theta)}}{{\\partial \\theta}} = \\frac{{\\partial J(\\theta)}}{{\\partial \\hat{y}}} \\cdot \\frac{{\\partial \\hat{y}}}{{\\partial h}} \\cdot \\frac{{\\partial h}}{{\\partial \\theta}}\n$$\n\nwhere $\\hat{y}$ is the predicted output of the neural network, $h$ is the pre-activation (weighted sum) of the final layer, and $\\theta$ represents the weights and biases of all the layers.\n\nTo update the parameters of the network, we typically use an optimization algorithm such as Stochastic Gradient Descent (SGD) or Adam, which leverages the computed gradients to perform parameter updates in the direction of minimizing the loss.\n\n## Implementing Backpropagation with PyTorch\n\nPyTorch is a popular deep learning library that provides an automatic differentiation engine, which can compute gradients of any function with respect to its input. This makes implementing backpropagation much easier.\n\nHere are the steps we will follow to implement backpropagation with PyTorch:\n\n1. Define the neural network structure.\n2. Define the loss function.\n3. Create an optimizer object.\n4. Write the training loop.\n\nLet's dive into the code implementation.\n\n## Step 1: Define the Neural Network Structure\n\nFirst, we need to define the structure of our neural network using the `torch.nn` module in PyTorch. We will create a simple feedforward neural network with two hidden layers.\n\n```{python}\nimport torch\nimport torch.nn as nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        return out\n```\n\nIn this code, we define a class `NeuralNetwork` which inherits from `nn.Module`. This allows us to use all of PyTorch's built-in capabilities for neural networks.\n\nWe define three fully connected layers (`nn.Linear`) with ReLU activation between them. ReLU is a commonly used activation function in neural networks.\n\nThe `forward` method defines the forward pass, which computes the output of the neural network given an input `x`.\n\n## Step 2: Define the Loss Function\n\nNext, we need to define the loss function that we will use to measure how well our neural network is performing. In this example, we will use the Mean Squared Error (MSE) loss.\n\n```{python}\ncriterion = nn.MSELoss()\n```\n\n## Step 3: Create an Optimizer Object\n\nTo update the parameters of the neural network during training, we need to use an optimizer. PyTorch provides various optimizers, such as SGD, Adam, and RMSprop.\n\nHere, we will use the Adam optimizer.\n\n```{python}\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```\n\n## Step 4: Write the Training Loop\n\nFinally, we can write the training loop. The training loop consists of the following steps:\n\n1. Compute the predicted output of the neural network.\n2. Compute the loss between the predicted output and the target output.\n3. Compute the gradients of the loss with respect to the network's parameters using the `backward` method.\n4. Update the parameters of the network using the optimizer's `step` method.\n5. Reset the gradients to zero.\n\n```{python}\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute loss\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\nIn this code snippet, `inputs` and `targets` represent the input and target output for each training example, respectively. We compute the predicted outputs using `model(inputs)`, and then compute the loss using `criterion(outputs, targets)`.\n\nWe zero out the gradients using `optimizer.zero_grad()` to avoid accumulating gradients from previous iterations. Then, we call `loss.backward()` to compute the gradients using backpropagation.\n\nFinally, we update the parameters of the network using `optimizer.step()` and repeat this process for each epoch.\n\nThat's it! You have now implemented backpropagation using PyTorch. By running this code and providing appropriate inputs and targets, you can train your neural network on your specific problem.\n\nRemember to preprocess your data, normalize it if necessary, split it into training and validation sets, and track the training and validation loss to monitor the progress of your neural network.\n\nI hope this tutorial was helpful in understanding backpropagation and its implementation with PyTorch. Happy learning!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"backpropagation_pytorch.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}