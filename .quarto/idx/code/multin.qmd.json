{"title":"Multinomial Naive Bayes","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"headingText":"Multinomial Naive Bayes","containsRefs":false,"markdown":"\n\n\n\nIn machine learning, **Naive Bayes** is a probabilistic algorithm that is based on Bayes' theorem. It is commonly used for classification problems and is known for its simplicity and efficiency. In particular, **Multinomial Naive Bayes** is a variation of the Naive Bayes algorithm that is specifically designed for discrete features, such as word counts in text documents.\n\n## Bayes' Theorem\n\nTo understand how Multinomial Naive Bayes works, let's first review Bayes' theorem. Bayes' theorem provides a way to calculate conditional probabilities. It can be formulated as follows:\n\n$P(A|B) = \\frac{{P(B|A) P(A)}}{{P(B)}}$\n\nWhere:\n\n- $P(A|B)$ is the probability of event A occurring given that event B has occurred.\n- $P(B|A)$ is the probability of event B occurring given that event A has occurred.\n- $P(A)$ is the probability of event A occurring.\n- $P(B)$ is the probability of event B occurring.\n\n## Multinomial Naive Bayes\n\nMultinomial Naive Bayes is specifically designed for problems with discrete features. It assumes that the features are generated from a multinomial distribution and that the features are conditionally independent given the class label. This assumption simplifies the conditional probability calculation and makes the algorithm computationally efficient.\n\nThe formula for the multinomial distribution:\n\n$p(x_i | y) = \\frac{n_{yi} + \\alpha}{n_y + \\alpha n}$\n\nWhere\n\n- $n_{yi}$ is the number of times feature i appears in class y\n- $n_y$ is the number of time class y appears\n- $\\alpha>0$ is the smoothing prior, which accounts for features not present in the learning samples and prevents zero probabilities in further computations. \n  + Setting $\\alpha=1$ is called Laplace smoothing, while $\\alpha<1$ is called Lidstone smoothing.\n\nFor a comrehensive analysis of Naive Bayes algorithms, visit [this link](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes).\n\nHere's a step-by-step overview of how Multinomial Naive Bayes works:\n\n1. **Preparing the Dataset**: First, we need a dataset consisting of samples with features and corresponding class labels. The features should be discrete, such as word counts in text documents.\n2. **Feature Extraction**: Next, we need to extract features from the dataset. This can involve techniques like tokenization, stemming, and vectorization.\n3. **Training**: We then split the dataset into a training set and a test set. The training set is used to calculate the probabilities required for classification.\n4. **Calculating Class Prior Probabilities**: We calculate the prior probability of each class by counting the frequency of each class label in the training set.\n5. **Calculating Conditional Probabilities**: We calculate the conditional probability of each feature given the class label by counting the frequency of each feature in each class.\n6. **Classifying New Instances**: Finally, we use the calculated probabilities to classify new instances. For each new instance, we calculate the posterior probability of each class given the features and select the class with the highest probability as the predicted class.\n\nLet's now implement Multinomial Naive Bayes in Python using the **scikit-learn** library.\n\n## Implementation\n\nFirst, we need to import the necessary libraries:\n\n```{python}\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n```\n\nNext, let's create an example dataset consisting of text documents and corresponding class labels:\n\n```{python}\ndocuments = ['The sun is shining',\n             'The weather is beautiful',\n             'I enjoy going for walks',\n             'I hate rainy days']\n\nlabels = ['positive', 'positive', 'negative', 'negative']\n```\n\nNext, we need to split the dataset into a training set and a test set:\n\n```{python}\ndoc_train, doc_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\nprint(doc_train)\n```\n\nNow, let's create a **CountVectorizer** object to extract features from the text documents:\n\n```{python}\nvectorizer = CountVectorizer()\n```\n\nWe can then use the **fit_transform()** method of the vectorizer to transform the documents into a feature matrix:\n\n```{python}\nx_train = vectorizer.fit_transform(doc_train)\nprint(x_train.todense())\nprint(vectorizer.vocabulary_)\n\ni2w = {i:w for w,i in vectorizer.vocabulary_.items()}\ndoc = x_train.toarray().copy().astype(str)\nfor i in range(doc.shape[0]):\n  for j in range(doc.shape[1]):\n    if doc[i,j]=='1':\n      doc[i,j] = str(i2w[j])\nprint(doc)\n```\n\n\n\nNow, let's create a **MultinomialNB** object and train it on the training set:\n\n```{python}\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)\n```\n\nWe can then use the trained model to classify the instances in the test set:\n\n```{python}\nx_test = vectorizer.transform(doc_test)\ny_pred = model.predict(x_test)\nprint(doc_test)\nprint(x_test.todense())\ndoc = x_test.toarray().copy().astype(str)\nfor i in range(doc.shape[0]):\n  for j in range(doc.shape[1]):\n    if doc[i,j]=='1':\n      doc[i,j] = str(i2w[j])\nprint(doc)\nprint(y_pred)\n```\n\nFinally, let's calculate the accuracy of the model:\n\n```{python}\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n```\n\nWe have successfully trained a Multinomial Naive Bayes classifier and used it to classify new instances.","srcMarkdownNoYaml":"\n\n\n# Multinomial Naive Bayes\n\nIn machine learning, **Naive Bayes** is a probabilistic algorithm that is based on Bayes' theorem. It is commonly used for classification problems and is known for its simplicity and efficiency. In particular, **Multinomial Naive Bayes** is a variation of the Naive Bayes algorithm that is specifically designed for discrete features, such as word counts in text documents.\n\n## Bayes' Theorem\n\nTo understand how Multinomial Naive Bayes works, let's first review Bayes' theorem. Bayes' theorem provides a way to calculate conditional probabilities. It can be formulated as follows:\n\n$P(A|B) = \\frac{{P(B|A) P(A)}}{{P(B)}}$\n\nWhere:\n\n- $P(A|B)$ is the probability of event A occurring given that event B has occurred.\n- $P(B|A)$ is the probability of event B occurring given that event A has occurred.\n- $P(A)$ is the probability of event A occurring.\n- $P(B)$ is the probability of event B occurring.\n\n## Multinomial Naive Bayes\n\nMultinomial Naive Bayes is specifically designed for problems with discrete features. It assumes that the features are generated from a multinomial distribution and that the features are conditionally independent given the class label. This assumption simplifies the conditional probability calculation and makes the algorithm computationally efficient.\n\nThe formula for the multinomial distribution:\n\n$p(x_i | y) = \\frac{n_{yi} + \\alpha}{n_y + \\alpha n}$\n\nWhere\n\n- $n_{yi}$ is the number of times feature i appears in class y\n- $n_y$ is the number of time class y appears\n- $\\alpha>0$ is the smoothing prior, which accounts for features not present in the learning samples and prevents zero probabilities in further computations. \n  + Setting $\\alpha=1$ is called Laplace smoothing, while $\\alpha<1$ is called Lidstone smoothing.\n\nFor a comrehensive analysis of Naive Bayes algorithms, visit [this link](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes).\n\nHere's a step-by-step overview of how Multinomial Naive Bayes works:\n\n1. **Preparing the Dataset**: First, we need a dataset consisting of samples with features and corresponding class labels. The features should be discrete, such as word counts in text documents.\n2. **Feature Extraction**: Next, we need to extract features from the dataset. This can involve techniques like tokenization, stemming, and vectorization.\n3. **Training**: We then split the dataset into a training set and a test set. The training set is used to calculate the probabilities required for classification.\n4. **Calculating Class Prior Probabilities**: We calculate the prior probability of each class by counting the frequency of each class label in the training set.\n5. **Calculating Conditional Probabilities**: We calculate the conditional probability of each feature given the class label by counting the frequency of each feature in each class.\n6. **Classifying New Instances**: Finally, we use the calculated probabilities to classify new instances. For each new instance, we calculate the posterior probability of each class given the features and select the class with the highest probability as the predicted class.\n\nLet's now implement Multinomial Naive Bayes in Python using the **scikit-learn** library.\n\n## Implementation\n\nFirst, we need to import the necessary libraries:\n\n```{python}\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n```\n\nNext, let's create an example dataset consisting of text documents and corresponding class labels:\n\n```{python}\ndocuments = ['The sun is shining',\n             'The weather is beautiful',\n             'I enjoy going for walks',\n             'I hate rainy days']\n\nlabels = ['positive', 'positive', 'negative', 'negative']\n```\n\nNext, we need to split the dataset into a training set and a test set:\n\n```{python}\ndoc_train, doc_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\nprint(doc_train)\n```\n\nNow, let's create a **CountVectorizer** object to extract features from the text documents:\n\n```{python}\nvectorizer = CountVectorizer()\n```\n\nWe can then use the **fit_transform()** method of the vectorizer to transform the documents into a feature matrix:\n\n```{python}\nx_train = vectorizer.fit_transform(doc_train)\nprint(x_train.todense())\nprint(vectorizer.vocabulary_)\n\ni2w = {i:w for w,i in vectorizer.vocabulary_.items()}\ndoc = x_train.toarray().copy().astype(str)\nfor i in range(doc.shape[0]):\n  for j in range(doc.shape[1]):\n    if doc[i,j]=='1':\n      doc[i,j] = str(i2w[j])\nprint(doc)\n```\n\n\n\nNow, let's create a **MultinomialNB** object and train it on the training set:\n\n```{python}\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)\n```\n\nWe can then use the trained model to classify the instances in the test set:\n\n```{python}\nx_test = vectorizer.transform(doc_test)\ny_pred = model.predict(x_test)\nprint(doc_test)\nprint(x_test.todense())\ndoc = x_test.toarray().copy().astype(str)\nfor i in range(doc.shape[0]):\n  for j in range(doc.shape[1]):\n    if doc[i,j]=='1':\n      doc[i,j] = str(i2w[j])\nprint(doc)\nprint(y_pred)\n```\n\nFinally, let's calculate the accuracy of the model:\n\n```{python}\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n```\n\nWe have successfully trained a Multinomial Naive Bayes classifier and used it to classify new instances."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"multin.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}