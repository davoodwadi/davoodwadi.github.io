{"markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"containsRefs":false,"markdown":"\n\n\nTo demonstrate the implementation of Multilayer Perceptron (MLP) using Pytorch, we will use the classic Iris dataset. MLP is a type of artificial neural network that is widely used in machine learning for classification tasks. In this tutorial, we will build a simple MLP model to classify the Iris flowers into different species.\n\nBut first, let's import the necessary libraries:\n\n```{python}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n```\n\nNow let's load the Iris dataset and split it into training and testing sets:\n\n```{python}\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nIn the code above, we first import the required libraries: `torch` for building and training the MLP model, `torch.nn` for defining the network architecture, `torch.optim` for optimizing the model parameters, `sklearn.datasets` for loading the Iris dataset, and `sklearn.model_selection` for splitting the dataset into training and testing sets.\n\nNext, we load the Iris dataset using `load_iris()` function and assign the feature data to `X` and the target labels to `y`. Then, we split the dataset into training and testing sets using `train_test_split()` function, where we specify the test size as 0.2 (20% of the data) and set the random state for reproducibility.\n\nNow, let's define our MLP model:\n\n```{python}\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n```\n\nIn the code above, we define a class `MLP` inheriting from `nn.Module`. The constructor (`__init__()`) takes three arguments: `input_size` which is the number of input features, `hidden_size` which is the number of neurons in the hidden layer, and `num_classes` which is the number of output classes. Inside the constructor, we define the layers of our MLP model using `nn.Linear` (fully connected) and `nn.ReLU` (rectified linear unit) activation function.\n\nThe `forward()` method defines the forward pass of the model. It takes an input `x`, passes it through the layers, and returns the output.\n\nNow let's create an instance of our MLP model:\n\n```{python}\ninput_size = X.shape[1]\nhidden_size = 64\nnum_classes = len(set(y))\n\nmodel = MLP(input_size, hidden_size, num_classes)\n```\n\nIn the code above, we set `input_size` as the number of features in the input data (`X.shape[1]`), `hidden_size` as 64 (you can change this value to experiment), and `num_classes` as the number of unique classes in the target labels (`len(set(y))`).\n\nNow let's define the loss function and optimizer:\n\n```{python}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n\nWe use `nn.CrossEntropyLoss()` as our loss function, which is suitable for multi-class classification problems. We also define the optimizer using `optim.Adam` and pass the model parameters (`model.parameters()`) along with the learning rate (`lr=0.001`).\n\nNext, let's train the MLP model:\n\n```{python}\nnum_epochs = 100\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    for i in range(0, X_train.shape[0], batch_size):\n        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n        batch_y = torch.LongTensor(y_train[i:i+batch_size])\n        \n        # Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n```\n\nIn the code above, we define the number of epochs (`num_epochs`) and the batch size (`batch_size`) for training. We iterate over each epoch and split the training data into batches. For each batch, we convert the numpy arrays to tensors using `torch.FloatTensor` and `torch.LongTensor` for the input features and target labels respectively.\n\nIn the forward pass, we pass the input batch (`batch_X`) through the model and compute the predicted outputs. We then calculate the loss between the predicted outputs and the target labels using the defined loss function.\n\nIn the backward pass, we zero the gradients with `optimizer.zero_grad()`, compute the gradients of the loss with respect to the model parameters using `loss.backward()`, and update the model parameters using the optimizer with `optimizer.step()`.\n\nFinally, we print the loss value every 10 epochs for tracking the training progress.\n\nAfter training, let's evaluate the MLP model on the test set:\n\n```{python}\nwith torch.no_grad():\n    outputs = model(torch.FloatTensor(X_test))\n    _, predicted = torch.max(outputs.data, 1)\n\naccuracy = (predicted == torch.LongTensor(y_test)).sum().item() / len(y_test)\nprint(f'Accuracy: {accuracy:.4f}')\n```\n\nIn the code above, we use `torch.no_grad()` to turn off gradient calculation and save memory. We pass the test features (`X_test`) through the trained model and obtain the predicted outputs. We then use `torch.max()` to get the class labels with the highest probability and compare them with the ground truth labels (`y_test`) to calculate the accuracy.\n\nThat's it! You have successfully implemented an MLP model using Pytorch for classification on the Iris dataset.","srcMarkdownNoYaml":"\n\n\nTo demonstrate the implementation of Multilayer Perceptron (MLP) using Pytorch, we will use the classic Iris dataset. MLP is a type of artificial neural network that is widely used in machine learning for classification tasks. In this tutorial, we will build a simple MLP model to classify the Iris flowers into different species.\n\nBut first, let's import the necessary libraries:\n\n```{python}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n```\n\nNow let's load the Iris dataset and split it into training and testing sets:\n\n```{python}\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nIn the code above, we first import the required libraries: `torch` for building and training the MLP model, `torch.nn` for defining the network architecture, `torch.optim` for optimizing the model parameters, `sklearn.datasets` for loading the Iris dataset, and `sklearn.model_selection` for splitting the dataset into training and testing sets.\n\nNext, we load the Iris dataset using `load_iris()` function and assign the feature data to `X` and the target labels to `y`. Then, we split the dataset into training and testing sets using `train_test_split()` function, where we specify the test size as 0.2 (20% of the data) and set the random state for reproducibility.\n\nNow, let's define our MLP model:\n\n```{python}\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n```\n\nIn the code above, we define a class `MLP` inheriting from `nn.Module`. The constructor (`__init__()`) takes three arguments: `input_size` which is the number of input features, `hidden_size` which is the number of neurons in the hidden layer, and `num_classes` which is the number of output classes. Inside the constructor, we define the layers of our MLP model using `nn.Linear` (fully connected) and `nn.ReLU` (rectified linear unit) activation function.\n\nThe `forward()` method defines the forward pass of the model. It takes an input `x`, passes it through the layers, and returns the output.\n\nNow let's create an instance of our MLP model:\n\n```{python}\ninput_size = X.shape[1]\nhidden_size = 64\nnum_classes = len(set(y))\n\nmodel = MLP(input_size, hidden_size, num_classes)\n```\n\nIn the code above, we set `input_size` as the number of features in the input data (`X.shape[1]`), `hidden_size` as 64 (you can change this value to experiment), and `num_classes` as the number of unique classes in the target labels (`len(set(y))`).\n\nNow let's define the loss function and optimizer:\n\n```{python}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n\nWe use `nn.CrossEntropyLoss()` as our loss function, which is suitable for multi-class classification problems. We also define the optimizer using `optim.Adam` and pass the model parameters (`model.parameters()`) along with the learning rate (`lr=0.001`).\n\nNext, let's train the MLP model:\n\n```{python}\nnum_epochs = 100\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    for i in range(0, X_train.shape[0], batch_size):\n        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n        batch_y = torch.LongTensor(y_train[i:i+batch_size])\n        \n        # Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n```\n\nIn the code above, we define the number of epochs (`num_epochs`) and the batch size (`batch_size`) for training. We iterate over each epoch and split the training data into batches. For each batch, we convert the numpy arrays to tensors using `torch.FloatTensor` and `torch.LongTensor` for the input features and target labels respectively.\n\nIn the forward pass, we pass the input batch (`batch_X`) through the model and compute the predicted outputs. We then calculate the loss between the predicted outputs and the target labels using the defined loss function.\n\nIn the backward pass, we zero the gradients with `optimizer.zero_grad()`, compute the gradients of the loss with respect to the model parameters using `loss.backward()`, and update the model parameters using the optimizer with `optimizer.step()`.\n\nFinally, we print the loss value every 10 epochs for tracking the training progress.\n\nAfter training, let's evaluate the MLP model on the test set:\n\n```{python}\nwith torch.no_grad():\n    outputs = model(torch.FloatTensor(X_test))\n    _, predicted = torch.max(outputs.data, 1)\n\naccuracy = (predicted == torch.LongTensor(y_test)).sum().item() / len(y_test)\nprint(f'Accuracy: {accuracy:.4f}')\n```\n\nIn the code above, we use `torch.no_grad()` to turn off gradient calculation and save memory. We pass the test features (`X_test`) through the trained model and obtain the predicted outputs. We then use `torch.max()` to get the class labels with the highest probability and compare them with the ground truth labels (`y_test`) to calculate the accuracy.\n\nThat's it! You have successfully implemented an MLP model using Pytorch for classification on the Iris dataset."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"MLP_pytorch.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}