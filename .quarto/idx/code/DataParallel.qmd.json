{"markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true},"eval":false},"containsRefs":false,"markdown":"\n\n\nTo train the CIFAR-10 dataset using PyTorch's DataParallel, we first need to have a basic understanding of what CIFAR-10 is and what PyTorch's DataParallel does.\n\nCIFAR-10 is a popular computer vision dataset that consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.\n\nPyTorch's DataParallel is a module that allows us to use multiple GPUs for training our models. It splits the input data across the GPUs, performs forward and backward passes on each GPU, and then aggregates the gradients to update the model parameters. This helps in speeding up the training process and achieving better performance.\n\nNow, let's dive into the code.\n\nFirst, we need to import the necessary libraries and modules:\n\n```{python}\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n```\n\nNext, we define the CIFAR-10 dataset and its transformations:\n\n```{python}\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n                                             download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n                                            download=True, transform=transform)\n```\n\nThen, we define the data loaders for our training and testing datasets:\n\n```{python}\nbatch_size = 128\nnum_workers = 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          shuffle=True, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, \n                         shuffle=False, num_workers=num_workers)\n```\n\nNow, let's define our model:\n\n```{python}\nclass CIFAR10Net(nn.Module):\n    def __init__(self):\n        super(CIFAR10Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout(x)\n        x = x.view(-1, 64 * 16 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CIFAR10Net()\n```\n\nAfter defining our model, we need to define the loss function and the optimizer:\n\n```{python}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\nNow, we are ready to train our model:\n\n```{python}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    \n    for input, target in train_loader:\n        input, target = input.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for input, target in test_loader:\n            input, target = input.to(device), target.to(device)\n            \n            output = model(input)\n            _, predictions = torch.max(output, 1)\n            \n            total_correct += (predictions == target).sum().item()\n            total_samples += target.size(0)\n    \n    accuracy = total_correct / total_samples\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}\")\n```\n\nFinally, we can evaluate our model on the test set:\n\n```{python}\nmodel.eval()\n\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for input, target in test_loader:\n        input, target = input.to(device), target.to(device)\n\n        output = model(input)\n        _, predictions = torch.max(output, 1)\n\n        total_correct += (predictions == target).sum().item()\n        total_samples += target.size(0)\n\naccuracy = total_correct / total_samples\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n```\n\nThat's it! We have successfully trained the CIFAR-10 dataset using PyTorch's DataParallel module.","srcMarkdownNoYaml":"\n\n\nTo train the CIFAR-10 dataset using PyTorch's DataParallel, we first need to have a basic understanding of what CIFAR-10 is and what PyTorch's DataParallel does.\n\nCIFAR-10 is a popular computer vision dataset that consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.\n\nPyTorch's DataParallel is a module that allows us to use multiple GPUs for training our models. It splits the input data across the GPUs, performs forward and backward passes on each GPU, and then aggregates the gradients to update the model parameters. This helps in speeding up the training process and achieving better performance.\n\nNow, let's dive into the code.\n\nFirst, we need to import the necessary libraries and modules:\n\n```{python}\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n```\n\nNext, we define the CIFAR-10 dataset and its transformations:\n\n```{python}\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n                                             download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n                                            download=True, transform=transform)\n```\n\nThen, we define the data loaders for our training and testing datasets:\n\n```{python}\nbatch_size = 128\nnum_workers = 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          shuffle=True, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, \n                         shuffle=False, num_workers=num_workers)\n```\n\nNow, let's define our model:\n\n```{python}\nclass CIFAR10Net(nn.Module):\n    def __init__(self):\n        super(CIFAR10Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout(x)\n        x = x.view(-1, 64 * 16 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CIFAR10Net()\n```\n\nAfter defining our model, we need to define the loss function and the optimizer:\n\n```{python}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n\nNow, we are ready to train our model:\n\n```{python}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    \n    for input, target in train_loader:\n        input, target = input.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for input, target in test_loader:\n            input, target = input.to(device), target.to(device)\n            \n            output = model(input)\n            _, predictions = torch.max(output, 1)\n            \n            total_correct += (predictions == target).sum().item()\n            total_samples += target.size(0)\n    \n    accuracy = total_correct / total_samples\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}\")\n```\n\nFinally, we can evaluate our model on the test set:\n\n```{python}\nmodel.eval()\n\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for input, target in test_loader:\n        input, target = input.to(device), target.to(device)\n\n        output = model(input)\n        _, predictions = torch.max(output, 1)\n\n        total_correct += (predictions == target).sum().item()\n        total_samples += target.size(0)\n\naccuracy = total_correct / total_samples\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n```\n\nThat's it! We have successfully trained the CIFAR-10 dataset using PyTorch's DataParallel module."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"DataParallel.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}