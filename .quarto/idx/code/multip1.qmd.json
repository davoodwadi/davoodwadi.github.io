{"title":"Multiple Linear Regression - Part 1","markdown":{"yaml":{"title":"Multiple Linear Regression - Part 1","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"headingText":"Design Matrix","containsRefs":false,"markdown":"\n\nIn machine learning and statistics, multiple linear regression is a commonly used technique to model the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the independent variables and the dependent variable.\n\nThe general form of multiple linear regression can be written as:\n\n$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon$$\n\nWhere:\n\n- $y$ is the dependent variable,\n- $x_1, x_2, ..., x_p$ are the independent variables,\n- $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p$ are the coefficients,\n- $\\epsilon$ is the noise term.\n\nTo perform multiple linear regression, we can use the Ordinary Least Squares (OLS) method, which aims to find the coefficients that minimize the sum of the squared differences between the actual and predicted values.\n\n\nIn multiple linear regression, we represent the independent variables as a matrix called the design matrix. The design matrix, denoted by $X$, has one row for each observation and one column for each independent variable.\n\nFor example, if we have n observations and p independent variables, the design matrix $X$ will be an n x p matrix.\n\n## Proof of OLS\n\nThe OLS method minimizes the sum of squared errors (SSE) between the actual dependent variable values and the predicted values. The SSE can be written as:\n\n$$SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n\nWhere:\n\n- $y_i$ is the actual dependent variable value for the i-th observation,\n- $\\hat{y}_i$ is the predicted dependent variable value for the i-th observation.\n\nTo find the coefficients that minimize SSE, we differentiate SSE with respect to each coefficient and set the derivatives to zero.\n\nLet's derive the formula for OLS step by step.\n\n1. The predicted value of the dependent variable can be written as:\n\n$$\\hat{y} = X\\beta$$\n\nWhere:\n\n- $\\hat{y}$ is an n x 1 vector of predicted values,\n- $X$ is the design matrix,\n- $\\beta$ is a vector of coefficients.\n\n2. The SSE can be expressed in matrix form as:\n\n$$SSE = (\\mathbf{y} - \\mathbf{\\hat{y}})^T (\\mathbf{y} - \\mathbf{\\hat{y}})$$\n\nWhere:\n\n- $\\mathbf{y}$ is an n x 1 vector of actual dependent variable values,\n- $\\mathbf{\\hat{y}}$ is an n x 1 vector of predicted dependent variable values.\n\n3. Expanding the above equation, we get:\n\n$$SSE = (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)$$\n\n4. Expanding the squared term, we get:\n\n$$SSE = \\mathbf{y}^T\\mathbf{y} - 2\\beta^TX^T\\mathbf{y} + \\beta^TX^TX\\beta$$\n\n5. To minimize SSE with respect to $\\beta$, we differentiate SSE with respect to $\\beta$ and set the derivative to zero:\n\n$$\\frac{\\partial SSE}{\\partial \\beta} = -2X^T\\mathbf{y} + 2X^TX\\beta = 0$$\n\n6. Solving for $\\beta$, we get:\n\n$$X^TX\\beta = X^T\\mathbf{y}$$\n\n$$\\beta = (X^TX)^{-1}X^T\\mathbf{y}$$\n\nThe above formula gives us the optimal values for $\\beta$ that minimize SSE.","srcMarkdownNoYaml":"\n\nIn machine learning and statistics, multiple linear regression is a commonly used technique to model the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the independent variables and the dependent variable.\n\nThe general form of multiple linear regression can be written as:\n\n$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon$$\n\nWhere:\n\n- $y$ is the dependent variable,\n- $x_1, x_2, ..., x_p$ are the independent variables,\n- $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p$ are the coefficients,\n- $\\epsilon$ is the noise term.\n\nTo perform multiple linear regression, we can use the Ordinary Least Squares (OLS) method, which aims to find the coefficients that minimize the sum of the squared differences between the actual and predicted values.\n\n## Design Matrix\n\nIn multiple linear regression, we represent the independent variables as a matrix called the design matrix. The design matrix, denoted by $X$, has one row for each observation and one column for each independent variable.\n\nFor example, if we have n observations and p independent variables, the design matrix $X$ will be an n x p matrix.\n\n## Proof of OLS\n\nThe OLS method minimizes the sum of squared errors (SSE) between the actual dependent variable values and the predicted values. The SSE can be written as:\n\n$$SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n\nWhere:\n\n- $y_i$ is the actual dependent variable value for the i-th observation,\n- $\\hat{y}_i$ is the predicted dependent variable value for the i-th observation.\n\nTo find the coefficients that minimize SSE, we differentiate SSE with respect to each coefficient and set the derivatives to zero.\n\nLet's derive the formula for OLS step by step.\n\n1. The predicted value of the dependent variable can be written as:\n\n$$\\hat{y} = X\\beta$$\n\nWhere:\n\n- $\\hat{y}$ is an n x 1 vector of predicted values,\n- $X$ is the design matrix,\n- $\\beta$ is a vector of coefficients.\n\n2. The SSE can be expressed in matrix form as:\n\n$$SSE = (\\mathbf{y} - \\mathbf{\\hat{y}})^T (\\mathbf{y} - \\mathbf{\\hat{y}})$$\n\nWhere:\n\n- $\\mathbf{y}$ is an n x 1 vector of actual dependent variable values,\n- $\\mathbf{\\hat{y}}$ is an n x 1 vector of predicted dependent variable values.\n\n3. Expanding the above equation, we get:\n\n$$SSE = (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)$$\n\n4. Expanding the squared term, we get:\n\n$$SSE = \\mathbf{y}^T\\mathbf{y} - 2\\beta^TX^T\\mathbf{y} + \\beta^TX^TX\\beta$$\n\n5. To minimize SSE with respect to $\\beta$, we differentiate SSE with respect to $\\beta$ and set the derivative to zero:\n\n$$\\frac{\\partial SSE}{\\partial \\beta} = -2X^T\\mathbf{y} + 2X^TX\\beta = 0$$\n\n6. Solving for $\\beta$, we get:\n\n$$X^TX\\beta = X^T\\mathbf{y}$$\n\n$$\\beta = (X^TX)^{-1}X^T\\mathbf{y}$$\n\nThe above formula gives us the optimal values for $\\beta$ that minimize SSE."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"multip1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"Multiple Linear Regression - Part 1","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}