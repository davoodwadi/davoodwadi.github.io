{"title":"L2 Regularization","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"headingText":"L2 Regularization","containsRefs":false,"markdown":"\n\n\nIn multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\n\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\n\nwhere:\n\n- $y$ is the dependent variable (target variable)\n- $X$ is the design matrix that consists of independent variables (features)\n- $\\beta$ is the vector of coefficients (slopes) for each feature in X\n- $\\epsilon$ is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients $\\beta$ that minimize the sum of squared residuals.\n\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\n\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter $\\lambda$.\n\nThe loss function for multiple linear regression with L2 regularization is given by:\n\n\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\n\nwhere:\n\n- $N$ is the number of samples in the dataset\n- $y_i$ is the target value for the i-th sample\n- $X_i$ is the feature vector for the i-th sample\n- $\\|\\beta\\|^2$ represents the squared L2 norm of the coefficient vector $\\beta$\n\nThe goal is to find the value of $\\beta$ that minimizes this loss function.\n\nLet's now implement L2 regularization for multiple linear regression using Python:\n\nFirst, let's import the necessary libraries:\n\n```{python}\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n```\n\nNext, we'll generate some random data for demonstration purposes:\n\n```{python}\n# Generate random data\nnp.random.seed(0)\nX = np.random.rand(100, 5)\nw = np.random.rand(5, 1)\ny = X**2 @ w + np.random.randn(100)*5\n```\n\nWe split the data into training and test sets:\n\n```{python}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nTo apply L2 regularization, we need to scale the input features using the StandardScaler:\n\n```{python}\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\nLet's use OLS without L2 regularization for a regression model:\n\n```{python}\n# Create a regression model without penalty\nno_ridge = Ridge(alpha=0.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nno_ridge.fit(X_train_scaled, y_train)\n```\n\nNext, we can evaluate the trained model on the test set:\n\n```{python}\n# Evaluate the model on the test set\nscore = no_ridge.score(X_test_scaled, y_test)\nprint(f\"No-penalty Regression Score: {score}\")\n```\n\nThe `score` represents the coefficient of determination $(R^2)$ of the prediction. Higher values of $R^2$ indicate better model performance.\n\n\nWe can now create and train the ridge regression model:\n\n```{python}\n# Create a ridge regression model\nridge = Ridge(alpha=10.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nridge.fit(X_train_scaled, y_train)\n```\n\nFinally, we can evaluate the trained model on the test set:\n\n```{python}\n# Evaluate the model on the test set\nscore = ridge.score(X_test_scaled, y_test)\nprint(f\"Ridge Regression Score: {score}\")\n```\n\nBy applying L2 regularization using ridge regression, we can improve the generalization of the multiple linear regression model by reducing overfitting and improving its performance on unseen data.","srcMarkdownNoYaml":"\n\n### L2 Regularization\n\nIn multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\n\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\n\nwhere:\n\n- $y$ is the dependent variable (target variable)\n- $X$ is the design matrix that consists of independent variables (features)\n- $\\beta$ is the vector of coefficients (slopes) for each feature in X\n- $\\epsilon$ is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients $\\beta$ that minimize the sum of squared residuals.\n\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\n\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter $\\lambda$.\n\nThe loss function for multiple linear regression with L2 regularization is given by:\n\n\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\n\nwhere:\n\n- $N$ is the number of samples in the dataset\n- $y_i$ is the target value for the i-th sample\n- $X_i$ is the feature vector for the i-th sample\n- $\\|\\beta\\|^2$ represents the squared L2 norm of the coefficient vector $\\beta$\n\nThe goal is to find the value of $\\beta$ that minimizes this loss function.\n\nLet's now implement L2 regularization for multiple linear regression using Python:\n\nFirst, let's import the necessary libraries:\n\n```{python}\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n```\n\nNext, we'll generate some random data for demonstration purposes:\n\n```{python}\n# Generate random data\nnp.random.seed(0)\nX = np.random.rand(100, 5)\nw = np.random.rand(5, 1)\ny = X**2 @ w + np.random.randn(100)*5\n```\n\nWe split the data into training and test sets:\n\n```{python}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nTo apply L2 regularization, we need to scale the input features using the StandardScaler:\n\n```{python}\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\nLet's use OLS without L2 regularization for a regression model:\n\n```{python}\n# Create a regression model without penalty\nno_ridge = Ridge(alpha=0.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nno_ridge.fit(X_train_scaled, y_train)\n```\n\nNext, we can evaluate the trained model on the test set:\n\n```{python}\n# Evaluate the model on the test set\nscore = no_ridge.score(X_test_scaled, y_test)\nprint(f\"No-penalty Regression Score: {score}\")\n```\n\nThe `score` represents the coefficient of determination $(R^2)$ of the prediction. Higher values of $R^2$ indicate better model performance.\n\n\nWe can now create and train the ridge regression model:\n\n```{python}\n# Create a ridge regression model\nridge = Ridge(alpha=10.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nridge.fit(X_train_scaled, y_train)\n```\n\nFinally, we can evaluate the trained model on the test set:\n\n```{python}\n# Evaluate the model on the test set\nscore = ridge.score(X_test_scaled, y_test)\nprint(f\"Ridge Regression Score: {score}\")\n```\n\nBy applying L2 regularization using ridge regression, we can improve the generalization of the multiple linear regression model by reducing overfitting and improving its performance on unseen data."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"Improv.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}