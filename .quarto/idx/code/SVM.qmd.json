{"title":"SVM","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"headingText":"SVM","containsRefs":false,"markdown":"\n\n\nSupport Vector Machines (SVM) is a powerful machine learning algorithm used for both classification and regression tasks. In this tutorial, we will focus on how to use SVM for classification.\n\nBefore diving into the code, let's understand the intuition behind SVM.\n\n## Intuition behind SVM\nSVM is based on the concept of finding a hyperplane that best separates the data points belonging to different classes. The hyperplane is determined by support vectors, which are the data points closest to the decision boundary.\n\nIn a binary classification problem, SVM aims to find a hyperplane that maximizes the margin between the support vectors of the two classes. The margin is the distance between the hyperplane and the nearest data points from each class.\n\nThe optimal hyperplane can be described by the equation: \n\n\n$w^T x - b = 0$\n\nwhere $w$ is the normal vector to the hyperplane and $b$ is the bias.\n\nThe equation of the decision function is:\n\n\n$f(x) = sign(w^T x - b)$\n\nwhere $sign(\\cdot)$ is the sign function.\n\nSVM can also handle non-linearly separable data by using a technique called the kernel trick. This technique transforms the original feature space into a higher-dimensional space, making the data linearly separable.\n\nNow, let's implement SVM for a classification problem using the famous Iris dataset.\n\n## Importing Libraries and Loading the Dataset\nThe first step is to import the required libraries and load the dataset. We will use scikit-learn library, which provides a simple API for SVM implementation.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n```\n\nNext, let's generate some synthetic data with two classes. We will use the `make_classification` function from the `sklearn.datasets` module to create a random dataset.\n\n```{python}\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\ny = y*2 - 1\n```\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\n\nNow, let's visualize the data using a scatter plot:\n\n```{python}\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == -1], y[y == -1], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n```\n\nWe have plotted the data points for each class on a scatter plot.\n\n\n## Training the Support Vector Machine Classifier\nOnce we have loaded the dataset, we can proceed to train the SVM classifier.\n\n```{python}\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVM classifier object\nsvm = SVC(kernel='linear', C=1e10)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n```\n\nIn this code block, we first split the data into training and testing sets using the `train_test_split()` function. We reserve 20% of the data for testing and set the `random_state` parameter for reproducibility purposes.\n\nNext, we create an SVM classifier object using the `SVC` class from scikit-learn. We specify the `kernel` parameter as 'linear' for a linear SVM.\n\nFinally, we train the SVM classifier using the `fit()` method by passing the training data (`X_train`) and the corresponding class labels (`y_train`).\n\n## Making Predictions and Evaluating the Model\nAfter training the SVM classifier, we can use it to make predictions on new, unseen data.\n\n```{python}\n# Make predictions on the testing set\ny_pred = svm.predict(X_test)\n\n# Evaluate the model\naccuracy = np.sum(y_pred == y_test) / len(y_test)\nprint(\"Accuracy: \", accuracy)\n```\n\nIn this code block, we use the trained SVM classifier to make predictions on the testing set (`X_test`). The predicted class labels are stored in the `y_pred` variable.\n\nWe then evaluate the model by calculating the accuracy of the predictions. The accuracy is defined as the number of correctly classified data points divided by the total number of data points.\n\n## Visualizing the Decision Boundary\nTo visualize the decision boundary and the support vectors, we can use the following code:\n\n```{python}\ndef plot_decision_boundary(classifier, X, y):\n    # Define the range of x values for the mesh grid\n    x_min, x_max = X.min() - 1, X.max() + 1\n\n    # Create a mesh grid\n    xx = np.linspace(x_min, x_max, 1000)[:, np.newaxis]\n\n    # Use the classifier to make predictions on the mesh grid\n    yy = classifier.decision_function(xx)\n\n    # get support vectors\n    i_sv = classifier.support_\n    sv_x = X[i_sv]\n    sv_y = y[i_sv]\n    print(f'Support vectors are: \\nX={sv_x}\\ny={sv_y}')\n\n    # get w and b\n    w = svm.coef_\n    b = svm.intercept_\n    print(f'W={w}\\nb={b}')\n    # print(f\"wX-b:\\n{w*X-b}\")\n\n    # where the decision function is zero\n    ind_0 = np.where((yy<=5e-2) & (yy>=-5e-2))[0]\n    # print(ind_0)\n    distance1 = xx[ind_0].mean() - sv_x[0]\n    distance2 = xx[ind_0].mean() - sv_x[1]\n    print(f'\\ndistance to \\nsupport 1: {distance1} \\nsupport 2:{distance2}')\n\n    # Plot the decision boundary and support vectors\n    plt.hlines(0,-2, 3)\n    plt.scatter(X, y, c=y, cmap=plt.cm.Paired, edgecolors='k')\n    plt.scatter(xx, yy, color='black', linewidth=3)\n    plt.scatter(sv_x,sv_y, color='red')\n    plt.scatter(X,w*X-b,color='yellow', alpha=0.5)\n    plt.scatter(xx[ind_0], np.zeros(len(ind_0),), marker='x', color='red')\n    plt.xlim(x_min, x_max)\n    # plt.ylim(-0.2, 1.2)\n    plt.xlabel('Feature')\n    plt.ylabel('label')\n    plt.show()\n\n# Visualize the decision boundary\nplot_decision_boundary(svm, X_train, y_train)\n```\n\nIn this code block, we define a helper function `plot_decision_boundary()` that takes a trained classifier object (`svm`), the training data (`X_train`), and the corresponding class labels (`y_train`) as input.\n\nThe function calculates the minimum and maximum values of the two features to define the plotting range. It then generates a mesh grid with a step size `h` and predicts the class labels for each point in the grid using `predict()` method.\n\nFinally, it plots the decision boundary by contouring the predicted class labels and scatter plots the training data points.\n\nRunning this code will display the decision boundary and the support vectors.\n\n\nThis is how we can implement SVM for classification in Python. SVM is a versatile algorithm and can be further fine-tuned by selecting different kernels and hyperparameters for better performance.","srcMarkdownNoYaml":"\n\n### SVM\n\nSupport Vector Machines (SVM) is a powerful machine learning algorithm used for both classification and regression tasks. In this tutorial, we will focus on how to use SVM for classification.\n\nBefore diving into the code, let's understand the intuition behind SVM.\n\n## Intuition behind SVM\nSVM is based on the concept of finding a hyperplane that best separates the data points belonging to different classes. The hyperplane is determined by support vectors, which are the data points closest to the decision boundary.\n\nIn a binary classification problem, SVM aims to find a hyperplane that maximizes the margin between the support vectors of the two classes. The margin is the distance between the hyperplane and the nearest data points from each class.\n\nThe optimal hyperplane can be described by the equation: \n\n\n$w^T x - b = 0$\n\nwhere $w$ is the normal vector to the hyperplane and $b$ is the bias.\n\nThe equation of the decision function is:\n\n\n$f(x) = sign(w^T x - b)$\n\nwhere $sign(\\cdot)$ is the sign function.\n\nSVM can also handle non-linearly separable data by using a technique called the kernel trick. This technique transforms the original feature space into a higher-dimensional space, making the data linearly separable.\n\nNow, let's implement SVM for a classification problem using the famous Iris dataset.\n\n## Importing Libraries and Loading the Dataset\nThe first step is to import the required libraries and load the dataset. We will use scikit-learn library, which provides a simple API for SVM implementation.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n```\n\nNext, let's generate some synthetic data with two classes. We will use the `make_classification` function from the `sklearn.datasets` module to create a random dataset.\n\n```{python}\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\ny = y*2 - 1\n```\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\n\nNow, let's visualize the data using a scatter plot:\n\n```{python}\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == -1], y[y == -1], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n```\n\nWe have plotted the data points for each class on a scatter plot.\n\n\n## Training the Support Vector Machine Classifier\nOnce we have loaded the dataset, we can proceed to train the SVM classifier.\n\n```{python}\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVM classifier object\nsvm = SVC(kernel='linear', C=1e10)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n```\n\nIn this code block, we first split the data into training and testing sets using the `train_test_split()` function. We reserve 20% of the data for testing and set the `random_state` parameter for reproducibility purposes.\n\nNext, we create an SVM classifier object using the `SVC` class from scikit-learn. We specify the `kernel` parameter as 'linear' for a linear SVM.\n\nFinally, we train the SVM classifier using the `fit()` method by passing the training data (`X_train`) and the corresponding class labels (`y_train`).\n\n## Making Predictions and Evaluating the Model\nAfter training the SVM classifier, we can use it to make predictions on new, unseen data.\n\n```{python}\n# Make predictions on the testing set\ny_pred = svm.predict(X_test)\n\n# Evaluate the model\naccuracy = np.sum(y_pred == y_test) / len(y_test)\nprint(\"Accuracy: \", accuracy)\n```\n\nIn this code block, we use the trained SVM classifier to make predictions on the testing set (`X_test`). The predicted class labels are stored in the `y_pred` variable.\n\nWe then evaluate the model by calculating the accuracy of the predictions. The accuracy is defined as the number of correctly classified data points divided by the total number of data points.\n\n## Visualizing the Decision Boundary\nTo visualize the decision boundary and the support vectors, we can use the following code:\n\n```{python}\ndef plot_decision_boundary(classifier, X, y):\n    # Define the range of x values for the mesh grid\n    x_min, x_max = X.min() - 1, X.max() + 1\n\n    # Create a mesh grid\n    xx = np.linspace(x_min, x_max, 1000)[:, np.newaxis]\n\n    # Use the classifier to make predictions on the mesh grid\n    yy = classifier.decision_function(xx)\n\n    # get support vectors\n    i_sv = classifier.support_\n    sv_x = X[i_sv]\n    sv_y = y[i_sv]\n    print(f'Support vectors are: \\nX={sv_x}\\ny={sv_y}')\n\n    # get w and b\n    w = svm.coef_\n    b = svm.intercept_\n    print(f'W={w}\\nb={b}')\n    # print(f\"wX-b:\\n{w*X-b}\")\n\n    # where the decision function is zero\n    ind_0 = np.where((yy<=5e-2) & (yy>=-5e-2))[0]\n    # print(ind_0)\n    distance1 = xx[ind_0].mean() - sv_x[0]\n    distance2 = xx[ind_0].mean() - sv_x[1]\n    print(f'\\ndistance to \\nsupport 1: {distance1} \\nsupport 2:{distance2}')\n\n    # Plot the decision boundary and support vectors\n    plt.hlines(0,-2, 3)\n    plt.scatter(X, y, c=y, cmap=plt.cm.Paired, edgecolors='k')\n    plt.scatter(xx, yy, color='black', linewidth=3)\n    plt.scatter(sv_x,sv_y, color='red')\n    plt.scatter(X,w*X-b,color='yellow', alpha=0.5)\n    plt.scatter(xx[ind_0], np.zeros(len(ind_0),), marker='x', color='red')\n    plt.xlim(x_min, x_max)\n    # plt.ylim(-0.2, 1.2)\n    plt.xlabel('Feature')\n    plt.ylabel('label')\n    plt.show()\n\n# Visualize the decision boundary\nplot_decision_boundary(svm, X_train, y_train)\n```\n\nIn this code block, we define a helper function `plot_decision_boundary()` that takes a trained classifier object (`svm`), the training data (`X_train`), and the corresponding class labels (`y_train`) as input.\n\nThe function calculates the minimum and maximum values of the two features to define the plotting range. It then generates a mesh grid with a step size `h` and predicts the class labels for each point in the grid using `predict()` method.\n\nFinally, it plots the decision boundary by contouring the predicted class labels and scatter plots the training data points.\n\nRunning this code will display the decision boundary and the support vectors.\n\n\nThis is how we can implement SVM for classification in Python. SVM is a versatile algorithm and can be further fine-tuned by selecting different kernels and hyperparameters for better performance."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"SVM.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}