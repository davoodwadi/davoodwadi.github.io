{"title":"Create design matrix X","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"headingText":"Create design matrix X","containsRefs":false,"markdown":"\n\n\nTo understand the generalization of L2 regularization on the test set for multiple linear regression with design matrix X, let's first discuss what L2 regularization is and how it is applied in multiple linear regression.\n\n**L2 Regularization in Multiple Linear Regression:**\n\nMultiple Linear Regression is a technique used to model the relationship between a dependent variable and multiple independent variables. The goal is to find the best fitting line that minimizes the sum of squared errors.\n\nHowever, in some cases, the model can become overfit, meaning it fits the training data too closely and does not generalize well to unseen data. This can lead to poor performance on the test set.\n\nL2 regularization, also known as Ridge regression, is a technique used to prevent overfitting. It adds a penalty term to the least squares objective function, which reduces the magnitude of the coefficients and helps in controlling the complexity of the model.\n\nThe L2 regularization term is given by:\n\n\n$\\text{L2 regularization term} = \\lambda \\sum_{i=1}^{m} \\beta_i^2$\n\nWhere:\n\n- \\(\\lambda\\) is the regularization parameter, which controls the amount of regularization applied. A higher value of \\(\\lambda\\) results in more regularization.\n- \\(\\beta_i\\) is the coefficient associated with the \\(i\\)th independent variable.\n\nIncluding the L2 regularization term in the objective function, the cost function for multiple linear regression with L2 regularization becomes:\n\n\n$\\text{{Cost function}} = \\frac{1}{2m} \\left( \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right)$\n\nWhere:\n\n- \\(m\\) is the number of training examples.\n- \\(h_\\theta(x^{(i)})\\) is the predicted value for the \\(i\\)th example using the model's parameters \\(\\theta\\).\n- \\(y^{(i)}\\) is the actual value for the \\(i\\)th example.\n\nNow, let's see how we can apply L2 regularization in multiple linear regression using Python.\n\n**Applying L2 Regularization in Multiple Linear Regression:**\n\nFirst, we need to import the required libraries for our example:\n\n```{python}\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n```\n\nNext, we need to create the design matrix X and the target variable y. The design matrix X contains the values of the independent variables, and the target variable y contains the corresponding dependent variable values.\n\n```{python}\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Create target variable y\ny = np.array([10, 20, 30])\n```\n\nWe split the data into training and test sets using the `train_test_split` function from scikit-learn. The training set will be used to train the model, and the test set will be used to evaluate its performance.\n\n```{python}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nNext, we create an instance of the Ridge regression model and train it on the training set.\n\n```{python}\n# Create an instance of the Ridge regression model\nridge = Ridge(alpha=0.5)\n\n# Train the model on the training set\nridge.fit(X_train, y_train)\n```\n\nWe can now use the trained model to make predictions on the test set.\n\n```{python}\n# Make predictions on the test set\ny_pred = ridge.predict(X_test)\n```\n\nFinally, we can evaluate the performance of the model using a performance metric such as mean squared error.\n\n```{python}\n# Calculate the mean squared error on the test set\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\n```\n\nThe mean squared error will give us an idea of how well the model is performing on the test set. A lower mean squared error indicates better performance.\n\nThis is how L2 regularization is applied in multiple linear regression to generalize the model on the test set. By adding a penalty term to the objective function, we can control the complexity of the model and prevent overfitting.","srcMarkdownNoYaml":"\n\n\nTo understand the generalization of L2 regularization on the test set for multiple linear regression with design matrix X, let's first discuss what L2 regularization is and how it is applied in multiple linear regression.\n\n**L2 Regularization in Multiple Linear Regression:**\n\nMultiple Linear Regression is a technique used to model the relationship between a dependent variable and multiple independent variables. The goal is to find the best fitting line that minimizes the sum of squared errors.\n\nHowever, in some cases, the model can become overfit, meaning it fits the training data too closely and does not generalize well to unseen data. This can lead to poor performance on the test set.\n\nL2 regularization, also known as Ridge regression, is a technique used to prevent overfitting. It adds a penalty term to the least squares objective function, which reduces the magnitude of the coefficients and helps in controlling the complexity of the model.\n\nThe L2 regularization term is given by:\n\n\n$\\text{L2 regularization term} = \\lambda \\sum_{i=1}^{m} \\beta_i^2$\n\nWhere:\n\n- \\(\\lambda\\) is the regularization parameter, which controls the amount of regularization applied. A higher value of \\(\\lambda\\) results in more regularization.\n- \\(\\beta_i\\) is the coefficient associated with the \\(i\\)th independent variable.\n\nIncluding the L2 regularization term in the objective function, the cost function for multiple linear regression with L2 regularization becomes:\n\n\n$\\text{{Cost function}} = \\frac{1}{2m} \\left( \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right)$\n\nWhere:\n\n- \\(m\\) is the number of training examples.\n- \\(h_\\theta(x^{(i)})\\) is the predicted value for the \\(i\\)th example using the model's parameters \\(\\theta\\).\n- \\(y^{(i)}\\) is the actual value for the \\(i\\)th example.\n\nNow, let's see how we can apply L2 regularization in multiple linear regression using Python.\n\n**Applying L2 Regularization in Multiple Linear Regression:**\n\nFirst, we need to import the required libraries for our example:\n\n```{python}\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n```\n\nNext, we need to create the design matrix X and the target variable y. The design matrix X contains the values of the independent variables, and the target variable y contains the corresponding dependent variable values.\n\n```{python}\n# Create design matrix X\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Create target variable y\ny = np.array([10, 20, 30])\n```\n\nWe split the data into training and test sets using the `train_test_split` function from scikit-learn. The training set will be used to train the model, and the test set will be used to evaluate its performance.\n\n```{python}\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nNext, we create an instance of the Ridge regression model and train it on the training set.\n\n```{python}\n# Create an instance of the Ridge regression model\nridge = Ridge(alpha=0.5)\n\n# Train the model on the training set\nridge.fit(X_train, y_train)\n```\n\nWe can now use the trained model to make predictions on the test set.\n\n```{python}\n# Make predictions on the test set\ny_pred = ridge.predict(X_test)\n```\n\nFinally, we can evaluate the performance of the model using a performance metric such as mean squared error.\n\n```{python}\n# Calculate the mean squared error on the test set\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\n```\n\nThe mean squared error will give us an idea of how well the model is performing on the test set. A lower mean squared error indicates better performance.\n\nThis is how L2 regularization is applied in multiple linear regression to generalize the model on the test set. By adding a penalty term to the objective function, we can control the complexity of the model and prevent overfitting."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"Genera.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}