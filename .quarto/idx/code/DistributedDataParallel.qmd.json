{"markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true},"eval":false},"containsRefs":false,"markdown":"\n\n\nTo train the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`, we need to perform the following steps:\n\n1. Import the necessary libraries:\n\n```{python}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nimport torch.multiprocessing as mp\nimport torchvision\nimport torchvision.transforms as transforms\n```\n\n2. Define the model architecture:\n\n```{python}\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 16 * 16, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(-1, 32 * 16 * 16)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CNN()\n```\n\n3. Define the training function:\n\n```{python}\ndef train(rank, world_size):\n    torch.manual_seed(0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize distributed training\n    dist.init_process_group(backend='nccl', init_method='env://', rank=rank, world_size=world_size)\n\n    # Load CIFAR-10 dataset\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n                                        transform=transforms.ToTensor())\n    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset, num_replicas=world_size, rank=rank)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=False, sampler=train_sampler)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    # Transfer model to GPU\n    model.to(device)\n\n    # Create DistributedDataParallel model\n    model = DistributedDataParallel(model)\n\n    # Training loop\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        train_correct = 0\n        total = 0\n\n        for inputs, labels in trainloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Compute metrics\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            train_loss += loss.item()\n\n        # Print epoch results\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {(train_correct/total)*100:.2f}%\")\n\n    # Cleanup\n    dist.destroy_process_group()\n```\n\n4. Define the main function for multi-processing:\n\n```{python}\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == '__main__':\n    main()\n```\n\n5. Run the main function to start the training process:\n\n```{python}\npython train_cifar10_distributed.py\n```\n\nThis code will train a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`. The `DistributedDataParallel` module enables us to train models on multiple GPUs or machines.\n\nThe CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat, etc.). The dataset is divided into 50,000 training images and 10,000 testing images.\n\nIn the code above, we define a CNN model with two convolutional layers and two fully connected layers. We then load the CIFAR-10 dataset, define the loss function and optimizer, and transfer the model to the GPU if available.\n\nWe create a `DistributedDataParallel` model from the base model to enable distributed training. The `DistributedDataParallel` wrapper will automatically scatter the input data to the available GPUs and gather the gradients during the backward pass.\n\nNext, we define the training loop, where we iterate over the batches of the training dataset. For each batch, we compute the forward pass, the loss, and perform the backward pass and optimization. We also keep track of the training loss and accuracy.\n\nAfter each epoch, we print the epoch's training loss and accuracy. Finally, we clean up the distributed training environment and destroy the process group.\n\nTo run the code, we use the `mp.spawn()` function to parallelize the training process across multiple processes, where each process trains on a separate GPU. The `world_size` parameter is set to the number of available GPUs.","srcMarkdownNoYaml":"\n\n\nTo train the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`, we need to perform the following steps:\n\n1. Import the necessary libraries:\n\n```{python}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nimport torch.multiprocessing as mp\nimport torchvision\nimport torchvision.transforms as transforms\n```\n\n2. Define the model architecture:\n\n```{python}\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 16 * 16, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(-1, 32 * 16 * 16)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CNN()\n```\n\n3. Define the training function:\n\n```{python}\ndef train(rank, world_size):\n    torch.manual_seed(0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize distributed training\n    dist.init_process_group(backend='nccl', init_method='env://', rank=rank, world_size=world_size)\n\n    # Load CIFAR-10 dataset\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n                                        transform=transforms.ToTensor())\n    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset, num_replicas=world_size, rank=rank)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=False, sampler=train_sampler)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    # Transfer model to GPU\n    model.to(device)\n\n    # Create DistributedDataParallel model\n    model = DistributedDataParallel(model)\n\n    # Training loop\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        train_correct = 0\n        total = 0\n\n        for inputs, labels in trainloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Compute metrics\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            train_loss += loss.item()\n\n        # Print epoch results\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {(train_correct/total)*100:.2f}%\")\n\n    # Cleanup\n    dist.destroy_process_group()\n```\n\n4. Define the main function for multi-processing:\n\n```{python}\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == '__main__':\n    main()\n```\n\n5. Run the main function to start the training process:\n\n```{python}\npython train_cifar10_distributed.py\n```\n\nThis code will train a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`. The `DistributedDataParallel` module enables us to train models on multiple GPUs or machines.\n\nThe CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat, etc.). The dataset is divided into 50,000 training images and 10,000 testing images.\n\nIn the code above, we define a CNN model with two convolutional layers and two fully connected layers. We then load the CIFAR-10 dataset, define the loss function and optimizer, and transfer the model to the GPU if available.\n\nWe create a `DistributedDataParallel` model from the base model to enable distributed training. The `DistributedDataParallel` wrapper will automatically scatter the input data to the available GPUs and gather the gradients during the backward pass.\n\nNext, we define the training loop, where we iterate over the batches of the training dataset. For each batch, we compute the forward pass, the loss, and perform the backward pass and optimization. We also keep track of the training loss and accuracy.\n\nAfter each epoch, we print the epoch's training loss and accuracy. Finally, we clean up the distributed training environment and destroy the process group.\n\nTo run the code, we use the `mp.spawn()` function to parallelize the training process across multiple processes, where each process trains on a separate GPU. The `world_size` parameter is set to the number of available GPUs."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"DistributedDataParallel.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}