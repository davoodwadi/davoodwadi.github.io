{"title":"Policy Iteration Algorithm","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true},"eval":false},"headingText":"Policy Iteration Algorithm","containsRefs":false,"markdown":"\n\n\n\nIn reinforcement learning, the **policy iteration algorithm** is used to find the optimal policy for an agent to maximize its expected return in a Markov Decision Process (MDP). The algorithm consists of two main steps: policy evaluation and policy improvement. \n\n1. **Policy Evaluation**: In this step, we evaluate the given policy by calculating the state-value function for each state. The state-value function, denoted as V(s), represents the expected return starting from state s and following the given policy. The Bellman equation for V(s) is given by:\n\n$V(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s', r|s, a) [r + \\gamma V(s')]$\n\nwhere:\n\n- $\\pi(a|s)$ is the probability of taking action a in state s according to the policy $\\pi$.\n- $p(s',r|s,a)$ is the probability of transitioning to state s' and receiving reward r, given that the agent took action a in state s.\n- $\\gamma$ is the discount factor, which determines the importance of future rewards compared to immediate rewards.\n\nThe policy evaluation step involves iteratively updating the state-value function until it converges to its true value under the given policy. \n\n2. **Policy Improvement**: Once the state-value function is updated, we can improve the policy by selecting the action that maximizes the expected return in each state. This can be done by using the *greedy policy improvement* rule:\n\n\n$\\pi'(s) = \\arg\\max_{a} \\sum_{s', r} p(s', r|s, a) [r + \\gamma V(s')]$\n\n\nwhere $\\pi'(s)$ is the new policy, and $V(s')$ is the updated state-value function.\n\nThe policy improvement step involves updating the policy based on the updated state-value function from the previous step.\n\nThis process of policy evaluation and policy improvement is repeated iteratively until the policy converges to the optimal policy.\n\nNow let's see an example of how to implement the policy iteration algorithm in Python:\n\n```{python}\n# Step 1: Policy Evaluation\ndef policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta):\n    # Initialize the state-value function with zeros\n    V = {s: 0 for s in states}\n\n    while True:\n        delta = 0\n        \n        # Iterate over all states\n        for s in states:\n            v = V[s]\n            new_v = 0\n            \n            # Iterate over all possible actions\n            for a in actions:\n                # Compute the expected return for each action\n                next_states = transitions[s][a]\n                expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n\n                # Compute the new state-value function\n                new_v += policy[s][a] * expected_return\n\n            # Update the state-value function for the current state\n            V[s] = new_v\n\n            # Calculate the maximum difference between the old and new state-value functions\n            delta = max(delta, abs(v - V[s]))\n        \n        # If the maximum difference is less than a threshold, we assume convergence\n        if delta < theta:\n            break\n    \n    return V\n\n# Step 2: Policy Improvement\ndef policy_improvement(states, actions, rewards, transitions, discount_factor, V):\n    # Initialize the new policy with zeros\n    new_policy = {s: {a: 0 for a in actions} for s in states}\n    \n    for s in states:\n        action_values = []\n        \n        # Iterate over all possible actions\n        for a in actions:\n            # Compute the expected return for each action\n            next_states = transitions[s][a]\n            expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n            \n            # Add the expected return to the list of action values\n            action_values.append(expected_return)\n        \n        # Assign the action with the maximum expected return as the new policy for the current state\n        idx = np.argmax(action_values)\n        new_policy[s][actions[idx]] = 1\n    \n    return new_policy\n\n# Step 3: Policy Iteration\ndef policy_iteration(states, actions, rewards, transitions, discount_factor, theta):\n    # Initialize a random policy\n    policy = {s: {a: 1 / len(actions) for a in actions} for s in states}\n\n    while True:\n        # Policy Evaluation\n        V = policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta)\n        \n        # Policy Improvement\n        new_policy = policy_improvement(states, actions, rewards, transitions, discount_factor, V)\n        \n        # If the new policy is the same as the old policy, we have converged to the optimal policy\n        if policy == new_policy:\n            break\n        \n        policy = new_policy\n    \n    return policy\n```\n\nIn the code above, we define three functions to implement the policy iteration algorithm:\n\n1. `policy_evaluation`: This function performs policy evaluation by iteratively updating the state-value function until it converges. It takes as input the policy, state and action spaces, rewards, transition probabilities, discount factor, and a convergence threshold.\n\n2. `policy_improvement`: This function improves the policy by selecting the action that maximizes the expected return in each state. It takes as input the state and action spaces, rewards, transition probabilities, discount factor, and the updated state-value function.\n\n3. `policy_iteration`: This function combines the policy evaluation and policy improvement steps to find the optimal policy. It iteratively updates the policy until it converges to the optimal policy.\n\nTo apply the policy iteration algorithm to a specific problem, you need to provide the state and action spaces, rewards, transition probabilities, discount factor, and convergence threshold.\n\nNow you can use the `policy_iteration` function to find the optimal policy for a given MDP.","srcMarkdownNoYaml":"\n\n\n# Policy Iteration Algorithm\n\nIn reinforcement learning, the **policy iteration algorithm** is used to find the optimal policy for an agent to maximize its expected return in a Markov Decision Process (MDP). The algorithm consists of two main steps: policy evaluation and policy improvement. \n\n1. **Policy Evaluation**: In this step, we evaluate the given policy by calculating the state-value function for each state. The state-value function, denoted as V(s), represents the expected return starting from state s and following the given policy. The Bellman equation for V(s) is given by:\n\n$V(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s', r|s, a) [r + \\gamma V(s')]$\n\nwhere:\n\n- $\\pi(a|s)$ is the probability of taking action a in state s according to the policy $\\pi$.\n- $p(s',r|s,a)$ is the probability of transitioning to state s' and receiving reward r, given that the agent took action a in state s.\n- $\\gamma$ is the discount factor, which determines the importance of future rewards compared to immediate rewards.\n\nThe policy evaluation step involves iteratively updating the state-value function until it converges to its true value under the given policy. \n\n2. **Policy Improvement**: Once the state-value function is updated, we can improve the policy by selecting the action that maximizes the expected return in each state. This can be done by using the *greedy policy improvement* rule:\n\n\n$\\pi'(s) = \\arg\\max_{a} \\sum_{s', r} p(s', r|s, a) [r + \\gamma V(s')]$\n\n\nwhere $\\pi'(s)$ is the new policy, and $V(s')$ is the updated state-value function.\n\nThe policy improvement step involves updating the policy based on the updated state-value function from the previous step.\n\nThis process of policy evaluation and policy improvement is repeated iteratively until the policy converges to the optimal policy.\n\nNow let's see an example of how to implement the policy iteration algorithm in Python:\n\n```{python}\n# Step 1: Policy Evaluation\ndef policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta):\n    # Initialize the state-value function with zeros\n    V = {s: 0 for s in states}\n\n    while True:\n        delta = 0\n        \n        # Iterate over all states\n        for s in states:\n            v = V[s]\n            new_v = 0\n            \n            # Iterate over all possible actions\n            for a in actions:\n                # Compute the expected return for each action\n                next_states = transitions[s][a]\n                expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n\n                # Compute the new state-value function\n                new_v += policy[s][a] * expected_return\n\n            # Update the state-value function for the current state\n            V[s] = new_v\n\n            # Calculate the maximum difference between the old and new state-value functions\n            delta = max(delta, abs(v - V[s]))\n        \n        # If the maximum difference is less than a threshold, we assume convergence\n        if delta < theta:\n            break\n    \n    return V\n\n# Step 2: Policy Improvement\ndef policy_improvement(states, actions, rewards, transitions, discount_factor, V):\n    # Initialize the new policy with zeros\n    new_policy = {s: {a: 0 for a in actions} for s in states}\n    \n    for s in states:\n        action_values = []\n        \n        # Iterate over all possible actions\n        for a in actions:\n            # Compute the expected return for each action\n            next_states = transitions[s][a]\n            expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n            \n            # Add the expected return to the list of action values\n            action_values.append(expected_return)\n        \n        # Assign the action with the maximum expected return as the new policy for the current state\n        idx = np.argmax(action_values)\n        new_policy[s][actions[idx]] = 1\n    \n    return new_policy\n\n# Step 3: Policy Iteration\ndef policy_iteration(states, actions, rewards, transitions, discount_factor, theta):\n    # Initialize a random policy\n    policy = {s: {a: 1 / len(actions) for a in actions} for s in states}\n\n    while True:\n        # Policy Evaluation\n        V = policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta)\n        \n        # Policy Improvement\n        new_policy = policy_improvement(states, actions, rewards, transitions, discount_factor, V)\n        \n        # If the new policy is the same as the old policy, we have converged to the optimal policy\n        if policy == new_policy:\n            break\n        \n        policy = new_policy\n    \n    return policy\n```\n\nIn the code above, we define three functions to implement the policy iteration algorithm:\n\n1. `policy_evaluation`: This function performs policy evaluation by iteratively updating the state-value function until it converges. It takes as input the policy, state and action spaces, rewards, transition probabilities, discount factor, and a convergence threshold.\n\n2. `policy_improvement`: This function improves the policy by selecting the action that maximizes the expected return in each state. It takes as input the state and action spaces, rewards, transition probabilities, discount factor, and the updated state-value function.\n\n3. `policy_iteration`: This function combines the policy evaluation and policy improvement steps to find the optimal policy. It iteratively updates the policy until it converges to the optimal policy.\n\nTo apply the policy iteration algorithm to a specific problem, you need to provide the state and action spaces, rewards, transition probabilities, discount factor, and convergence threshold.\n\nNow you can use the `policy_iteration` function to find the optimal policy for a given MDP."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"policy iteration.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}