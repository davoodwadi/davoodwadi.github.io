{"title":"Naive Bayes Classifier","markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true}},"headingText":"Naive Bayes Classifier","containsRefs":false,"markdown":"\n\n\n\nThe Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem. It is commonly used for classification problems and is especially effective when dealing with high-dimensional data. Naive Bayes assumes that the presence of a particular feature in a class is independent of the presence of other features.\n\n## Bayes' Theorem\n\nBefore diving into Naive Bayes, let's start with Bayes' theorem. Bayes' theorem allows us to calculate the probability of a hypothesis given some observed evidence. It is stated as:\n\n\n$P(h|e) = \\frac{{P(e|h) \\cdot P(h)}}{{P(e)}}$\n\nWhere:\n\n- $P(h|e)$ is the posterior probability of hypothesis $h$ given evidence $e$.\n- $P(e|h)$ is the probability of evidence $e$ given hypothesis $h$.\n- $P(h)$ is the prior probability of hypothesis $h$.\n- $P(e)$ is the probability of evidence $e$.\n\nIn the context of Naive Bayes, we can reframe this theorem as:\n\n$P(y|X) = \\frac{{P(X|y) \\cdot P(y)}}{{P(X)}}$\n\nWhere:\n\n- $X$ represents the input features.\n- $y$ represents the class or target variable.\n- $P(y|X)$ is the posterior probability of class $y$ given features $X$.\n- $P(X|y)$ is the probability of observing features $X$ given class $y$.\n- $P(y)$ is the prior probability of class $y$.\n- $P(X)$ is the probability of observing features $X$.\n\n## Naive Bayes Classifier\n\nThe Naive Bayes classifier assumes that the presence of each feature is independent of the presence of other features, given the class variable. This is where the \"Naive\" part comes in. Despite this simplifying assumption, Naive Bayes can still be very effective in practice, especially with text classification tasks.\n\nThere are three common types of Naive Bayes classifiers:\n1. Gaussian Naive Bayes: It assumes that the features are normally distributed.\n2. Multinomial Naive Bayes: It is suitable for discrete features (e.g., word counts).\n3. Bernoulli Naive Bayes: It is suitable for binary features (e.g., true/false).\n\nIn this tutorial, we will focus on Gaussian Naive Bayes, which is commonly used for continuous features.\n\n## Gaussian Naive Bayes\n\nGaussian Naive Bayes assumes that the continuous features in each class are normally distributed. It calculates the mean and standard deviation for each feature in each class and uses a Gaussian probability density function to estimate the likelihood of observing a particular feature value given a class.\nThe formula for the Gaussian probability density function is:\n\n\n$P(x|y) = \\frac{1}{{\\sqrt{{2\\pi\\sigma_y^2}}}} \\cdot e^{-\\frac{{(x - \\mu_y)^2}}{{2\\sigma_y^2}}}$\n\nWhere:\n\n- $x$ is the feature value.\n- $y$ is the class label.\n- $\\mu_y$ is the mean of the feature values in class $y$.\n- $\\sigma_y$ is the standard deviation of the feature values in class $y$.\n- $\\pi$ is the mathematical constant pi.\n\n## Implementation\n\nNow let's see the implementation of Gaussian Naive Bayes in Python using the `sklearn` library.\n\nStep 1: Import the required libraries.\n```{python}\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n```\n\nStep 2: Load the dataset.\n```{python}\niris = load_iris()\nX = iris.data\ny = iris.target\n```\n\nStep 3: Split the dataset into training and testing sets.\n```{python}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nStep 4: Create a Gaussian Naive Bayes classifier and fit it to the training data.\n```{python}\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n```\n\nStep 5: Make predictions on the test data.\n```{python}\ny_pred = clf.predict(X_test)\n```\n\nStep 6: Evaluate the accuracy of the classifier.\n```{python}\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n```\n\nThat's it! You have successfully implemented Gaussian Naive Bayes for classification in Python using `sklearn`.\n\nNote: This tutorial only covers the basic usage of Naive Bayes. There are many other aspects and variations of Naive Bayes that you can explore, such as Laplace smoothing, handling missing values, and dealing with categorical features.","srcMarkdownNoYaml":"\n\n\n# Naive Bayes Classifier\n\nThe Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem. It is commonly used for classification problems and is especially effective when dealing with high-dimensional data. Naive Bayes assumes that the presence of a particular feature in a class is independent of the presence of other features.\n\n## Bayes' Theorem\n\nBefore diving into Naive Bayes, let's start with Bayes' theorem. Bayes' theorem allows us to calculate the probability of a hypothesis given some observed evidence. It is stated as:\n\n\n$P(h|e) = \\frac{{P(e|h) \\cdot P(h)}}{{P(e)}}$\n\nWhere:\n\n- $P(h|e)$ is the posterior probability of hypothesis $h$ given evidence $e$.\n- $P(e|h)$ is the probability of evidence $e$ given hypothesis $h$.\n- $P(h)$ is the prior probability of hypothesis $h$.\n- $P(e)$ is the probability of evidence $e$.\n\nIn the context of Naive Bayes, we can reframe this theorem as:\n\n$P(y|X) = \\frac{{P(X|y) \\cdot P(y)}}{{P(X)}}$\n\nWhere:\n\n- $X$ represents the input features.\n- $y$ represents the class or target variable.\n- $P(y|X)$ is the posterior probability of class $y$ given features $X$.\n- $P(X|y)$ is the probability of observing features $X$ given class $y$.\n- $P(y)$ is the prior probability of class $y$.\n- $P(X)$ is the probability of observing features $X$.\n\n## Naive Bayes Classifier\n\nThe Naive Bayes classifier assumes that the presence of each feature is independent of the presence of other features, given the class variable. This is where the \"Naive\" part comes in. Despite this simplifying assumption, Naive Bayes can still be very effective in practice, especially with text classification tasks.\n\nThere are three common types of Naive Bayes classifiers:\n1. Gaussian Naive Bayes: It assumes that the features are normally distributed.\n2. Multinomial Naive Bayes: It is suitable for discrete features (e.g., word counts).\n3. Bernoulli Naive Bayes: It is suitable for binary features (e.g., true/false).\n\nIn this tutorial, we will focus on Gaussian Naive Bayes, which is commonly used for continuous features.\n\n## Gaussian Naive Bayes\n\nGaussian Naive Bayes assumes that the continuous features in each class are normally distributed. It calculates the mean and standard deviation for each feature in each class and uses a Gaussian probability density function to estimate the likelihood of observing a particular feature value given a class.\nThe formula for the Gaussian probability density function is:\n\n\n$P(x|y) = \\frac{1}{{\\sqrt{{2\\pi\\sigma_y^2}}}} \\cdot e^{-\\frac{{(x - \\mu_y)^2}}{{2\\sigma_y^2}}}$\n\nWhere:\n\n- $x$ is the feature value.\n- $y$ is the class label.\n- $\\mu_y$ is the mean of the feature values in class $y$.\n- $\\sigma_y$ is the standard deviation of the feature values in class $y$.\n- $\\pi$ is the mathematical constant pi.\n\n## Implementation\n\nNow let's see the implementation of Gaussian Naive Bayes in Python using the `sklearn` library.\n\nStep 1: Import the required libraries.\n```{python}\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n```\n\nStep 2: Load the dataset.\n```{python}\niris = load_iris()\nX = iris.data\ny = iris.target\n```\n\nStep 3: Split the dataset into training and testing sets.\n```{python}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\nStep 4: Create a Gaussian Naive Bayes classifier and fit it to the training data.\n```{python}\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n```\n\nStep 5: Make predictions on the test data.\n```{python}\ny_pred = clf.predict(X_test)\n```\n\nStep 6: Evaluate the accuracy of the classifier.\n```{python}\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n```\n\nThat's it! You have successfully implemented Gaussian Naive Bayes for classification in Python using `sklearn`.\n\nNote: This tutorial only covers the basic usage of Naive Bayes. There are many other aspects and variations of Naive Bayes that you can explore, such as Laplace smoothing, handling missing values, and dealing with categorical features."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"naive.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}