{"markdown":{"yaml":{"title":"","format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"cache":true},"eval":false},"containsRefs":false,"markdown":"\n\n\nThe Value Iteration Algorithm is a method used in reinforcement learning to find the optimal value function for a given Markov Decision Process (MDP). It is an iterative algorithm that aims to converge to the optimal value function by iteratively updating the values of states based on the Bellman Optimality Equation.\n\nThe Bellman Optimality Equation for a state s in an MDP is given by:\n\n$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^*(s')]$\n\nwhere:\n- $V^*(s)$ is the optimal value function for state s.\n- $\\max_a$ represents the selection of the action that maximizes the equation.\n- $P(s'|s,a)$ is the probability of transitioning to state s' from state s given action a.\n- $R(s,a,s')$ is the reward for transitioning from state s to state s' using action a.\n- $\\gamma$ is the discount factor that determines the importance of future rewards.\n\nThe algorithm starts with an initial estimate of the optimal value function and iteratively updates the values of states until convergence. The value of each state is updated by selecting the action that maximizes the equation.\n\nNow, let's implement the Value Iteration Algorithm in Python:\n\n```{python}\nimport numpy as np\n\ndef value_iteration(env, gamma, epsilon):\n    # Initialize the value function\n    V = np.zeros(env.n_states)\n    \n    # Iterate until convergence\n    while True:\n        delta = 0\n        # Update the value of each state\n        for s in range(env.n_states):\n            # Find the maximum Q-value for the state\n            max_value = -np.inf\n            for a in range(env.n_actions):\n                q_value = 0\n                # Calculate the Q-value for each action\n                for s_prime in range(env.n_states):\n                    transition_prob = env.transition_probabilities[s,a,s_prime]\n                    reward = env.rewards[s,a,s_prime]\n                    q_value += transition_prob * (reward + gamma * V[s_prime])\n                max_value = max(max_value, q_value)\n            # Update the value function\n            delta = max(delta, np.abs(max_value - V[s]))\n            V[s] = max_value\n        \n        # Check for convergence\n        if delta < epsilon:\n            break\n    \n    return V\n```\n\nLet's go through the code step by step:\n\n1. The code starts by importing the necessary libraries, including numpy for numerical operations.\n\n2. The `value_iteration` function takes three arguments: `env` (the environment), `gamma` (the discount factor), and `epsilon` (the convergence threshold).\n\n3. We initialize the value function `V` as an array of zeros with the same size as the number of states in the environment.\n\n4. The algorithm enters a loop that continues until convergence.\n\n5. Inside the loop, we set the `delta` variable to 0. `delta` is used to keep track of the maximum change in the value function during an iteration.\n\n6. We iterate over all the states in the environment.\n\n7. For each state, we iterate over all the possible actions.\n\n8. For each action, we calculate the Q-value using nested loops. The Q-value is calculated as the sum of the transition probabilities multiplied by the sum of the rewards and the discounted value of the next state.\n\n9. We update the `max_value` variable if the current Q-value is greater than the current maximum value.\n\n10. After all actions have been considered, we update the value of the state in the value function array.\n\n11. We update the `delta` variable with the maximum change in any state during the iteration.\n\n12. We check if the `delta` is less than the convergence threshold `epsilon`. If it is, the algorithm breaks out of the loop and returns the optimal value function.\n\n13. If the convergence condition is not met, the algorithm continues to the next iteration.\n\nThat's it! The `value_iteration` function returns the optimal value function for the given MDP. This value function can be used to determine the optimal policy for taking actions in the environment.","srcMarkdownNoYaml":"\n\n\nThe Value Iteration Algorithm is a method used in reinforcement learning to find the optimal value function for a given Markov Decision Process (MDP). It is an iterative algorithm that aims to converge to the optimal value function by iteratively updating the values of states based on the Bellman Optimality Equation.\n\nThe Bellman Optimality Equation for a state s in an MDP is given by:\n\n$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^*(s')]$\n\nwhere:\n- $V^*(s)$ is the optimal value function for state s.\n- $\\max_a$ represents the selection of the action that maximizes the equation.\n- $P(s'|s,a)$ is the probability of transitioning to state s' from state s given action a.\n- $R(s,a,s')$ is the reward for transitioning from state s to state s' using action a.\n- $\\gamma$ is the discount factor that determines the importance of future rewards.\n\nThe algorithm starts with an initial estimate of the optimal value function and iteratively updates the values of states until convergence. The value of each state is updated by selecting the action that maximizes the equation.\n\nNow, let's implement the Value Iteration Algorithm in Python:\n\n```{python}\nimport numpy as np\n\ndef value_iteration(env, gamma, epsilon):\n    # Initialize the value function\n    V = np.zeros(env.n_states)\n    \n    # Iterate until convergence\n    while True:\n        delta = 0\n        # Update the value of each state\n        for s in range(env.n_states):\n            # Find the maximum Q-value for the state\n            max_value = -np.inf\n            for a in range(env.n_actions):\n                q_value = 0\n                # Calculate the Q-value for each action\n                for s_prime in range(env.n_states):\n                    transition_prob = env.transition_probabilities[s,a,s_prime]\n                    reward = env.rewards[s,a,s_prime]\n                    q_value += transition_prob * (reward + gamma * V[s_prime])\n                max_value = max(max_value, q_value)\n            # Update the value function\n            delta = max(delta, np.abs(max_value - V[s]))\n            V[s] = max_value\n        \n        # Check for convergence\n        if delta < epsilon:\n            break\n    \n    return V\n```\n\nLet's go through the code step by step:\n\n1. The code starts by importing the necessary libraries, including numpy for numerical operations.\n\n2. The `value_iteration` function takes three arguments: `env` (the environment), `gamma` (the discount factor), and `epsilon` (the convergence threshold).\n\n3. We initialize the value function `V` as an array of zeros with the same size as the number of states in the environment.\n\n4. The algorithm enters a loop that continues until convergence.\n\n5. Inside the loop, we set the `delta` variable to 0. `delta` is used to keep track of the maximum change in the value function during an iteration.\n\n6. We iterate over all the states in the environment.\n\n7. For each state, we iterate over all the possible actions.\n\n8. For each action, we calculate the Q-value using nested loops. The Q-value is calculated as the sum of the transition probabilities multiplied by the sum of the rewards and the discounted value of the next state.\n\n9. We update the `max_value` variable if the current Q-value is greater than the current maximum value.\n\n10. After all actions have been considered, we update the value of the state in the value function array.\n\n11. We update the `delta` variable with the maximum change in any state during the iteration.\n\n12. We check if the `delta` is less than the convergence threshold `epsilon`. If it is, the algorithm breaks out of the loop and returns the optimal value function.\n\n13. If the convergence condition is not met, the algorithm continues to the next iteration.\n\nThat's it! The `value_iteration` function returns the optimal value function for the given MDP. This value function can be used to determine the optimal policy for taking actions in the environment."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"value iteration.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}