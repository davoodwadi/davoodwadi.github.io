{"title":"MATH60629A - Machine Learning I (Fall 2023)","markdown":{"yaml":{"title":"MATH60629A - Machine Learning I (Fall 2023)","aliases":["index.html"],"resources":["*.pdf"],"format":{"html":{"toc":true,"toc-depth":4}}},"headingText":"Course Information","containsRefs":false,"markdown":"\n\n[Code Tutorials](#code-tutorials)       |       [Contact Information](#contact-information) \t| [Bibliographic Sources](#bibliographic-sources) \t| [Class Information](#class-information) \t| [Evaluations](#evaluations) \t| [Session Materials](sessions.qmd)\n\n### Code Tutorials\n* Week 1\n    + [Python basics](code/Session1.ipynb)\n* Week 2\n    + [Model capacity](code/Model.qmd)\n    + [Overfitting](code/overfi.qmd)\n    + [Linear regression - Part 1](code/multip1.qmd)\n    + [Linear regression - Part 2](code/multip2.qmd)\n    + [Bias/variance tradeoff](code/bias%20v.qmd)\n    + [L2 regularization](code/Improv.qmd)\n* Week 3\n    + [Linear least squares for classification](code/Linear_classification.qmd)\n    + [SVM for classification](code/SVM.qmd)\n    + [Gaussian Naive Bayes](code/naive.qmd)\n    + [Multinomial Naive Bayes](code/multin.qmd)\n    + [Precision, Recall, F1-Score](code/prf1.qmd)\n    + [Precision-Recall Curve](code/precis.qmd)\n* Week 4\n    + [Introduction to PyTorch (Part 1)](Introduction_to_PyTorch.ipynb) ([colab](https://github.com/davoodwadi/davoodwadi.github.io/blob/main/Introduction_to_PyTorch.ipynb))\n* Week 5\n    + [Introduction to PyTorch (Part 2)](code/pytorch_intro.qmd)\n    + [Dataset class in PyTorch](code/datasets_pytorch.qmd)\n    + [Batching and Dataloader in PyTorch](code/dataloader_pytorch.qmd)\n    + [Backpropagation with PyTorch](code/backpropagation_pytorch.qmd)\n    + [Linear regression with gradient descent](code/gradie2.qmd)\n    + [Classification using MLP with Pytorch](code/MLP_pytorch.qmd)\n\n* Week 6\n    + [Minibatch training in Pytorch](Minibatch_PyTorch.ipynb)\n    + [Batch Normalization](Batch_normalization.ipynb)\n    + [High accuracy: CIFAR-10](High_accuracy_CIFAR10.ipynb)\n    + [Introduction to RNNs](code/RNN_timeseries.qmd)\n    + [Time dependence](linear_separation.ipynb)\n    + [Language modeling (RNN and Transformer)](code/LM.ipynb)\n\n* Week 7\n    + [K-Means implementation (session 7 - in-class code)](session_7.ipynb)\n* Week 10\n    + [Multi-GPU training with DataParallel](code/DataParallel.qmd)\n    + [Multi-Node training with DistributedDataParallel](code/DistributedDataParallel.qmd)\n\n* Week 11\n    + [Singular Value Decomposition with PyTorch](SVD.ipynb)\n    + [Singular Value Decomposition for MovieLens100k](SVD_movielens100k.ipynb)\n* Week 12\n    + [Value iteration](code/value%20iteration.qmd)\n    + [Policy iteration](code/policy%20iteration.qmd)\n    \n\n\n### Contact Information\n::: {.callout-note appearance=\"minimal\" collapse=true}\n\n* **Davood Wadi**\n    + Part-time Faculty Lecturer\n    + <davood.wadi@hec.ca>\n    + For office hours:\n        - [Book meeting](https://calendly.com/davoodwadi/30min) \n\n\n* **Val√©rie Boucher**\n    + Assistant to Academic Activities\n    + <valerie.boucher@hec.ca>\n    + 514-340-5670\n    + Office: 4.632\n    + Availability: \n        - Monday to Friday\n        - 8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n \n* **Antonietta Florio**\n    + Assistant to Academic Activities\n    + <antonietta.florio@hec.ca>\n    + 514-340-6473\n    + Office: 4.632\n    + Availability:\n        - From Monday to Friday\n        - From 8 am to 12 pm and from 1 pm to 4:30 pm \n* **Jennifer Caron**\n    + Executive Assistant\n    + <jennifer.caron@hec.ca>\n    + 514-340-6473\n    + Office: 4.632\n    + Availability:\n        - From Monday to Friday\n        - From 8 am to 12 pm and from 1 pm to 4:30 pm\n:::\n\n### Bibliographic Sources\n::: {.callout-note appearance=\"minimal\" collapse=true}\n* The Elements of Statistical Learning: Data Mining, Inference, and Prediction\n    + [Available online](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n\n \n* Goodfellow, Ian . Deep learning , MIT Press\nISBN: 9780262035613\n    + [Available online](https://www.deeplearningbook.org)\n \n* Richard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n    + [Available online](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwjAx8CEmsLbAhUFjlkKHd3hBnQQFggpMAE&url=http%3A%2F%2Fincompleteideas.net%2Fbook%2Fbookdraft2017nov5.pdf&usg=AOvVaw00kFmqVbFSdkU3PTkJMJrO)\n \n* Murphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press\nISBN: 9780262018029\n    + Available at the library\n \n* Ricci, Francesco. Recommender systems handbook, Springer\nISBN: 148997637X\n    + Available at the library\n \n* Parsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O'Reilly Media\nISBN: 9781491906156\n    + Available at the library\n \n* McKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O'Reilly Media\n    + Available at the library\n \n* Bishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York\nISBN: 9780387310732\n    + Available at the library\n \n* Leskovec, Jurij. Mining of massive datasets, Cambridge University Press\n    + Available at the library\n \n* Grus, Joel. Data science from scratch, O'Reilly Media\n    + Available at the library\n \n* Sandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O'Reilly Media\n    + Available at the library\n:::\n\n### Class Information\n::: {.callout-note appearance=\"minimal\" collapse=false}\n* Location\n    + Building C-Ste-Cath, Room Groupe Cholette\n* Time\n    + Tuesdays, 3:30 PM - 6:30 PM EDT\n:::\n\n## Evaluations\n### [Bonus] Class participation (5%)\n\nTo receive this 5% bonus point:\n\n1. Prepare for each session's code before the class starts.\n2. Log in to [deepnote](https://www.deepnote.com) with your **Full Name**.\n3. During the coding exercises in class, add your answer to the questions as a *comment*.\n\n### Homework (20%)\n* Due on 24 October 2023\n* Homework assignments are counted for 20% of your final grade.\n* The homework should be done in jupyter notebook [here](MATH60629A_Homework.ipynb) or Google Colab [here](https://colab.research.google.com/github/davoodwadi/MATH60629A.A2023-MACHINE-LEARNING-I/blob/main/MATH60629A_Homework.ipynb).\n\n* **Instructions**:\n    + The homework is due by 11:59PM EST on the due date. \n    + Please upload a PDF version of your assignment on ZoneCours and insert the shareable link to your notebook in the same interface.\n    + Homeworks must be completed individually. \n        - Discussion with others is okay, but you must write solutions yourself.\n    + All code used to arrive at answers is submitted along with answers.\n    + **Notes:**\n        - Please provide your code answers in the code block under each question and verbal answers in text boxes assigned in the notebook (where applicable).\n        - Please run the notebook before the submission so that the outputs are displayed.\n        - Please make sure that your results are reproducible. You may use random seeds from `random` and `numpy` packages. For `scikit-learn` modules, you may use the `random_state` argument.\n\n### Project (30%)\nThe aim of this project is to allow you to learn about machine learning by trying to solve a task with it.\n\nFirst, select a question that can be answered using machine learning. I expect that your question will be about a model/algorithm or about an application. Then design a study that will try to answer your question. Your study must have an element of novelty. For example the novelty could be an extension or a variation of an existing algorithm or results of an existing method on a new dataset.\n\nYour study should involve reading and understanding some background material. Your study must involve running some experiments. You are free to use (or not) any of the tools or models we have seen in class.\n\n\n**Study plan**: *(1 upload per group)* Please submit a one-page summary of your proposed research question and study to ZoneCours. I will meet with each group to discuss study plans during the lecture on Week 9. I will send you a schedule the day before. We will probably only have about 15 minutes so please make sure that your study plan is clear and precise. You may also include questions that you would like us to discuss at the end of the document.\n\n**The group report**: *(1 upload per group)* Your report must contain a description of the question you are trying to answer, a clear description of the model/algorithm you are studying, a survey of related work which proper references, an empirical section that reports your results, and a conclusion that summarizes your findings and (if pertinent) highlights possible future directions of investigation. Your report should be no longer than 10 pages in length (plus references) for pairs or 13 pages (plus references) for teams of three.\n\n\n**The individual report**: *(1 upload per student)* You will also submit a brief individual report (at most one page), which will: (1) Describe the parts of the project you worked on (which machine learning methods you applied, which preprocessing steps you performed on the data, which parts of the term paper you wrote, who you worked with on what parts, etc.) and what parts of the project your teammates worked on. (2) What you learned from the project. The purpose of the individual report is to facilitate fair grading and to allow the instructor to understand well what you learned from the project.\n\nProject Report (30%)\n\n* Clarity/Relevance of problem statement and description of approach: 10%\n* Discussion of relationship to previous work and references: 4%\n* Design and execution of experiments: 10%\n* Figures/Tables/Writing: easily readable, properly labeled, \n* informative: 5%\n* Indiviual report: 1%\n\n\n#### Project Topic\n\nConcretely, your project could take one of the following forms:\n\n* A competition on [kaggle.com](https://kaggle.com) that you try multiple ML methods on and can achieve a validation set performance in the top 100 on the leaderboard.\n* A benchmark task on [paperswithcode](https://paperswithcode.com) (e.g. [Speech Recognition](https://paperswithcode.com/task/speech-recognition)) where you try multiple methods and compare their performance.\n* A task in your own field whereby you apply multiple ML methods to solve the task.\n* An ML method that you apply to a new task (e.g. [Multi-head attention](https://paperswithcode.com/method/multi-head-attention) applied to time-series prediction)\n\n*Note*. The list above is not exhaustive. You are encouraged to be creative about the topic. Pick a topic that you find interesting and is relevant. The appropriateness of the topic will be discussed and resolved in the session on [Study plan](#timeline).\n\n#### Group Report Structure\nYour group report should read like a published paper. Similarly, your poster presentation should have a similar structure to conference posters. You might want to check out similar papers to your topic on the structure of your report (e.g. [A Comparison of Deep Learning Approach for Underwater Object Detection](https://www.jurnal.iaii.or.id/index.php/RESTI/article/view/3931/573)).\n\nRegardless of your topic, as a general rule of thumb, your Group Report should be structured as follows:\n\n1. Introduction:\n    Use this section to introduce the topic, its relevance, and a summary of the whole paper. This section should address the question, \"Why the target audience of the paper should care about your work?\"\n\n2. Related Work:\n    This section discusses what has been done, in prior research, on your topic. While most of the cited papers should be from academic publications, it is not uncommon to see non-academic references in papers (e.g. [stackoverflow](https://stackoverflow.com)). You might want to use academic search engines (e.g. [SemancticScholar](https://semanticscholar.org)) to find relavant papers.\n\n3. Experiments\n    This section should cover the steps you take to answer the question you posed in the Introduction. This includes data preparation, model and hyper-parameter selection, and evaluation steps.\n\n4. Results\n    In this section you present the resutls of your experiments with appropriate tables and figures.\n\n5. Conclusion\n    In your final section, you would summarize what you did in the paper, acknowledge the limitations of your work (All papers have this; no work is perfect.), suggest future research directions to address your limitations.\n\n#### Timeline\n\n* Team Registration, due: **October 1**. Fill this [form](https://forms.gle/DkNsrh8xpYyU1kAh6).\n* Study plan, due: **October 28** (by the end of the day EDT).\n* Handing in: Through *ZoneCours*\n* Project meeting, **October 31**\n* Project Presentation, due: **December 1**. Upload the PDF of your poster/slides to *ZoneCours*.\n* In-class Presentation, on **December 5**.\n* Final individual report, due: **December 15**, 2023 (by the end of the day EDT).\n    + Handing in: Through *ZoneCours* (per each team member).\n\n\n### Project Presentation (10%)\nMake a poster that describes your project [**You do not need to print your poster**]. You can think of a poster as supporting material for your oral presentation (in that way it is similar to slides). It could also follow a similar structure: begin by motivating your work, then (quickly) highlight related work, talk in depth about your solution, then go into results (pictures and tables are good tools for that), finally conclude and perhaps mention one or two ideas for future work.\n\n* Project Presentation (10%)\n    + Clarity of presentation: 3%\n    + Slide or Poster quality: 2%\n    + Correctness: 2%\n    + Answers to questions: 3%\n\n\n\n### Final Exam (30%)\n+ Dec 14, 2023\n+ 9:00 am - 12:00 pm EDT\n+ Location to be communicated\n\n[Past exam - Fall 2018](exam_80629_A18.pdf)\n\n### Capsule Quizzes (10%)\n* In-class quizzes of the capsules\n* Capsule quizzes are counted for 10% of your final grade.\n* Time to complete each quiz is 10 minutes.\n    + Quiz 1, **Sep 20, 2023 11:30 AM to Sep 22, 2023 11:30 AM**.\n    + Quiz 2, **Sep 27, 2023 11:30 AM to Sep 29, 2023 11:30 AM**.\n    + Quiz 3 - CNNs and RNNs, **Nov 6, 2023 12:00 AM to Nov 8, 2023 12:00 AM**.\n    + Quiz 4 - Unsupervised Learning, **Nov 13, 2023 12:00 AM to Nov 15, 2023 12:00 AM**.\n    + Quiz 5 - Parallel Computing, **Nov 20, 2023 12:00 AM to Nov 22, 2023 12:00 AM**.\n    + Quiz 6 - Reinforcement Learning, **Nov 27, 2023 12:00 AM to Nov 29, 2023 12:00 AM**.\n\n## In-class Tools\n* [Cups pacing webapp](https://ml1-pace.onrender.com/ml1)","srcMarkdownNoYaml":"\n## Course Information\n\n[Code Tutorials](#code-tutorials)       |       [Contact Information](#contact-information) \t| [Bibliographic Sources](#bibliographic-sources) \t| [Class Information](#class-information) \t| [Evaluations](#evaluations) \t| [Session Materials](sessions.qmd)\n\n### Code Tutorials\n* Week 1\n    + [Python basics](code/Session1.ipynb)\n* Week 2\n    + [Model capacity](code/Model.qmd)\n    + [Overfitting](code/overfi.qmd)\n    + [Linear regression - Part 1](code/multip1.qmd)\n    + [Linear regression - Part 2](code/multip2.qmd)\n    + [Bias/variance tradeoff](code/bias%20v.qmd)\n    + [L2 regularization](code/Improv.qmd)\n* Week 3\n    + [Linear least squares for classification](code/Linear_classification.qmd)\n    + [SVM for classification](code/SVM.qmd)\n    + [Gaussian Naive Bayes](code/naive.qmd)\n    + [Multinomial Naive Bayes](code/multin.qmd)\n    + [Precision, Recall, F1-Score](code/prf1.qmd)\n    + [Precision-Recall Curve](code/precis.qmd)\n* Week 4\n    + [Introduction to PyTorch (Part 1)](Introduction_to_PyTorch.ipynb) ([colab](https://github.com/davoodwadi/davoodwadi.github.io/blob/main/Introduction_to_PyTorch.ipynb))\n* Week 5\n    + [Introduction to PyTorch (Part 2)](code/pytorch_intro.qmd)\n    + [Dataset class in PyTorch](code/datasets_pytorch.qmd)\n    + [Batching and Dataloader in PyTorch](code/dataloader_pytorch.qmd)\n    + [Backpropagation with PyTorch](code/backpropagation_pytorch.qmd)\n    + [Linear regression with gradient descent](code/gradie2.qmd)\n    + [Classification using MLP with Pytorch](code/MLP_pytorch.qmd)\n\n* Week 6\n    + [Minibatch training in Pytorch](Minibatch_PyTorch.ipynb)\n    + [Batch Normalization](Batch_normalization.ipynb)\n    + [High accuracy: CIFAR-10](High_accuracy_CIFAR10.ipynb)\n    + [Introduction to RNNs](code/RNN_timeseries.qmd)\n    + [Time dependence](linear_separation.ipynb)\n    + [Language modeling (RNN and Transformer)](code/LM.ipynb)\n\n* Week 7\n    + [K-Means implementation (session 7 - in-class code)](session_7.ipynb)\n* Week 10\n    + [Multi-GPU training with DataParallel](code/DataParallel.qmd)\n    + [Multi-Node training with DistributedDataParallel](code/DistributedDataParallel.qmd)\n\n* Week 11\n    + [Singular Value Decomposition with PyTorch](SVD.ipynb)\n    + [Singular Value Decomposition for MovieLens100k](SVD_movielens100k.ipynb)\n* Week 12\n    + [Value iteration](code/value%20iteration.qmd)\n    + [Policy iteration](code/policy%20iteration.qmd)\n    \n\n\n### Contact Information\n::: {.callout-note appearance=\"minimal\" collapse=true}\n\n* **Davood Wadi**\n    + Part-time Faculty Lecturer\n    + <davood.wadi@hec.ca>\n    + For office hours:\n        - [Book meeting](https://calendly.com/davoodwadi/30min) \n\n\n* **Val√©rie Boucher**\n    + Assistant to Academic Activities\n    + <valerie.boucher@hec.ca>\n    + 514-340-5670\n    + Office: 4.632\n    + Availability: \n        - Monday to Friday\n        - 8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n \n* **Antonietta Florio**\n    + Assistant to Academic Activities\n    + <antonietta.florio@hec.ca>\n    + 514-340-6473\n    + Office: 4.632\n    + Availability:\n        - From Monday to Friday\n        - From 8 am to 12 pm and from 1 pm to 4:30 pm \n* **Jennifer Caron**\n    + Executive Assistant\n    + <jennifer.caron@hec.ca>\n    + 514-340-6473\n    + Office: 4.632\n    + Availability:\n        - From Monday to Friday\n        - From 8 am to 12 pm and from 1 pm to 4:30 pm\n:::\n\n### Bibliographic Sources\n::: {.callout-note appearance=\"minimal\" collapse=true}\n* The Elements of Statistical Learning: Data Mining, Inference, and Prediction\n    + [Available online](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n\n \n* Goodfellow, Ian . Deep learning , MIT Press\nISBN: 9780262035613\n    + [Available online](https://www.deeplearningbook.org)\n \n* Richard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n    + [Available online](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwjAx8CEmsLbAhUFjlkKHd3hBnQQFggpMAE&url=http%3A%2F%2Fincompleteideas.net%2Fbook%2Fbookdraft2017nov5.pdf&usg=AOvVaw00kFmqVbFSdkU3PTkJMJrO)\n \n* Murphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press\nISBN: 9780262018029\n    + Available at the library\n \n* Ricci, Francesco. Recommender systems handbook, Springer\nISBN: 148997637X\n    + Available at the library\n \n* Parsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O'Reilly Media\nISBN: 9781491906156\n    + Available at the library\n \n* McKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O'Reilly Media\n    + Available at the library\n \n* Bishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York\nISBN: 9780387310732\n    + Available at the library\n \n* Leskovec, Jurij. Mining of massive datasets, Cambridge University Press\n    + Available at the library\n \n* Grus, Joel. Data science from scratch, O'Reilly Media\n    + Available at the library\n \n* Sandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O'Reilly Media\n    + Available at the library\n:::\n\n### Class Information\n::: {.callout-note appearance=\"minimal\" collapse=false}\n* Location\n    + Building C-Ste-Cath, Room Groupe Cholette\n* Time\n    + Tuesdays, 3:30 PM - 6:30 PM EDT\n:::\n\n## Evaluations\n### [Bonus] Class participation (5%)\n\nTo receive this 5% bonus point:\n\n1. Prepare for each session's code before the class starts.\n2. Log in to [deepnote](https://www.deepnote.com) with your **Full Name**.\n3. During the coding exercises in class, add your answer to the questions as a *comment*.\n\n### Homework (20%)\n* Due on 24 October 2023\n* Homework assignments are counted for 20% of your final grade.\n* The homework should be done in jupyter notebook [here](MATH60629A_Homework.ipynb) or Google Colab [here](https://colab.research.google.com/github/davoodwadi/MATH60629A.A2023-MACHINE-LEARNING-I/blob/main/MATH60629A_Homework.ipynb).\n\n* **Instructions**:\n    + The homework is due by 11:59PM EST on the due date. \n    + Please upload a PDF version of your assignment on ZoneCours and insert the shareable link to your notebook in the same interface.\n    + Homeworks must be completed individually. \n        - Discussion with others is okay, but you must write solutions yourself.\n    + All code used to arrive at answers is submitted along with answers.\n    + **Notes:**\n        - Please provide your code answers in the code block under each question and verbal answers in text boxes assigned in the notebook (where applicable).\n        - Please run the notebook before the submission so that the outputs are displayed.\n        - Please make sure that your results are reproducible. You may use random seeds from `random` and `numpy` packages. For `scikit-learn` modules, you may use the `random_state` argument.\n\n### Project (30%)\nThe aim of this project is to allow you to learn about machine learning by trying to solve a task with it.\n\nFirst, select a question that can be answered using machine learning. I expect that your question will be about a model/algorithm or about an application. Then design a study that will try to answer your question. Your study must have an element of novelty. For example the novelty could be an extension or a variation of an existing algorithm or results of an existing method on a new dataset.\n\nYour study should involve reading and understanding some background material. Your study must involve running some experiments. You are free to use (or not) any of the tools or models we have seen in class.\n\n\n**Study plan**: *(1 upload per group)* Please submit a one-page summary of your proposed research question and study to ZoneCours. I will meet with each group to discuss study plans during the lecture on Week 9. I will send you a schedule the day before. We will probably only have about 15 minutes so please make sure that your study plan is clear and precise. You may also include questions that you would like us to discuss at the end of the document.\n\n**The group report**: *(1 upload per group)* Your report must contain a description of the question you are trying to answer, a clear description of the model/algorithm you are studying, a survey of related work which proper references, an empirical section that reports your results, and a conclusion that summarizes your findings and (if pertinent) highlights possible future directions of investigation. Your report should be no longer than 10 pages in length (plus references) for pairs or 13 pages (plus references) for teams of three.\n\n\n**The individual report**: *(1 upload per student)* You will also submit a brief individual report (at most one page), which will: (1) Describe the parts of the project you worked on (which machine learning methods you applied, which preprocessing steps you performed on the data, which parts of the term paper you wrote, who you worked with on what parts, etc.) and what parts of the project your teammates worked on. (2) What you learned from the project. The purpose of the individual report is to facilitate fair grading and to allow the instructor to understand well what you learned from the project.\n\nProject Report (30%)\n\n* Clarity/Relevance of problem statement and description of approach: 10%\n* Discussion of relationship to previous work and references: 4%\n* Design and execution of experiments: 10%\n* Figures/Tables/Writing: easily readable, properly labeled, \n* informative: 5%\n* Indiviual report: 1%\n\n\n#### Project Topic\n\nConcretely, your project could take one of the following forms:\n\n* A competition on [kaggle.com](https://kaggle.com) that you try multiple ML methods on and can achieve a validation set performance in the top 100 on the leaderboard.\n* A benchmark task on [paperswithcode](https://paperswithcode.com) (e.g. [Speech Recognition](https://paperswithcode.com/task/speech-recognition)) where you try multiple methods and compare their performance.\n* A task in your own field whereby you apply multiple ML methods to solve the task.\n* An ML method that you apply to a new task (e.g. [Multi-head attention](https://paperswithcode.com/method/multi-head-attention) applied to time-series prediction)\n\n*Note*. The list above is not exhaustive. You are encouraged to be creative about the topic. Pick a topic that you find interesting and is relevant. The appropriateness of the topic will be discussed and resolved in the session on [Study plan](#timeline).\n\n#### Group Report Structure\nYour group report should read like a published paper. Similarly, your poster presentation should have a similar structure to conference posters. You might want to check out similar papers to your topic on the structure of your report (e.g. [A Comparison of Deep Learning Approach for Underwater Object Detection](https://www.jurnal.iaii.or.id/index.php/RESTI/article/view/3931/573)).\n\nRegardless of your topic, as a general rule of thumb, your Group Report should be structured as follows:\n\n1. Introduction:\n    Use this section to introduce the topic, its relevance, and a summary of the whole paper. This section should address the question, \"Why the target audience of the paper should care about your work?\"\n\n2. Related Work:\n    This section discusses what has been done, in prior research, on your topic. While most of the cited papers should be from academic publications, it is not uncommon to see non-academic references in papers (e.g. [stackoverflow](https://stackoverflow.com)). You might want to use academic search engines (e.g. [SemancticScholar](https://semanticscholar.org)) to find relavant papers.\n\n3. Experiments\n    This section should cover the steps you take to answer the question you posed in the Introduction. This includes data preparation, model and hyper-parameter selection, and evaluation steps.\n\n4. Results\n    In this section you present the resutls of your experiments with appropriate tables and figures.\n\n5. Conclusion\n    In your final section, you would summarize what you did in the paper, acknowledge the limitations of your work (All papers have this; no work is perfect.), suggest future research directions to address your limitations.\n\n#### Timeline\n\n* Team Registration, due: **October 1**. Fill this [form](https://forms.gle/DkNsrh8xpYyU1kAh6).\n* Study plan, due: **October 28** (by the end of the day EDT).\n* Handing in: Through *ZoneCours*\n* Project meeting, **October 31**\n* Project Presentation, due: **December 1**. Upload the PDF of your poster/slides to *ZoneCours*.\n* In-class Presentation, on **December 5**.\n* Final individual report, due: **December 15**, 2023 (by the end of the day EDT).\n    + Handing in: Through *ZoneCours* (per each team member).\n\n\n### Project Presentation (10%)\nMake a poster that describes your project [**You do not need to print your poster**]. You can think of a poster as supporting material for your oral presentation (in that way it is similar to slides). It could also follow a similar structure: begin by motivating your work, then (quickly) highlight related work, talk in depth about your solution, then go into results (pictures and tables are good tools for that), finally conclude and perhaps mention one or two ideas for future work.\n\n* Project Presentation (10%)\n    + Clarity of presentation: 3%\n    + Slide or Poster quality: 2%\n    + Correctness: 2%\n    + Answers to questions: 3%\n\n\n\n### Final Exam (30%)\n+ Dec 14, 2023\n+ 9:00 am - 12:00 pm EDT\n+ Location to be communicated\n\n[Past exam - Fall 2018](exam_80629_A18.pdf)\n\n### Capsule Quizzes (10%)\n* In-class quizzes of the capsules\n* Capsule quizzes are counted for 10% of your final grade.\n* Time to complete each quiz is 10 minutes.\n    + Quiz 1, **Sep 20, 2023 11:30 AM to Sep 22, 2023 11:30 AM**.\n    + Quiz 2, **Sep 27, 2023 11:30 AM to Sep 29, 2023 11:30 AM**.\n    + Quiz 3 - CNNs and RNNs, **Nov 6, 2023 12:00 AM to Nov 8, 2023 12:00 AM**.\n    + Quiz 4 - Unsupervised Learning, **Nov 13, 2023 12:00 AM to Nov 15, 2023 12:00 AM**.\n    + Quiz 5 - Parallel Computing, **Nov 20, 2023 12:00 AM to Nov 22, 2023 12:00 AM**.\n    + Quiz 6 - Reinforcement Learning, **Nov 27, 2023 12:00 AM to Nov 29, 2023 12:00 AM**.\n\n## In-class Tools\n* [Cups pacing webapp](https://ml1-pace.onrender.com/ml1)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":4,"output-file":"courses.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"cosmo","title":"MATH60629A - Machine Learning I (Fall 2023)","aliases":["index.html"],"resources":["*.pdf"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}