{
  "hash": "53f4924e15013c404cd57b5ec52cdd6f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\n\n\nSure! PyTorch is a popular open-source machine learning library for Python. It provides an efficient and flexible framework for building deep learning models. Many beginners in PyTorch are already familiar with Numpy, which is another Python library used for numerical computing. In this tutorial, we will introduce PyTorch using the similarities between PyTorch and Numpy.\n\nTo begin, let's start by installing PyTorch using the following command:\n\n::: {#b183010c .cell execution_count=1}\n``` {.python .cell-code}\n!pip install torch\n```\n:::\n\n\nAfter installing PyTorch, we can import it into our Python code using the following line:\n\n::: {#9e1bdd55 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\nNow, let's draw some comparisons between PyTorch and Numpy.\n\n## Tensors and Arrays\n\nPyTorch uses tensors to store and manipulate data, while Numpy uses arrays. Tensors and arrays behave similarly and share many common operations. For example, creating a tensor/array, accessing elements, and performing basic operations are done in a similar way.\n\nLet's compare how we can create a tensor and an array in PyTorch and Numpy respectively:\n\n::: {#136ca92a .cell execution_count=3}\n``` {.python .cell-code}\n# Creating a PyTorch tensor\ntensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(tensor)\n\n# Creating a Numpy array\nimport numpy as np\narray = np.array([[1, 2, 3], [4, 5, 6]])\nprint(array)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n[[1 2 3]\n [4 5 6]]\n```\n:::\n:::\n\n\nBoth the PyTorch tensor and Numpy array store data in a multi-dimensional structure.\n\n## Accessing Elements\n\nAccessing elements in PyTorch tensors and Numpy arrays is similar, as both use indexing and slicing methods.\n\n::: {#76e442da .cell execution_count=4}\n``` {.python .cell-code}\n# Accessing elements in PyTorch tensor\nprint(tensor[1, 2])  # Output: 6\n\n# Accessing elements in Numpy array\nprint(array[1, 2])  # Output: 6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(6)\n6\n```\n:::\n:::\n\n\n## Basic Operations\n\nBasic operations such as addition, subtraction, multiplication, and division can be easily performed on PyTorch tensors and Numpy arrays.\n\n::: {#e30e510b .cell execution_count=5}\n``` {.python .cell-code}\n# Perform addition on PyTorch tensor\ntensor_sum = tensor + tensor\nprint(tensor_sum)\n\n# Perform addition on Numpy array\narray_sum = array + array\nprint(array_sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12]])\n[[ 2  4  6]\n [ 8 10 12]]\n```\n:::\n:::\n\n\nBoth the tensor_sum and array_sum will output the element-wise sum of their respective objects.\n\n## Shape and Reshaping\n\nThe shape and reshaping of tensors and arrays are crucial operations in both libraries. Here's how we can determine the shape of a tensor/array and reshape it:\n\n::: {#989f89da .cell execution_count=6}\n``` {.python .cell-code}\n# Shape of PyTorch tensor\nprint(tensor.shape)  # Output: torch.Size([2, 3])\n\n# Shape of Numpy array\nprint(array.shape)  # Output: (2, 3)\n\n# Reshaping PyTorch tensor\ntensor_reshaped = tensor.view(3, 2)\nprint(tensor_reshaped)\n\n# Reshaping Numpy array\narray_reshaped = array.reshape(3, 2)\nprint(array_reshaped)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([2, 3])\n(2, 3)\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n[[1 2]\n [3 4]\n [5 6]]\n```\n:::\n:::\n\n\n## Automatic Differentiation\n\nOne of the key advantages of PyTorch over Numpy is its ability to perform automatic differentiation. It makes training deep learning models much easier by computing gradients automatically.\n\n::: {#34ff9647 .cell execution_count=7}\n``` {.python .cell-code}\n# Enable automatic differentiation in PyTorch\ntensor = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n\n# Perform a forward operation\noutput = tensor.sum()\n\n# Perform automatic differentiation\noutput.backward()\n\n# Access the gradients\nprint(tensor.grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n```\n:::\n:::\n\n\nThe `requires_grad=True` argument enables the computation of gradients for the tensor. The `output.backward()` function computes the gradients of `output` with respect to all tensors that have `requires_grad=True`. Finally, `tensor.grad` retrieves the gradients.\n\nThese are some of the similarities between PyTorch and Numpy. Understanding these similarities can make it easier for beginners to transition from Numpy to PyTorch.\n\n",
    "supporting": [
      "pytorch_intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}