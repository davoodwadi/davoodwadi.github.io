{
  "hash": "8a4fedc60792bef25720d68f5b5f94fb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\n---\n\n\n\nA confusion matrix is a useful tool for evaluating the performance of a classification model. It provides a tabular representation of the predicted and actual classes of a binary classification problem. The matrix helps us understand how well the model is performing by showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n\nLet's define the confusion matrix for a binary classification problem using the following notations:\n\n- TP: Number of true positive predictions\n- TN: Number of true negative predictions\n- FP: Number of false positive predictions\n- FN: Number of false negative predictions\n\nTo illustrate this, let's consider a dataset of 100 samples. Our binary classifier predicts whether a sample is positive or negative. After running the prediction, we obtain the following results:\n\n- There are 60 true positive predictions (TP = 60).\n- There are 30 true negative predictions (TN = 30).\n- There are 5 false positive predictions (FP = 5).\n- There are 5 false negative predictions (FN = 5).\n\nNow, let's plot these values in a confusion matrix:\n\n|                 | Predicted Positive | Predicted Negative |\n|-----------------|-------------------|--------------------|\n| Actual Positive | TP = 60           | FN = 5             |\n| Actual Negative | FP = 5            | TN = 30            |\n\nIn Python, we can use the scikit-learn library to calculate the confusion matrix. Here's an example of how to compute the confusion matrix for binary classification:\n\nStep 1: Import the necessary libraries\n\n::: {#c968dd6b .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n```\n:::\n\n\nStep 2: Define the actual and predicted classes\n\n::: {#433f32e3 .cell execution_count=2}\n``` {.python .cell-code}\nactual = np.array([1, 1, 0, 0, 1, 0, 0, 1, 0, 1])\npredicted = np.array([1, 0, 0, 1, 1, 0, 1, 1, 0, 0])\n```\n:::\n\n\nStep 3: Calculate the confusion matrix using the `confusion_matrix` function\n\n::: {#4027babe .cell execution_count=3}\n``` {.python .cell-code}\ncm = confusion_matrix(actual, predicted)\nprint(cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[3 2]\n [2 3]]\n```\n:::\n:::\n\n\nOutput:\n```\n[[3 2]\n [2 3]]\n```\n\nIn this example, we have 3 true positive predictions, 3 true negative predictions, 2 false positive predictions, and 2 false negative predictions. Hence, the confusion matrix is:\n\n|                 | Predicted Positive | Predicted Negative |\n|-----------------|-------------------|--------------------|\n| Actual Positive | TP = 3            | FN = 2             |\n| Actual Negative | FP = 2            | TN = 3             |\n\nThe confusion matrix provides essential information for evaluating the performance of a binary classification model, such as accuracy, precision, recall, and F1 score. It helps us understand the model's strengths and weaknesses, identify any imbalances in the predictions, and make informed decisions about improving the model.\n\n",
    "supporting": [
      "confus_files"
    ],
    "filters": [],
    "includes": {}
  }
}