{
  "hash": "e9b16dbb1a8bef7d5c7141b59e11eaa0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\nexecute: \n  cache: true\neval: false\n---\n\n\n\n\n\nTo train the CIFAR-10 dataset using PyTorch's DataParallel, we first need to have a basic understanding of what CIFAR-10 is and what PyTorch's DataParallel does.\n\nCIFAR-10 is a popular computer vision dataset that consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.\n\nPyTorch's DataParallel is a module that allows us to use multiple GPUs for training our models. It splits the input data across the GPUs, performs forward and backward passes on each GPU, and then aggregates the gradients to update the model parameters. This helps in speeding up the training process and achieving better performance.\n\nNow, let's dive into the code.\n\nFirst, we need to import the necessary libraries and modules:\n\n::: {#30b63c01 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n```\n:::\n\n\nNext, we define the CIFAR-10 dataset and its transformations:\n\n::: {#2a9dceb7 .cell execution_count=2}\n``` {.python .cell-code}\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n                                             download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n                                            download=True, transform=transform)\n```\n:::\n\n\nThen, we define the data loaders for our training and testing datasets:\n\n::: {#aa043918 .cell execution_count=3}\n``` {.python .cell-code}\nbatch_size = 128\nnum_workers = 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          shuffle=True, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, \n                         shuffle=False, num_workers=num_workers)\n```\n:::\n\n\nNow, let's define our model:\n\n::: {#88a70934 .cell execution_count=4}\n``` {.python .cell-code}\nclass CIFAR10Net(nn.Module):\n    def __init__(self):\n        super(CIFAR10Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout(x)\n        x = x.view(-1, 64 * 16 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CIFAR10Net()\n```\n:::\n\n\nAfter defining our model, we need to define the loss function and the optimizer:\n\n::: {#09ca7a50 .cell execution_count=5}\n``` {.python .cell-code}\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n```\n:::\n\n\nNow, we are ready to train our model:\n\n::: {#5069280a .cell execution_count=6}\n``` {.python .cell-code}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    \n    for input, target in train_loader:\n        input, target = input.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for input, target in test_loader:\n            input, target = input.to(device), target.to(device)\n            \n            output = model(input)\n            _, predictions = torch.max(output, 1)\n            \n            total_correct += (predictions == target).sum().item()\n            total_samples += target.size(0)\n    \n    accuracy = total_correct / total_samples\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}\")\n```\n:::\n\n\nFinally, we can evaluate our model on the test set:\n\n::: {#821172ca .cell execution_count=7}\n``` {.python .cell-code}\nmodel.eval()\n\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for input, target in test_loader:\n        input, target = input.to(device), target.to(device)\n\n        output = model(input)\n        _, predictions = torch.max(output, 1)\n\n        total_correct += (predictions == target).sum().item()\n        total_samples += target.size(0)\n\naccuracy = total_correct / total_samples\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n```\n:::\n\n\nThat's it! We have successfully trained the CIFAR-10 dataset using PyTorch's DataParallel module.\n\n",
    "supporting": [
      "DataParallel_files"
    ],
    "filters": [],
    "includes": {}
  }
}