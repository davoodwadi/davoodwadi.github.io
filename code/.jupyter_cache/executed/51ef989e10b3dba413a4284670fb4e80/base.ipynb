{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fcb5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/Users/davoodwadi/MLCourse/davoodwadi.github.io/code':\n",
    "  os.chdir(r'/Users/davoodwadi/MLCourse/davoodwadi.github.io/code')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c29979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Policy Evaluation\n",
    "def policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta):\n",
    "    # Initialize the state-value function with zeros\n",
    "    V = {s: 0 for s in states}\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        # Iterate over all states\n",
    "        for s in states:\n",
    "            v = V[s]\n",
    "            new_v = 0\n",
    "            \n",
    "            # Iterate over all possible actions\n",
    "            for a in actions:\n",
    "                # Compute the expected return for each action\n",
    "                next_states = transitions[s][a]\n",
    "                expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n",
    "\n",
    "                # Compute the new state-value function\n",
    "                new_v += policy[s][a] * expected_return\n",
    "\n",
    "            # Update the state-value function for the current state\n",
    "            V[s] = new_v\n",
    "\n",
    "            # Calculate the maximum difference between the old and new state-value functions\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        # If the maximum difference is less than a threshold, we assume convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Step 2: Policy Improvement\n",
    "def policy_improvement(states, actions, rewards, transitions, discount_factor, V):\n",
    "    # Initialize the new policy with zeros\n",
    "    new_policy = {s: {a: 0 for a in actions} for s in states}\n",
    "    \n",
    "    for s in states:\n",
    "        action_values = []\n",
    "        \n",
    "        # Iterate over all possible actions\n",
    "        for a in actions:\n",
    "            # Compute the expected return for each action\n",
    "            next_states = transitions[s][a]\n",
    "            expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n",
    "            \n",
    "            # Add the expected return to the list of action values\n",
    "            action_values.append(expected_return)\n",
    "        \n",
    "        # Assign the action with the maximum expected return as the new policy for the current state\n",
    "        idx = np.argmax(action_values)\n",
    "        new_policy[s][actions[idx]] = 1\n",
    "    \n",
    "    return new_policy\n",
    "\n",
    "# Step 3: Policy Iteration\n",
    "def policy_iteration(states, actions, rewards, transitions, discount_factor, theta):\n",
    "    # Initialize a random policy\n",
    "    policy = {s: {a: 1 / len(actions) for a in actions} for s in states}\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = policy_improvement(states, actions, rewards, transitions, discount_factor, V)\n",
    "        \n",
    "        # If the new policy is the same as the old policy, we have converged to the optimal policy\n",
    "        if policy == new_policy:\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}