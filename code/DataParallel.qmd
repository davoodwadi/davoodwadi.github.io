
---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  cache: true
eval: false
---


To train the CIFAR-10 dataset using PyTorch's DataParallel, we first need to have a basic understanding of what CIFAR-10 is and what PyTorch's DataParallel does.

CIFAR-10 is a popular computer vision dataset that consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.

PyTorch's DataParallel is a module that allows us to use multiple GPUs for training our models. It splits the input data across the GPUs, performs forward and backward passes on each GPU, and then aggregates the gradients to update the model parameters. This helps in speeding up the training process and achieving better performance.

Now, let's dive into the code.

First, we need to import the necessary libraries and modules:

```{python}
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
```

Next, we define the CIFAR-10 dataset and its transformations:

```{python}
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, 
                                             download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, 
                                            download=True, transform=transform)
```

Then, we define the data loaders for our training and testing datasets:

```{python}
batch_size = 128
num_workers = 2

train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                          shuffle=True, num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                         shuffle=False, num_workers=num_workers)
```

Now, let's define our model:

```{python}
class CIFAR10Net(nn.Module):
    def __init__(self):
        super(CIFAR10Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout(0.25)
        self.fc1 = nn.Linear(64 * 16 * 16, 512)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.dropout(x)
        x = x.view(-1, 64 * 16 * 16)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

model = CIFAR10Net()
```

After defining our model, we need to define the loss function and the optimizer:

```{python}
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

Now, we are ready to train our model:

```{python}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    
    for input, target in train_loader:
        input, target = input.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(input)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    
    model.eval()
    
    total_correct = 0
    total_samples = 0
    
    with torch.no_grad():
        for input, target in test_loader:
            input, target = input.to(device), target.to(device)
            
            output = model(input)
            _, predictions = torch.max(output, 1)
            
            total_correct += (predictions == target).sum().item()
            total_samples += target.size(0)
    
    accuracy = total_correct / total_samples
    
    print(f"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}")
```

Finally, we can evaluate our model on the test set:

```{python}
model.eval()

total_correct = 0
total_samples = 0

with torch.no_grad():
    for input, target in test_loader:
        input, target = input.to(device), target.to(device)

        output = model(input)
        _, predictions = torch.max(output, 1)

        total_correct += (predictions == target).sum().item()
        total_samples += target.size(0)

accuracy = total_correct / total_samples

print(f"Test Accuracy: {accuracy:.4f}")
```

That's it! We have successfully trained the CIFAR-10 dataset using PyTorch's DataParallel module.