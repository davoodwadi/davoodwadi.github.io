
---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  cache: true
---


In the field of machine learning and classification, evaluating the model's performance is crucial. One common way to evaluate classification models is by using a confusion matrix. The confusion matrix provides a detailed breakdown of the model's predictions and their corresponding actual labels. From the confusion matrix, we can calculate several performance metrics, including precision, recall, and F1-score.

Let's start by understanding what a confusion matrix is.

A confusion matrix is a table that visualizes the performance of a classification model. It consists of four different values:

- True Positive (TP): The number of positive instances that the model correctly predicted as positive.
- False Positive (FP): The number of negative instances that the model incorrectly predicted as positive.
- True Negative (TN): The number of negative instances that the model correctly predicted as negative.
- False Negative (FN): The number of positive instances that the model incorrectly predicted as negative.

The confusion matrix is typically presented in the following format:

|         | Predicted Positive | Predicted Negative |
|---------|--------------------|--------------------|
| Actual Positive |      TP          |        FN          |
| Actual Negative |      FP          |        TN          |


Now, let's calculate precision, recall, and F1-score using the confusion matrix.

**Precision** measures the accuracy of positive predictions. It is calculated using the formula:


$\text{{Precision}} = \frac{TP}{{TP + FP}}$

**Recall**, also known as the sensitivity or true positive rate, measures the proportion of actual positive instances that were correctly classified. It is calculated using the formula:


$\text{{Recall}} = \frac{TP}{{TP + FN}}$

**F1-score** is the harmonic mean of precision and recall. It provides a balanced measure between the two. F1-score is calculated using the formula:


$F1 = \frac{2 \times \text{{Precision}} \times \text{{Recall}}}{{\text{{Precision}} + \text{{Recall}}}}$
