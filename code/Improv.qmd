---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  enabled: true
---

### L2 Regularization

In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:

\begin{equation}
y = X\beta + \epsilon
\end{equation}

where:

- $y$ is the dependent variable (target variable)
- $X$ is the design matrix that consists of independent variables (features)
- $\beta$ is the vector of coefficients (slopes) for each feature in X
- $\epsilon$ is the error term

The Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients $\beta$ that minimize the sum of squared residuals.

L2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.

L2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter $\lambda$.

The loss function for multiple linear regression with L2 regularization is given by:

\begin{equation}
L(\beta) = \frac{1}{N} \left[\sum_{i=1}^{N} (y_i - X_i\beta)^2 + \lambda \|\beta\|^2\right]
\end{equation}

where:

- $N$ is the number of samples in the dataset
- $y_i$ is the target value for the i-th sample
- $X_i$ is the feature vector for the i-th sample
- $\|\beta\|^2$ represents the squared L2 norm of the coefficient vector $\beta$

The goal is to find the value of $\beta$ that minimizes this loss function.

Let's now implement L2 regularization for multiple linear regression using Python:

First, let's import the necessary libraries:

```{python}
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

Next, we'll generate some random data for demonstration purposes:

```{python}
# Generate random data
np.random.seed(0)
X = np.random.rand(100, 5)
w = np.random.rand(5, 1)
y = X**2 @ w + np.random.randn(100)*5
```

We split the data into training and test sets:

```{python}
# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

To apply L2 regularization, we need to scale the input features using the StandardScaler:

```{python}
# Scale the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

Let's use OLS without L2 regularization for a regression model:

```{python}
# Create a regression model without penalty
no_ridge = Ridge(alpha=0.0)  # alpha is the regularization parameter (lambda)

# Train the model
no_ridge.fit(X_train_scaled, y_train)
```

Next, we can evaluate the trained model on the test set:

```{python}
# Evaluate the model on the test set
score = no_ridge.score(X_test_scaled, y_test)
print(f"No-penalty Regression Score: {score}")
```

The `score` represents the coefficient of determination $(R^2)$ of the prediction. Higher values of $R^2$ indicate better model performance.


We can now create and train the ridge regression model:

```{python}
# Create a ridge regression model
ridge = Ridge(alpha=10.0)  # alpha is the regularization parameter (lambda)

# Train the model
ridge.fit(X_train_scaled, y_train)
```

Finally, we can evaluate the trained model on the test set:

```{python}
# Evaluate the model on the test set
score = ridge.score(X_test_scaled, y_test)
print(f"Ridge Regression Score: {score}")
```

By applying L2 regularization using ridge regression, we can improve the generalization of the multiple linear regression model by reducing overfitting and improving its performance on unseen data.