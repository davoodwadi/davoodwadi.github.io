
---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  cache: true
eval: false
---


To train the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`, we need to perform the following steps:

1. Import the necessary libraries:

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel
import torch.multiprocessing as mp
import torchvision
import torchvision.transforms as transforms
```

2. Define the model architecture:

```{python}
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(32 * 16 * 16, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = x.view(-1, 32 * 16 * 16)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = CNN()
```

3. Define the training function:

```{python}
def train(rank, world_size):
    torch.manual_seed(0)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize distributed training
    dist.init_process_group(backend='nccl', init_method='env://', rank=rank, world_size=world_size)

    # Load CIFAR-10 dataset
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,
                                        transform=transforms.ToTensor())
    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset, num_replicas=world_size, rank=rank)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=False, sampler=train_sampler)

    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

    # Transfer model to GPU
    model.to(device)

    # Create DistributedDataParallel model
    model = DistributedDataParallel(model)

    # Training loop
    num_epochs = 10
    for epoch in range(num_epochs):
        train_loss = 0.0
        train_correct = 0
        total = 0

        for inputs, labels in trainloader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Compute metrics
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            train_loss += loss.item()

        # Print epoch results
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {(train_correct/total)*100:.2f}%")

    # Cleanup
    dist.destroy_process_group()
```

4. Define the main function for multi-processing:

```{python}
def main():
    world_size = torch.cuda.device_count()
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)

if __name__ == '__main__':
    main()
```

5. Run the main function to start the training process:

```{python}
python train_cifar10_distributed.py
```

This code will train a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch's `DistributedDataParallel`. The `DistributedDataParallel` module enables us to train models on multiple GPUs or machines.

The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat, etc.). The dataset is divided into 50,000 training images and 10,000 testing images.

In the code above, we define a CNN model with two convolutional layers and two fully connected layers. We then load the CIFAR-10 dataset, define the loss function and optimizer, and transfer the model to the GPU if available.

We create a `DistributedDataParallel` model from the base model to enable distributed training. The `DistributedDataParallel` wrapper will automatically scatter the input data to the available GPUs and gather the gradients during the backward pass.

Next, we define the training loop, where we iterate over the batches of the training dataset. For each batch, we compute the forward pass, the loss, and perform the backward pass and optimization. We also keep track of the training loss and accuracy.

After each epoch, we print the epoch's training loss and accuracy. Finally, we clean up the distributed training environment and destroy the process group.

To run the code, we use the `mp.spawn()` function to parallelize the training process across multiple processes, where each process trains on a separate GPU. The `world_size` parameter is set to the number of available GPUs.