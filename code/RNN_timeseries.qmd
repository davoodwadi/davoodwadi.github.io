---
title: ""
format:
  html:
    code-fold: false
jupyter: python3
execute: 
  cache: true
---


In this tutorial, I will explain how to use a Recurrent Neural Network (RNN) to predict time series data using the PyTorch library. 

First, let's start by installing the required packages. We need to install PyTorch and matplotlib for visualization.

```
!pip install torch
!pip install matplotlib
```

Now that we have the necessary packages installed, let's import them into our code.

```{python}
import matplotlib.pyplot as plt
```

```{python}
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
```

The next step is to generate some synthetic time series data for training our model. We will generate a sine wave with some added noise.

```{python}
# Set random seed for reproducibility
torch.manual_seed(0)

# Generate time points
num_points = 1000
t = torch.arange(0, num_points)

# Generate sine wave
value = torch.sin(0.1*t)

# Add noise
value += 0.2*torch.randn(num_points)
```

Let's visualize the generated time series using matplotlib.

```{python}
plt.figure(figsize=(10, 6))
# using .tolist method instead of
# .numpy() because of dependency issues
# in numpy>2 and torch>2
plt.plot(t, value)
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Time Series Data')
plt.show()
```

Now, we need to prepare our data for training the RNN. We will split the data into input sequences and corresponding output sequences. Each input sequence will contain a certain number of time steps (e.g., 10), and the corresponding output sequence will be the next time step.

```{python}
# Define number of time steps for input sequence
num_steps = 10

# Split data into input and output sequences
x = []
y = []
for i in range(len(value) - num_steps):
    # creating list of x; each item has num_steps values
    x.append(value[i:i+num_steps])
    # creating list of y; each item has the next value
    y.append(value[i+num_steps])

# Convert data to PyTorch tensors
# turning list x to tensor & adding new dimension (input size) at the end
x = torch.stack(x)[..., None]
# turning list y to tensor & adding new dimension at the end
y = torch.stack(y)[..., None]

x_train = x[0:100]
x_test = x[100:]

y_train = y[0:100]
y_test = y[100:]

print(x_train.shape)
print(y_train.shape)
```

Next, we need to define our RNN model using the nn.RNN module from PyTorch.

```{python}
# Define RNN model
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        # If batch_first=True, then the input and output tensors are provided as (batch, seq, feature)
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        # nn.Linear to transform hidden state from hidden_size to output_size
        self.fc = nn.Linear(hidden_size, output_size)
    
    def init_h(self, x):
        '''
        To initialize the hidden state for the first iteration
        shape: d x num_layers, N, H if input is batched (our case)
        shape: d x num_layers, H if input not batched
        d: 2 if bidirectional else 1
        N: batch size
        H: hidden_size
        num_layers: number of RNN layers
        '''
        batch_size = x.shape[0]
        self.h0 = torch.zeros(1, batch_size, self.hidden_size, requires_grad=True)
        return self.h0

    def forward(self, x, hidden):
        # shape: N, L, DxH 
        # shape: 990, 10, 16
        out, hidden = self.rnn(x, hidden)
        # choose the last output of out: out[:, -1, :]
        # pass it to the fc layer
        out = self.fc(out[:, -1, :])
        return out, hidden

# Create an instance of the RNN model
input_size = 1
hidden_size = 16
output_size = 1
model = RNN(input_size, hidden_size, output_size)
```

Note that the `self.fc = nn.Linear(hidden_size, output_size)` transforms the hidden state from `hidden_size` to `output_size`. This is a useful use case of fully connected layers, which helps us transform our feature space from $n$ dimensions to $n\prime$ dimensions.


Before training our model, we need to define the loss function and optimizer.

```{python}
# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
```

Now, we can train our RNN model using a loop. We will iterate over the data for a certain number of epochs and update the model parameters based on the computed loss.

```{python}
# Set number of epochs
num_epochs = 100
# initialize the hidden state
h = model.init_h(x_train)
# Train the model
for epoch in range(num_epochs):
    # Forward pass
    output, h = model(x_train, h)
    loss = criterion(output, y_train)
    
    # Backward and optimize
    loss.backward()
    optimizer.step()
    # set the gradients of the model tensors to zero
    optimizer.zero_grad()
    # detach h from the computation graph
    h.detach_()
    
    # Print progress
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

Finally, let's visualize the predicted values and compare them with the ground truth.

```{python}
h = model.init_h(x_test)
# Generate predictions
with torch.no_grad():
    y_pred, h = model(x_test, h)
# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(t.tolist(), value.tolist(), label='Ground Truth')
plt.plot(t[10:110].tolist(), output.detach().tolist(), label='Predicted train')
plt.plot(t[110:].tolist(), y_pred.detach().tolist(), label='Predicted test')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Time Series Prediction')
plt.legend()
plt.show()
```
