[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "I’m a researcher and educator with a focus on Digital Marketing and the application of Artificial Intelligence in Marketing.\nI currently teach the following courses:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\nWeek 4\n\nIntroduction to PyTorch (Part 1) (colab)\n\nWeek 5\n\nIntroduction to PyTorch (Part 2)\nDataset class in PyTorch\nBatching and Dataloader in PyTorch\nBackpropagation with PyTorch\nLinear regression with gradient descent\nClassification using MLP with Pytorch\n\nWeek 6\n\nMinibatch training in Pytorch\nIntroduction to RNNs\n\nWeek 10\n\nMulti-GPU training with DataParallel\nMulti-Node training with DistributedDataParallel\n\nWeek 11\n\nSingular Value Decomposition with PyTorch\nSingular Value Decomposition for MovieLens100k\n\nWeek 12\n\nValue iteration\nPolicy iteration\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Courses",
    "section": "About this blog",
    "text": "About this blog\nI currently teach the following courses:"
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "Davood Wadi",
    "section": "Courses",
    "text": "Courses\n+ MIT: *Applied Data Science Program*\n+ HEC Montreal: *MATH60629A - Machine Learning I*"
  },
  {
    "objectID": "index.html#experiences",
    "href": "index.html#experiences",
    "title": "Davood Wadi",
    "section": "Experiences",
    "text": "Experiences\n\nPhD in Marketing - HEC Montreal (Best 2023 Thesis Award Nominee)\nPartner with intelChain.io as AI Scientist"
  },
  {
    "objectID": "courses.html#course-outline",
    "href": "courses.html#course-outline",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Davood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT\n\nFinal Exam\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated"
  },
  {
    "objectID": "courses.html#course-information",
    "href": "courses.html#course-information",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\nWeek 4\n\nIntroduction to PyTorch (Part 1) (colab)\n\nWeek 5\n\nIntroduction to PyTorch (Part 2)\nDataset class in PyTorch\nBatching and Dataloader in PyTorch\nBackpropagation with PyTorch\nLinear regression with gradient descent\nClassification using MLP with Pytorch\n\nWeek 6\n\nMinibatch training in Pytorch\nIntroduction to RNNs\n\nWeek 10\n\nMulti-GPU training with DataParallel\nMulti-Node training with DistributedDataParallel\n\nWeek 11\n\nSingular Value Decomposition with PyTorch\nSingular Value Decomposition for MovieLens100k\n\nWeek 12\n\nValue iteration\nPolicy iteration\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "courses.html#evaluations",
    "href": "courses.html#evaluations",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "Evaluations",
    "text": "Evaluations\n\n[Bonus] Class participation (5%)\nTo receive this 5% bonus point:\n\nPrepare for each session’s code before the class starts.\nLog in to deepnote with your Full Name.\nDuring the coding exercises in class, add your answer to the questions as a comment.\n\n\n\nHomework (20%)\n\nDue on 24 October 2023\nHomework assignments are counted for 20% of your final grade.\nThe homework should be done in jupyter notebook here or Google Colab here.\nInstructions:\n\nThe homework is due by 11:59PM EST on the due date.\nPlease upload a PDF version of your assignment on ZoneCours and insert the shareable link to your notebook in the same interface.\nHomeworks must be completed individually.\n\nDiscussion with others is okay, but you must write solutions yourself.\n\nAll code used to arrive at answers is submitted along with answers.\nNotes:\n\nPlease provide your code answers in the code block under each question and verbal answers in text boxes assigned in the notebook (where applicable).\nPlease run the notebook before the submission so that the outputs are displayed.\nPlease make sure that your results are reproducible. You may use random seeds from random and numpy packages. For scikit-learn modules, you may use the random_state argument.\n\n\n\n\n\nProject (30%)\nThe aim of this project is to allow you to learn about machine learning by trying to solve a task with it.\nFirst, select a question that can be answered using machine learning. I expect that your question will be about a model/algorithm or about an application. Then design a study that will try to answer your question. Your study must have an element of novelty. For example the novelty could be an extension or a variation of an existing algorithm or results of an existing method on a new dataset.\nYour study should involve reading and understanding some background material. Your study must involve running some experiments. You are free to use (or not) any of the tools or models we have seen in class.\nStudy plan: (1 upload per group) Please submit a one-page summary of your proposed research question and study to ZoneCours. I will meet with each group to discuss study plans during the lecture on Week 9. I will send you a schedule the day before. We will probably only have about 15 minutes so please make sure that your study plan is clear and precise. You may also include questions that you would like us to discuss at the end of the document.\nThe group report: (1 upload per group) Your report must contain a description of the question you are trying to answer, a clear description of the model/algorithm you are studying, a survey of related work which proper references, an empirical section that reports your results, and a conclusion that summarizes your findings and (if pertinent) highlights possible future directions of investigation. Your report should be no longer than 10 pages in length (plus references) for pairs or 13 pages (plus references) for teams of three.\nThe individual report: (1 upload per student) You will also submit a brief individual report (at most one page), which will: (1) Describe the parts of the project you worked on (which machine learning methods you applied, which preprocessing steps you performed on the data, which parts of the term paper you wrote, who you worked with on what parts, etc.) and what parts of the project your teammates worked on. (2) What you learned from the project. The purpose of the individual report is to facilitate fair grading and to allow the instructor to understand well what you learned from the project.\nProject Report (30%)\n\nClarity/Relevance of problem statement and description of approach: 10%\nDiscussion of relationship to previous work and references: 4%\nDesign and execution of experiments: 10%\nFigures/Tables/Writing: easily readable, properly labeled,\ninformative: 5%\nIndiviual report: 1%\n\n\nProject Topic\nConcretely, your project could take one of the following forms:\n\nA competition on kaggle.com that you try multiple ML methods on and can achieve a validation set performance in the top 100 on the leaderboard.\nA benchmark task on paperswithcode (e.g. Speech Recognition) where you try multiple methods and compare their performance.\nA task in your own field whereby you apply multiple ML methods to solve the task.\nAn ML method that you apply to a new task (e.g. Multi-head attention applied to time-series prediction)\n\nNote. The list above is not exhaustive. You are encouraged to be creative about the topic. Pick a topic that you find interesting and is relevant. The appropriateness of the topic will be discussed and resolved in the session on Study plan.\n\n\nGroup Report Structure\nYour group report should read like a published paper. Similarly, your poster presentation should have a similar structure to conference posters. You might want to check out similar papers to your topic on the structure of your report (e.g. A Comparison of Deep Learning Approach for Underwater Object Detection).\nRegardless of your topic, as a general rule of thumb, your Group Report should be structured as follows:\n\nIntroduction: Use this section to introduce the topic, its relevance, and a summary of the whole paper. This section should address the question, “Why the target audience of the paper should care about your work?”\nRelated Work: This section discusses what has been done, in prior research, on your topic. While most of the cited papers should be from academic publications, it is not uncommon to see non-academic references in papers (e.g. stackoverflow). You might want to use academic search engines (e.g. SemancticScholar) to find relavant papers.\nExperiments This section should cover the steps you take to answer the question you posed in the Introduction. This includes data preparation, model and hyper-parameter selection, and evaluation steps.\nResults In this section you present the resutls of your experiments with appropriate tables and figures.\nConclusion In your final section, you would summarize what you did in the paper, acknowledge the limitations of your work (All papers have this; no work is perfect.), suggest future research directions to address your limitations.\n\n\n\nTimeline\n\nTeam Registration, due: October 1. Fill this form.\nStudy plan, due: October 28 (by the end of the day EDT).\nHanding in: Through ZoneCours\nProject meeting, October 31\nProject Presentation, due: December 1. Upload the PDF of your poster/slides to ZoneCours.\nIn-class Presentation, on December 5.\nFinal individual report, due: December 15, 2023 (by the end of the day EDT).\n\nHanding in: Through ZoneCours (per each team member).\n\n\n\n\n\nProject Presentation (10%)\nMake a poster that describes your project [You do not need to print your poster]. You can think of a poster as supporting material for your oral presentation (in that way it is similar to slides). It could also follow a similar structure: begin by motivating your work, then (quickly) highlight related work, talk in depth about your solution, then go into results (pictures and tables are good tools for that), finally conclude and perhaps mention one or two ideas for future work.\n\nProject Presentation (10%)\n\nClarity of presentation: 3%\nSlide or Poster quality: 2%\nCorrectness: 2%\nAnswers to questions: 3%\n\n\n\n\nFinal Exam (30%)\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated\n\nPast exam - Fall 2018\n\n\nCapsule Quizzes (10%)\n\nIn-class quizzes of the capsules\nCapsule quizzes are counted for 10% of your final grade.\nTime to complete each quiz is 10 minutes.\n\nQuiz 0, 20 September 11:30 AM to 22 September 11:30 AM.\nQuiz 1, 27 September 11:30 AM to 29 September 11:30 AM.\nQuiz 2, TBA.\nQuiz 3, TBA.\nQuiz 4, TBA.\nQuiz 5, TBA.\nQuiz 6, TBA."
  },
  {
    "objectID": "docs/MATH60629A_Homework.html",
    "href": "docs/MATH60629A_Homework.html",
    "title": "Instructions:",
    "section": "",
    "text": "Due date: October 24, 2023\n# enter you full name and HEC ID\nfull_name = \"\"\nHEC_ID = \"\""
  },
  {
    "objectID": "docs/MATH60629A_Homework.html#svm-for-classfication",
    "href": "docs/MATH60629A_Homework.html#svm-for-classfication",
    "title": "Instructions:",
    "section": "SVM for classfication",
    "text": "SVM for classfication\n\n(1pt) Train a linear SVM on the training set for each one of these C hyperparameter values: {0:001; 0:01; 0:1; 1; 10}. Find the best hyperparameter on the validation set.\n\n\n[Your code here]\n\n\n(1pt) Using the best hyperparameter C, evaluate the accuracy, precision, recall, and F1-score on the test set.\n\n\n[Your code here]\n\n\n(0.5pt) Plot the confusion matrix on the test set and explain the reason for your false negatives/positives.\n\n\n[Your code here]\n\nObservations\n[_____]"
  },
  {
    "objectID": "MATH60629A_Homework.html",
    "href": "MATH60629A_Homework.html",
    "title": "Instructions:",
    "section": "",
    "text": "Due date: October 24, 2023\n# enter you full name and HEC ID\nfull_name = \"\"\nHEC_ID = \"\""
  },
  {
    "objectID": "MATH60629A_Homework.html#svm-for-classfication",
    "href": "MATH60629A_Homework.html#svm-for-classfication",
    "title": "Instructions:",
    "section": "SVM for classfication",
    "text": "SVM for classfication\n\n(1pt) Train a linear SVM on the training set for each one of these C hyperparameter values: {0:001; 0:01; 0:1; 1; 10}. Find the best hyperparameter on the validation set.\n\n\n[Your code here]\n\n\n(1pt) Using the best hyperparameter C, evaluate the accuracy, precision, recall, and F1-score on the test set.\n\n\n[Your code here]\n\n\n(0.5pt) Plot the confusion matrix on the test set and explain the reason for your false negatives/positives.\n\n\n[Your code here]\n\nObservations\n[_____]"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\nUseful links\n\nImage Kernels\nVisualizing what ConvNets learn\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "sessions.html#session-materials",
    "href": "sessions.html#session-materials",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\nUseful links\n\nImage Kernels\nVisualizing what ConvNets learn\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "courses.html#in-class-tools",
    "href": "courses.html#in-class-tools",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "In-class Tools",
    "text": "In-class Tools\n\nCups pacing webapp"
  },
  {
    "objectID": "code/Session1.html",
    "href": "code/Session1.html",
    "title": "Python Basics",
    "section": "",
    "text": "To start let’s load NumPy, a package for scientific computing with Python. We usually load it as the np shorthand.\n\nimport numpy as np\n\nLet’s initialize two arrays\n\na = np.random.randn(1,5)\nb = np.random.randn(1,5)"
  },
  {
    "objectID": "code/Session1.html#loading-necessary-packages",
    "href": "code/Session1.html#loading-necessary-packages",
    "title": "Python Basics",
    "section": "",
    "text": "To start let’s load NumPy, a package for scientific computing with Python. We usually load it as the np shorthand.\n\nimport numpy as np\n\nLet’s initialize two arrays\n\na = np.random.randn(1,5)\nb = np.random.randn(1,5)"
  },
  {
    "objectID": "code/Session1.html#dot-product",
    "href": "code/Session1.html#dot-product",
    "title": "Python Basics",
    "section": "Dot product",
    "text": "Dot product\nTo perform a dot product of two arrays, the shape of the arrays should match. Let’s get the shape of our arrays.\n\na.shape, b.shape\n\n((1, 5), (1, 5))\n\n\nboth arrays have a shape of \\((1, 5)\\).\nIn general, the shape of the two arrays should be \\((n, k) (k, m)\\), where \\(k\\) is the common dimension of the two arrays.\nFor our example, we can transpose the array b to make the shapes match. The NumPy command for transposition of arrays is .T.\n\na.shape, b.T.shape\n\n((1, 5), (5, 1))\n\n\nNow that the two arrays have the matching shapes, we can calculate their dot product using the @ operator.\n\na @ b.T\n\narray([[0.56077435]])\n\n\nThe result of the dot product is an array of shape \\((1,1)\\), or a scalar. In general, the result of a dot product has a shape of \\((n, m)\\)"
  },
  {
    "objectID": "code/Session1.html#best-fit-line",
    "href": "code/Session1.html#best-fit-line",
    "title": "Python Basics",
    "section": "Best fit line",
    "text": "Best fit line\nLet us now simulate a some data and find the best fit line, a line that minimizes the average distance of all data points to the line.\nHere our X is a vector of shape \\((100, 1)\\) samples from a standard normal distribution. Here we have \\(100\\) points with \\(1\\) feature for each point.\nThe y is 10 times x, with the shape \\((100, 1)\\).\n\\[y = X \\cdot coeff\\]\n\nX = np.random.randn(100, 1)\ncoeff = np.ones((1, 1)) * 10\ny = X @ coeff\ny.shape\n\n(100, 1)"
  },
  {
    "objectID": "code/Session1.html#fitting-the-data-using-sklearn",
    "href": "code/Session1.html#fitting-the-data-using-sklearn",
    "title": "Python Basics",
    "section": "Fitting the data using sklearn",
    "text": "Fitting the data using sklearn\nIn the next session, we will formulate a closed-form solution for finding the parameters of linear regression.\nFor now, let’s use the scikit-learn package to find the best line.\nWe’ll import the LinearRegression class from the linear_model submodule of sklearn. Each submodule of sklearn contains classes for that particular topic.\n\nfrom sklearn.linear_model import LinearRegression\n\nNow, let’s create an instance of the LinearRegression model and fit it to the data.\n\nmodel = LinearRegression()\n\n\nmodel.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nMost sklearn classes involve two steps: 1. Instantiating the class. This is where you provide the necessary hyper-parameters. - model = LinearRegression() 2. Fitting the data. - model.fit(X,y)\nNext, we can retrieve the coefficients of the fitted model by accessing the .coeff_ attribute of the model object.\n\na = model.coef_\na\n\narray([[10.]])\n\n\nIn our case, the coefficient is an array of shape \\((1,1)\\). This is in line with the shape of our X (\\(n,1\\)) and y (\\(n, 1\\)).\nWe can also access the intercept of the model using the .intercept_ attribute. Since we didn’t add an intercept in our data simulation, the value of the intercept should be \\(0\\).\nNote. In numerical computations, very small numbers (e.g. \\(2 \\times 10^{-16}\\)) are considered to be \\(0\\).\n\nb = model.intercept_\nb\n\narray([2.22044605e-16])"
  },
  {
    "objectID": "code/Session1.html#plotting-the-line-on-the-data",
    "href": "code/Session1.html#plotting-the-line-on-the-data",
    "title": "Python Basics",
    "section": "Plotting the line on the data",
    "text": "Plotting the line on the data\nWe can now plot the line and the data to see the fit.\nFirst, we get the predictions from the model using its fitted parameters a and b.\n\ny_pred = X @ a + b\n\nWe’ll use seaborn and Matplotlib to create two separate plots on the same axis, ax.\n\nax = sns.scatterplot(x=X[:,0], y=y[:,0], label=\"Actual y\")\nax = sns.lineplot(x=X[:,0], y=y_pred[:,0], ax=ax, color='red', label=\"Predicted y\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x120000280&gt;"
  },
  {
    "objectID": "code/Model.html",
    "href": "code/Model.html",
    "title": "Model capacity",
    "section": "",
    "text": "Model capacity refers to the ability of a machine learning model to capture and represent complex relationships between the input variables (features) and the target variable (labels). It determines the complexity and flexibility of the model in fitting the training data.\nIn other words, model capacity represents the amount of information or patterns that a model can learn from the data. A model with a high capacity can learn intricate relationships in the training data, which may result in overfitting. On the other hand, a model with low capacity may not be able to capture the underlying patterns in the data, leading to underfitting.\nThe capacity of a model can be controlled by adjusting its architectural complexity. For example, in neural networks, increasing the number of hidden layers and hidden units increases the capacity of the model.\nMathematically, we can define model capacity as the number of parameters that the model has to learn. For example, in linear regression, the model capacity is determined by the number of coefficients (slope and intercept) that the model needs to estimate. In neural networks, the model capacity is determined by the number of weights and biases associated with each neuron.\nNow, let’s understand the concept of model capacity using a simple example with polynomial regression. Polynomial regression is a form of linear regression where the relationship between the input feature (x) and the target variable (y) is modeled as an nth-degree polynomial.\nFirst, let’s generate some synthetic data that follows a quadratic relationship between x and y:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-5, 5, 100)\ny = 2 * x ** 2 + np.random.normal(0, 4, 100)\n\n# Plot the data\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Quadratic Relationship')\nplt.show()\n\n\n\n\nBy visualizing the data, we can observe that the relationship between x and y follows a quadratic curve. Now, let’s try fitting this data using different polynomial regression models with different degrees of complexity.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Fit linear regression model\nlinear_model = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())\nlinear_model.fit(x.reshape(-1, 1), y)\n\n# Fit quadratic regression model\nquadratic_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\nquadratic_model.fit(x.reshape(-1, 1), y)\n\n# Fit cubic regression model\ncubic_model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\ncubic_model.fit(x.reshape(-1, 1), y)\n\n# Predict on new data points\nx_test = np.linspace(-5, 5, 100)\ny_linear = linear_model.predict(x_test.reshape(-1, 1))\ny_quadratic = quadratic_model.predict(x_test.reshape(-1, 1))\ny_cubic = cubic_model.predict(x_test.reshape(-1, 1))\n\n# Plot the regression curves\nplt.scatter(x, y, label='Data')\nplt.plot(x_test, y_linear, label='Linear')\nplt.plot(x_test, y_quadratic, label='Quadratic')\nplt.plot(x_test, y_cubic, label='Cubic')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Polynomial Regression')\nplt.legend()\nplt.show()\n\n\n\n\nIn the above code, we fit three polynomial regression models: linear, quadratic, and cubic regression. The degree of the polynomial is indicated by the degree parameter in the PolynomialFeatures class. We can observe that as we increase the complexity (degree) of the polynomial, the models can better capture the underlying quadratic relationship.\nHowever, it’s important to note that high-capacity models can also be prone to overfitting, especially when the amount of training data is limited. Therefore, it’s crucial to balance the model’s capacity with the complexity of the problem at hand and the size of the training dataset.\nTo summarize, model capacity refers to the amount of information or patterns a machine learning model can learn from the data. It is determined by the number of parameters that the model needs to estimate. Increasing the model’s capacity can improve its ability to represent complex relationships in the data, but it can also increase the risk of overfitting."
  },
  {
    "objectID": "code/overfi.html",
    "href": "code/overfi.html",
    "title": "Overfitting",
    "section": "",
    "text": "Overfitting occurs when a machine learning model performs very well on the training data, but fails to generalize well on unseen data. It happens when the model captures noise and random fluctuations in the training data instead of the underlying pattern or relationship.\nOne way to understand overfitting is to consider fitting a polynomial to data points. The degree of the polynomial determines its complexity. A higher degree polynomial can fit the training data more closely, but it may also capture random noise, resulting in poor performance on new data.\nTo demonstrate overfitting using polynomials, we will generate a dataset with some noise and fit polynomials of different degrees to it. We will then visualize the models to see how they fit the data.\nLet’s start by importing the necessary libraries and generating the dataset. We will use the numpy library for array operations and random number generation, and the matplotlib library for data visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the dataset\nnp.random.seed(0)\nX = np.linspace(-1, 1, 20)\ny = 2 * X + np.random.normal(0, 0.5, 20)\n\nIn the above code, we first import the required libraries: numpy and matplotlib.pyplot. We then set a random seed to ensure reproducibility.\nNext, we create an array X with 20 equally spaced points between -1 and 1 using the linspace function. We add some noise to the array y using the numpy.random.normal function. Here, we use a linear relationship with some Gaussian noise to generate our dataset.\nNow, we will plot the generated dataset to visualize it.\n\nplt.scatter(X, y)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Dataset')\nplt.show()\n\n\n\n\nThe code above uses the scatter function from matplotlib.pyplot to create a scatter plot of the dataset. It also adds labels to the x and y axes and sets a title for the plot. Finally, the show function is called to display the plot.\nNow, let’s fit polynomials of different degrees to the dataset and see how they fit the data.\n\n# Polynomial fitting and visualization\ndegrees = [1, 3, 9, 12]\n\nplt.scatter(X, y)\n\nfor degree in degrees:\n    # Fit polynomial of given degree\n    coeffs = np.polyfit(X, y, degree)\n    poly = np.poly1d(coeffs)\n    \n    # Generate x values for plotting\n    x_plot = np.linspace(-1, 1, 100)\n    \n    # Compute predicted y values\n    y_plot = poly(x_plot)\n    \n    # Plot the polynomial\n    plt.plot(x_plot, y_plot, label=f'Degree {degree}')\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Fitting Polynomials')\nplt.legend()\nplt.show()\n\n\n\n\nIn the above code, we define a list degrees with the degrees of the polynomials that we want to fit to the dataset. We then iterate over each degree and perform the following steps:\n\nFit a polynomial of the given degree to the dataset using the polyfit function from numpy.\nCreate a polynomial object using the computed coefficients using the poly1d function from numpy.\nGenerate a set of x values for plotting using the linspace function from numpy.\nCompute the predicted y values for the generated x values using the polynomial.\nPlot the polynomial curve using the plot function from matplotlib.pyplot with a label indicating the degree of the polynomial.\n\nFinally, we add labels and a title to the plot, and display a legend to distinguish the different polynomial curves.\nWhen you run the code, you will see a plot showing the dataset points as scatter points, and different polynomial curves fitted to the data. From this visualization, you can observe the effect of overfitting as the degree of the polynomial increases. Higher degree polynomials tend to fit the training data more closely, but they also capture random noise and fluctuations, resulting in poor generalization to new data."
  },
  {
    "objectID": "code/multip1.html",
    "href": "code/multip1.html",
    "title": "Multiple Linear Regression - Part 1",
    "section": "",
    "text": "In machine learning and statistics, multiple linear regression is a commonly used technique to model the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the independent variables and the dependent variable.\nThe general form of multiple linear regression can be written as:\n\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\\]\nWhere:\nTo perform multiple linear regression, we can use the Ordinary Least Squares (OLS) method, which aims to find the coefficients that minimize the sum of the squared differences between the actual and predicted values."
  },
  {
    "objectID": "code/multip1.html#design-matrix",
    "href": "code/multip1.html#design-matrix",
    "title": "Multiple Linear Regression - Part 1",
    "section": "Design Matrix",
    "text": "Design Matrix\nIn multiple linear regression, we represent the independent variables as a matrix called the design matrix. The design matrix, denoted by \\(X\\), has one row for each observation and one column for each independent variable.\nFor example, if we have n observations and p independent variables, the design matrix \\(X\\) will be an n x p matrix."
  },
  {
    "objectID": "code/multip1.html#proof-of-ols",
    "href": "code/multip1.html#proof-of-ols",
    "title": "Multiple Linear Regression - Part 1",
    "section": "Proof of OLS",
    "text": "Proof of OLS\nThe OLS method minimizes the sum of squared errors (SSE) between the actual dependent variable values and the predicted values. The SSE can be written as:\n\\[SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nWhere:\n\n\\(y_i\\) is the actual dependent variable value for the i-th observation,\n\\(\\hat{y}_i\\) is the predicted dependent variable value for the i-th observation.\n\nTo find the coefficients that minimize SSE, we differentiate SSE with respect to each coefficient and set the derivatives to zero.\nLet’s derive the formula for OLS step by step.\n\nThe predicted value of the dependent variable can be written as:\n\n\\[\\hat{y} = X\\beta\\]\nWhere:\n\n\\(\\hat{y}\\) is an n x 1 vector of predicted values,\n\\(X\\) is the design matrix,\n\\(\\beta\\) is a vector of coefficients.\n\n\nThe SSE can be expressed in matrix form as:\n\n\\[SSE = (\\mathbf{y} - \\mathbf{\\hat{y}})^T (\\mathbf{y} - \\mathbf{\\hat{y}})\\]\nWhere:\n\n\\(\\mathbf{y}\\) is an n x 1 vector of actual dependent variable values,\n\\(\\mathbf{\\hat{y}}\\) is an n x 1 vector of predicted dependent variable values.\n\n\nExpanding the above equation, we get:\n\n\\[SSE = (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)\\]\n\nExpanding the squared term, we get:\n\n\\[SSE = \\mathbf{y}^T\\mathbf{y} - 2\\beta^TX^T\\mathbf{y} + \\beta^TX^TX\\beta\\]\n\nTo minimize SSE with respect to \\(\\beta\\), we differentiate SSE with respect to \\(\\beta\\) and set the derivative to zero:\n\n\\[\\frac{\\partial SSE}{\\partial \\beta} = -2X^T\\mathbf{y} + 2X^TX\\beta = 0\\]\n\nSolving for \\(\\beta\\), we get:\n\n\\[X^TX\\beta = X^T\\mathbf{y}\\]\n\\[\\beta = (X^TX)^{-1}X^T\\mathbf{y}\\]\nThe above formula gives us the optimal values for \\(\\beta\\) that minimize SSE."
  },
  {
    "objectID": "code/multip2.html",
    "href": "code/multip2.html",
    "title": "Multiple Linear Regression - Part 2",
    "section": "",
    "text": "Multiple linear regression is a powerful technique used for predicting a continuous outcome variable based on multiple predictor variables. In this tutorial, we will learn how to perform multiple linear regression using a design matrix in Python.\nFirst, let’s define the problem. In multiple linear regression, we have a dependent variable (also called the response or target variable) and several independent variables (also called features, input variables, or predictors). The goal is to find the best linear relationship between the predictors and the target variable.\nThe general equation for a multiple linear regression model with ‘p’ predictors is given by:\nY = β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ + ε\nWhere: - Y is the target variable - X₁, X₂, …, Xₚ are the predictor variables - β₀, β₁, β₂, …, βₚ are the coefficients (or weights) of the predictors - ε is the error term, representing the randomness or noise in the relationship\nTo estimate the coefficients (β₀, β₁, β₂, …, βₚ), we need to minimize the sum of squared residuals, which measures the differences between the actual values of the target variable (Y) and the predicted values from the regression model.\nIn multiple linear regression, the predictors are often organized into a design matrix (X), where each row represents an observation and each column represents a predictor variable.\nNow let’s see how to perform multiple linear regression using a design matrix in Python.\n\nimport numpy as np\n\n# Define the design matrix X\nX = np.array([[3,  3, -3],\n              [-4, 5,  6],\n              [7, -8,  9]])\n\n# Define the target variable Y\nY = np.array([10, 20, 30])\n\n# Calculate beta coefficients using the normal equation\nbeta = np.linalg.inv(X.T @ X) @ X.T @ Y\n\nprint('Beta coefficients:', beta)\n\nBeta coefficients: [3.56321839 2.98850575 3.2183908 ]\n\n\nIn the above code, we first import the necessary libraries. Then, we define the design matrix X as a 2-dimensional numpy array that contains the predictor variables. Each row represents an observation, and each column represents a predictor variable. We also define the target variable Y as a 1-dimensional numpy array.\nNext, we use the normal equation to calculate the beta coefficients. The normal equation is given by:\n\\[β = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y\\]\n\n\\(X^T\\) is the transpose of X\n\\((X^T \\cdot X)^{-1}\\) is the inverse of the matrix product of \\(X^T\\) and \\(X\\)\n\\(X^T \\cdot Y\\) is the matrix product of \\(X^T\\) and \\(Y\\)\n\nFinally, we print the beta coefficients, which represent the weights or coefficients of the predictors in the multiple linear regression model."
  },
  {
    "objectID": "code/L2 nor.html",
    "href": "code/L2 nor.html",
    "title": "L2 Normalization for Multiple Linear Regression with Design Matrix X",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 normalization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 normalization for multiple linear regression using Python:\n\nStep 1: Importing the Required Libraries\nWe will start by importing the necessary libraries for our implementation.\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\n\nStep 2: Generating Sample Data\nTo demonstrate the L2 normalization for multiple linear regression, we will generate some sample data.\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n\nStep 3: Fitting the Multiple Linear Regression Model\nWe will fit the multiple linear regression model using the OLS method.\n\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n\nStep 4: Fitting the Ridge Regression Model with L2 Normalization\nNow, let’s fit the Ridge regression model with L2 normalization to the data.\n\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 normalization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n\nStep 5: Comparing Results\nFinally, let’s compare the coefficients and MSE of the OLS and Ridge models.\n\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 normalization on the coefficients and model performance.\nBy introducing L2 normalization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\nNote that L2 normalization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 normalization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term."
  },
  {
    "objectID": "code/L2 reg.html",
    "href": "code/L2 reg.html",
    "title": "L2 Regularization for Multiple Linear Regression with Design Matrix X",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 regularization for multiple linear regression using Python:\n\nStep 1: Importing the Required Libraries\nWe will start by importing the necessary libraries for our implementation.\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\n\nStep 2: Generating Sample Data\nTo demonstrate the L2 regularization for multiple linear regression, we will generate some sample data.\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n\nStep 3: Fitting the Multiple Linear Regression Model\nWe will fit the multiple linear regression model using the OLS method.\n\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n\nStep 4: Fitting the Ridge Regression Model with L2 Regularization\nNow, let’s fit the Ridge regression model with L2 regularization to the data.\n\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 regularization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n\nStep 5: Comparing Results\nFinally, let’s compare the coefficients and MSE of the OLS and Ridge models.\n\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 regularization on the coefficients and model performance.\nBy introducing L2 regularization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\nNote that L2 regularization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 regularization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term."
  },
  {
    "objectID": "code/Genera.html",
    "href": "code/Genera.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To understand the generalization of L2 regularization on the test set for multiple linear regression with design matrix X, let’s first discuss what L2 regularization is and how it is applied in multiple linear regression.\nL2 Regularization in Multiple Linear Regression:\nMultiple Linear Regression is a technique used to model the relationship between a dependent variable and multiple independent variables. The goal is to find the best fitting line that minimizes the sum of squared errors.\nHowever, in some cases, the model can become overfit, meaning it fits the training data too closely and does not generalize well to unseen data. This can lead to poor performance on the test set.\nL2 regularization, also known as Ridge regression, is a technique used to prevent overfitting. It adds a penalty term to the least squares objective function, which reduces the magnitude of the coefficients and helps in controlling the complexity of the model.\nThe L2 regularization term is given by:\n\\(\\text{L2 regularization term} = \\lambda \\sum_{i=1}^{m} \\beta_i^2\\)\nWhere:\n\n() is the regularization parameter, which controls the amount of regularization applied. A higher value of () results in more regularization.\n(_i) is the coefficient associated with the (i)th independent variable.\n\nIncluding the L2 regularization term in the objective function, the cost function for multiple linear regression with L2 regularization becomes:\n\\(\\text{{Cost function}} = \\frac{1}{2m} \\left( \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right)\\)\nWhere:\n\n(m) is the number of training examples.\n(h_(x^{(i)})) is the predicted value for the (i)th example using the model’s parameters ().\n(y^{(i)}) is the actual value for the (i)th example.\n\nNow, let’s see how we can apply L2 regularization in multiple linear regression using Python.\nApplying L2 Regularization in Multiple Linear Regression:\nFirst, we need to import the required libraries for our example:\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nNext, we need to create the design matrix X and the target variable y. The design matrix X contains the values of the independent variables, and the target variable y contains the corresponding dependent variable values.\n\n# Create design matrix X\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Create target variable y\ny = np.array([10, 20, 30])\n\nWe split the data into training and test sets using the train_test_split function from scikit-learn. The training set will be used to train the model, and the test set will be used to evaluate its performance.\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nNext, we create an instance of the Ridge regression model and train it on the training set.\n\n# Create an instance of the Ridge regression model\nridge = Ridge(alpha=0.5)\n\n# Train the model on the training set\nridge.fit(X_train, y_train)\n\nRidge(alpha=0.5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=0.5)\n\n\nWe can now use the trained model to make predictions on the test set.\n\n# Make predictions on the test set\ny_pred = ridge.predict(X_test)\n\nFinally, we can evaluate the performance of the model using a performance metric such as mean squared error.\n\n# Calculate the mean squared error on the test set\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\n\nMean Squared Error: 0.28698979591836626\n\n\nThe mean squared error will give us an idea of how well the model is performing on the test set. A lower mean squared error indicates better performance.\nThis is how L2 regularization is applied in multiple linear regression to generalize the model on the test set. By adding a penalty term to the objective function, we can control the complexity of the model and prevent overfitting."
  },
  {
    "objectID": "code/Improv.html",
    "href": "code/Improv.html",
    "title": "L2 Regularization",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 regularization for multiple linear regression using Python:\nFirst, let’s import the necessary libraries:\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nNext, we’ll generate some random data for demonstration purposes:\n\n# Generate random data\nnp.random.seed(0)\nX = np.random.rand(100, 5)\nw = np.random.rand(5, 1)\ny = X**2 @ w + np.random.randn(100)*5\n\nWe split the data into training and test sets:\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nTo apply L2 regularization, we need to scale the input features using the StandardScaler:\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nLet’s use OLS without L2 regularization for a regression model:\n\n# Create a regression model without penalty\nno_ridge = Ridge(alpha=0.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nno_ridge.fit(X_train_scaled, y_train)\n\nRidge(alpha=0.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=0.0)\n\n\nNext, we can evaluate the trained model on the test set:\n\n# Evaluate the model on the test set\nscore = no_ridge.score(X_test_scaled, y_test)\nprint(f\"No-penalty Regression Score: {score}\")\n\nNo-penalty Regression Score: 0.9411011679689105\n\n\nThe score represents the coefficient of determination \\((R^2)\\) of the prediction. Higher values of \\(R^2\\) indicate better model performance.\nWe can now create and train the ridge regression model:\n\n# Create a ridge regression model\nridge = Ridge(alpha=10.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nridge.fit(X_train_scaled, y_train)\n\nRidge(alpha=10.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=10.0)\n\n\nFinally, we can evaluate the trained model on the test set:\n\n# Evaluate the model on the test set\nscore = ridge.score(X_test_scaled, y_test)\nprint(f\"Ridge Regression Score: {score}\")\n\nRidge Regression Score: 0.9439855609099314\n\n\nBy applying L2 regularization using ridge regression, we can improve the generalization of the multiple linear regression model by reducing overfitting and improving its performance on unseen data."
  },
  {
    "objectID": "code/bias v.html",
    "href": "code/bias v.html",
    "title": "Bias-Variance Trade-off",
    "section": "",
    "text": "The bias-variance trade-off is a fundamental concept in machine learning that helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. In supervised learning problems, we aim to find a model, usually a mathematical function, that can accurately predict the target variable based on the input features."
  },
  {
    "objectID": "code/bias v.html#bias-and-variance",
    "href": "code/bias v.html#bias-and-variance",
    "title": "Bias-Variance Trade-off",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nBefore diving into the bias-variance trade-off, let’s briefly explain two important concepts: bias and variance.\n\nBias measures how much our model’s predictions deviate from the true values. A model with high bias oversimplifies the underlying relationship between the features and the target variable. This can lead to underfitting, where the model fails to capture the patterns and relationships in the data.\nVariance measures the variability of model predictions for different training sets. A model with high variance is too sensitive to the specific training examples and does not generalize well to new, unseen data. This can lead to overfitting, where the model fits the training data too well but performs poorly on new data.\n\nThe aim is to find a good balance between bias and variance, where the model captures the underlying patterns in the training data without overfitting."
  },
  {
    "objectID": "code/bias v.html#bias-variance-trade-off",
    "href": "code/bias v.html#bias-variance-trade-off",
    "title": "Bias-Variance Trade-off",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\nThe bias-variance trade-off states that as we decrease the bias of a model (increasing complexity), we tend to increase its variance, and vice versa. This trade-off occurs because model complexity allows for a better fit to the training data, but at the risk of poor performance on new data.\nTo illustrate this concept, let’s consider a regression problem where we can adjust the complexity of a model by changing the degree of the polynomial used for fitting the data."
  },
  {
    "objectID": "code/bias v.html#example",
    "href": "code/bias v.html#example",
    "title": "Bias-Variance Trade-off",
    "section": "Example",
    "text": "Example\n\nImporting Required Libraries\nWe start by importing the necessary libraries: NumPy for numerical operations and matplotlib for visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# To plot the graphs inline in Jupyter Notebook\n%matplotlib inline\n\n\n\nGenerating Synthetic Data\nNext, we generate some synthetic data with a nonlinear relationship between the input features and the target variable using the numpy library.\n\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Generate input features\nX = np.linspace(-5, 5, 100).reshape(-1, 1)\n\n# Generate target variable with noise\nY_true = X**3 - X**2 + X + np.random.randn(100, 1)\n\nHere, we generate 100 samples of input features X ranging from -5 to 5. The target variable Y_true is generated using a cubic relationship with some random Gaussian noise.\n\n\nFitting Polynomial Models\nWe will now fit polynomial models with different degrees to the synthetic data and observe the effect of model complexity on bias and variance.\n\n# Create a function to fit polynomial models and visualize the results\ndef fit_polynomial(X, Y_true, degree):\n    # Fit polynomial regression model\n    poly_features = np.polynomial.Polynomial.fit(X.flatten(), Y_true.flatten(), degree)\n    Y_pred = poly_features(X.flatten())\n    \n    # Compute bias and variance\n    bias = np.mean(np.abs(Y_true - Y_pred))\n    variance = np.var(Y_pred)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X, Y_true, label='True Data', color='b')\n    plt.plot(X, Y_pred, label='Predicted', color='r')\n    plt.title(f'Polynomial Regression (Degree = {degree})\\nBias = {bias:.2f}, Variance = {variance:.2f}')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n# Fit polynomial models with degrees 1, 2, 3, 5, 10\nfit_polynomial(X, Y_true, degree=1)\nfit_polynomial(X, Y_true, degree=2)\nfit_polynomial(X, Y_true, degree=3)\nfit_polynomial(X, Y_true, degree=5)\nfit_polynomial(X, Y_true, degree=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this code block, we define a function fit_polynomial that takes the input features X, true target variable Y_true, and the degree of the polynomial model to be fitted as arguments. Inside the function, we use the numpy.polynomial.Polynomial.fit function to fit a polynomial regression model with the desired degree.\nFor each degree of the polynomial model, we compute the bias and variance using the mean absolute error and variance of the predicted values. Then, we plot the true data points, the predicted curve, and display the bias and variance in the title of the plot.\n\n\nAnalysis and Observations\nBy running the code above, we get a series of plots showing the true data points and the predicted curves for polynomial regression models with different degrees. Each plot also displays the corresponding bias and variance values.\n\nFor a linear model (degree=1), the model is too simple to capture the underlying cubic relationship in the data. Hence, it has a high bias and performs poorly in terms of fitting the data.\nAs the degree of the polynomial model increases, the model can fit the data more accurately, resulting in reduced bias. However, as the complexity increases (degree=5 and 10), we observe that the models start to capture the random fluctuations in the data, resulting in higher variance. These models may fit the training data very well but are likely to perform poorly on unseen data.\nThe model with a degree of 3 strikes a good balance between bias and variance, as it captures the underlying cubic relationship while avoiding overfitting."
  },
  {
    "objectID": "code/bias v.html#conclusion",
    "href": "code/bias v.html#conclusion",
    "title": "Bias-Variance Trade-off",
    "section": "Conclusion",
    "text": "Conclusion\nThe bias-variance trade-off is a fundamental concept in machine learning. It helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. By finding the right balance between bias and variance, we can develop models that accurately represent the patterns in the data without overfitting or oversimplifying the relationships."
  },
  {
    "objectID": "code/Linear_classification.html",
    "href": "code/Linear_classification.html",
    "title": "Linear least squares",
    "section": "",
    "text": "Linear least squares is a technique used for regression problems, where we aim to predict continuous numerical values. However, it can also be used for classification tasks by transforming the problem into a binary classification problem.\nIn linear least squares for classification, we use a linear model to classify data into two classes. We assign class labels of -1 and 1 to the two classes. The goal is to find a linear boundary that best separates the two classes, minimizing the sum of squared distances between the data points and the decision boundary.\nLet’s see how we can do this in Python:\nFirst, we need to import the required libraries: numpy and matplotlib.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNext, let’s generate some synthetic data with two classes. We will use the make_classification function from the sklearn.datasets module to create a random dataset.\n\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\nNow, let’s visualize the data using a scatter plot:\n\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n\n\n\n\nWe have plotted the data points for each class on a scatter plot.\nTo apply linear least squares for classification, we need to add a column of ones to our feature matrix X to incorporate the bias term in the linear equation.\n\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n\nNow, let’s define our linear model and solve for the optimal parameters using linear least squares.\n\ntheta = np.linalg.inv(X.T @ X) @ X.T @ y\n\nHere, theta is the vector of parameters that defines our linear model. The equation used to solve for theta is:\n\\(\\theta = (X^T X)^{-1} X^T y\\)\nFinally, let’s visualize the decision boundary of our linear model along with the data points.\n\nplt.scatter(X[y == 1][:,1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0][:,1], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\n\n# Plotting the decision boundary\nx_boundary = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n# x2_boundary = -(theta[0] + theta[1]*x1_boundary) / theta[2]\ny_boundary = x_boundary * theta[1] +  theta[0]\nplt.plot(x_boundary, y_boundary, color='black', linewidth=2)\n\nplt.show()\n\n\n\n\nWe have plotted the decision boundary determined by our linear model, which separates the two classes.\nLinear least squares for classification is a simple technique for linearly separable datasets. Note that this approach assumes the data points are linearly separable and does not work well for nonlinear classification problems."
  },
  {
    "objectID": "code/SVM.html",
    "href": "code/SVM.html",
    "title": "SVM",
    "section": "",
    "text": "Support Vector Machines (SVM) is a powerful machine learning algorithm used for both classification and regression tasks. In this tutorial, we will focus on how to use SVM for classification.\nBefore diving into the code, let’s understand the intuition behind SVM."
  },
  {
    "objectID": "code/SVM.html#intuition-behind-svm",
    "href": "code/SVM.html#intuition-behind-svm",
    "title": "SVM",
    "section": "Intuition behind SVM",
    "text": "Intuition behind SVM\nSVM is based on the concept of finding a hyperplane that best separates the data points belonging to different classes. The hyperplane is determined by support vectors, which are the data points closest to the decision boundary.\nIn a binary classification problem, SVM aims to find a hyperplane that maximizes the margin between the support vectors of the two classes. The margin is the distance between the hyperplane and the nearest data points from each class.\nThe optimal hyperplane can be described by the equation:\n\\(w^T x - b = 0\\)\nwhere \\(w\\) is the normal vector to the hyperplane and \\(b\\) is the bias.\nThe equation of the decision function is:\n\\(f(x) = sign(w^T x - b)\\)\nwhere \\(sign(\\cdot)\\) is the sign function.\nSVM can also handle non-linearly separable data by using a technique called the kernel trick. This technique transforms the original feature space into a higher-dimensional space, making the data linearly separable.\nNow, let’s implement SVM for a classification problem using the famous Iris dataset."
  },
  {
    "objectID": "code/SVM.html#importing-libraries-and-loading-the-dataset",
    "href": "code/SVM.html#importing-libraries-and-loading-the-dataset",
    "title": "SVM",
    "section": "Importing Libraries and Loading the Dataset",
    "text": "Importing Libraries and Loading the Dataset\nThe first step is to import the required libraries and load the dataset. We will use scikit-learn library, which provides a simple API for SVM implementation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nNext, let’s generate some synthetic data with two classes. We will use the make_classification function from the sklearn.datasets module to create a random dataset.\n\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\ny = y*2 - 1\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\nNow, let’s visualize the data using a scatter plot:\n\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == -1], y[y == -1], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n\n\n\n\nWe have plotted the data points for each class on a scatter plot."
  },
  {
    "objectID": "code/SVM.html#training-the-support-vector-machine-classifier",
    "href": "code/SVM.html#training-the-support-vector-machine-classifier",
    "title": "SVM",
    "section": "Training the Support Vector Machine Classifier",
    "text": "Training the Support Vector Machine Classifier\nOnce we have loaded the dataset, we can proceed to train the SVM classifier.\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVM classifier object\nsvm = SVC(kernel='linear', C=1e10)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\nSVC(C=10000000000.0, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=10000000000.0, kernel='linear')\n\n\nIn this code block, we first split the data into training and testing sets using the train_test_split() function. We reserve 20% of the data for testing and set the random_state parameter for reproducibility purposes.\nNext, we create an SVM classifier object using the SVC class from scikit-learn. We specify the kernel parameter as ‘linear’ for a linear SVM.\nFinally, we train the SVM classifier using the fit() method by passing the training data (X_train) and the corresponding class labels (y_train)."
  },
  {
    "objectID": "code/SVM.html#making-predictions-and-evaluating-the-model",
    "href": "code/SVM.html#making-predictions-and-evaluating-the-model",
    "title": "SVM",
    "section": "Making Predictions and Evaluating the Model",
    "text": "Making Predictions and Evaluating the Model\nAfter training the SVM classifier, we can use it to make predictions on new, unseen data.\n\n# Make predictions on the testing set\ny_pred = svm.predict(X_test)\n\n# Evaluate the model\naccuracy = np.sum(y_pred == y_test) / len(y_test)\nprint(\"Accuracy: \", accuracy)\n\nAccuracy:  1.0\n\n\nIn this code block, we use the trained SVM classifier to make predictions on the testing set (X_test). The predicted class labels are stored in the y_pred variable.\nWe then evaluate the model by calculating the accuracy of the predictions. The accuracy is defined as the number of correctly classified data points divided by the total number of data points."
  },
  {
    "objectID": "code/SVM.html#visualizing-the-decision-boundary",
    "href": "code/SVM.html#visualizing-the-decision-boundary",
    "title": "SVM",
    "section": "Visualizing the Decision Boundary",
    "text": "Visualizing the Decision Boundary\nTo visualize the decision boundary and the support vectors, we can use the following code:\n\ndef plot_decision_boundary(classifier, X, y):\n    # Define the range of x values for the mesh grid\n    x_min, x_max = X.min() - 1, X.max() + 1\n\n    # Create a mesh grid\n    xx = np.linspace(x_min, x_max, 1000)[:, np.newaxis]\n\n    # Use the classifier to make predictions on the mesh grid\n    yy = classifier.decision_function(xx)\n\n    # get support vectors\n    i_sv = classifier.support_\n    sv_x = X[i_sv]\n    sv_y = y[i_sv]\n    print(f'Support vectors are: \\nX={sv_x}\\ny={sv_y}')\n\n    # get w and b\n    w = svm.coef_\n    b = svm.intercept_\n    print(f'W={w}\\nb={b}')\n    # print(f\"wX-b:\\n{w*X-b}\")\n\n    # where the decision function is zero\n    ind_0 = np.where((yy&lt;=5e-2) & (yy&gt;=-5e-2))[0]\n    # print(ind_0)\n    distance1 = xx[ind_0].mean() - sv_x[0]\n    distance2 = xx[ind_0].mean() - sv_x[1]\n    print(f'\\ndistance to \\nsupport 1: {distance1} \\nsupport 2:{distance2}')\n\n    # Plot the decision boundary and support vectors\n    plt.hlines(0,-2, 3)\n    plt.scatter(X, y, c=y, cmap=plt.cm.Paired, edgecolors='k')\n    plt.scatter(xx, yy, color='black', linewidth=3)\n    plt.scatter(sv_x,sv_y, color='red')\n    plt.scatter(X,w*X-b,color='yellow', alpha=0.5)\n    plt.scatter(xx[ind_0], np.zeros(len(ind_0),), marker='x', color='red')\n    plt.xlim(x_min, x_max)\n    # plt.ylim(-0.2, 1.2)\n    plt.xlabel('Feature')\n    plt.ylabel('label')\n    plt.show()\n\n# Visualize the decision boundary\nplot_decision_boundary(svm, X_train, y_train)\n\nSupport vectors are: \nX=[[-0.61120543]\n [ 0.65311592]]\ny=[-1  1]\nW=[[1.58187634]]\nb=[-0.0331486]\n\ndistance to \nsupport 1: [0.63295407] \nsupport 2:[-0.63136728]\n\n\n\n\n\nIn this code block, we define a helper function plot_decision_boundary() that takes a trained classifier object (svm), the training data (X_train), and the corresponding class labels (y_train) as input.\nThe function calculates the minimum and maximum values of the two features to define the plotting range. It then generates a mesh grid with a step size h and predicts the class labels for each point in the grid using predict() method.\nFinally, it plots the decision boundary by contouring the predicted class labels and scatter plots the training data points.\nRunning this code will display the decision boundary and the support vectors.\nThis is how we can implement SVM for classification in Python. SVM is a versatile algorithm and can be further fine-tuned by selecting different kernels and hyperparameters for better performance."
  },
  {
    "objectID": "w3/week3_exercise.html",
    "href": "w3/week3_exercise.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "!pip install imbalanced-learn\n\nCollecting imbalanced-learn\n  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: numpy&gt;=1.17.3 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.11.2)\nRequirement already satisfied: scikit-learn&gt;=1.0.2 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.3.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (3.2.0)\nDownloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 235.6/235.6 kB 4.5 MB/s eta 0:00:00:00:01\nInstalling collected packages: imbalanced-learn\nSuccessfully installed imbalanced-learn-0.11.0\n\n\n\nfrom imblearn.datasets import fetch_datasets\n\n\n# Load the dataset\ndata = fetch_datasets()['mammography']\nX, y = data.data, data.target\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame(np.hstack([X,y.reshape(-1,1)]))\ndf.columns = [f\"feature_{i}\" for i in range(1,7)] + ['y']\ndf\n\n\n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\ny\n\n\n\n\n0\n0.230020\n5.072578\n-0.276061\n0.832444\n-0.377866\n0.480322\n-1.0\n\n\n1\n0.155491\n-0.169390\n0.670652\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n2\n-0.784415\n-0.443654\n5.674705\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n3\n0.546088\n0.131415\n-0.456387\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n4\n-0.102987\n-0.394994\n-0.140816\n0.979703\n-0.377866\n1.013566\n-1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11178\n-0.250012\n-0.377300\n-0.321142\n1.269157\n3.652984\n1.092791\n1.0\n\n\n11179\n0.281343\n-0.417112\n-0.366224\n0.851010\n2.789649\n1.345700\n1.0\n\n\n11180\n1.204988\n1.763724\n-0.501468\n1.562408\n6.489072\n0.931294\n1.0\n\n\n11181\n0.736644\n-0.222474\n-0.050653\n1.509665\n0.539269\n1.315229\n1.0\n\n\n11182\n0.177003\n-0.191508\n-0.501468\n1.578864\n7.750705\n1.555951\n1.0\n\n\n\n\n11183 rows × 7 columns\n\n\n\n\nX.shape, y.shape\n\n((11183, 6), (11183,))"
  },
  {
    "objectID": "code/naive.html",
    "href": "code/naive.html",
    "title": "Naive Bayes Classifier",
    "section": "",
    "text": "The Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification problems and is especially effective when dealing with high-dimensional data. Naive Bayes assumes that the presence of a particular feature in a class is independent of the presence of other features."
  },
  {
    "objectID": "code/naive.html#bayes-theorem",
    "href": "code/naive.html#bayes-theorem",
    "title": "Naive Bayes Classifier",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nBefore diving into Naive Bayes, let’s start with Bayes’ theorem. Bayes’ theorem allows us to calculate the probability of a hypothesis given some observed evidence. It is stated as:\n\\(P(h|e) = \\frac{{P(e|h) \\cdot P(h)}}{{P(e)}}\\)\nWhere:\n\n\\(P(h|e)\\) is the posterior probability of hypothesis \\(h\\) given evidence \\(e\\).\n\\(P(e|h)\\) is the probability of evidence \\(e\\) given hypothesis \\(h\\).\n\\(P(h)\\) is the prior probability of hypothesis \\(h\\).\n\\(P(e)\\) is the probability of evidence \\(e\\).\n\nIn the context of Naive Bayes, we can reframe this theorem as:\n\\(P(y|X) = \\frac{{P(X|y) \\cdot P(y)}}{{P(X)}}\\)\nWhere:\n\n\\(X\\) represents the input features.\n\\(y\\) represents the class or target variable.\n\\(P(y|X)\\) is the posterior probability of class \\(y\\) given features \\(X\\).\n\\(P(X|y)\\) is the probability of observing features \\(X\\) given class \\(y\\).\n\\(P(y)\\) is the prior probability of class \\(y\\).\n\\(P(X)\\) is the probability of observing features \\(X\\)."
  },
  {
    "objectID": "code/naive.html#naive-bayes-classifier",
    "href": "code/naive.html#naive-bayes-classifier",
    "title": "Naive Bayes Classifier",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\nThe Naive Bayes classifier assumes that the presence of each feature is independent of the presence of other features, given the class variable. This is where the “Naive” part comes in. Despite this simplifying assumption, Naive Bayes can still be very effective in practice, especially with text classification tasks.\nThere are three common types of Naive Bayes classifiers: 1. Gaussian Naive Bayes: It assumes that the features are normally distributed. 2. Multinomial Naive Bayes: It is suitable for discrete features (e.g., word counts). 3. Bernoulli Naive Bayes: It is suitable for binary features (e.g., true/false).\nIn this tutorial, we will focus on Gaussian Naive Bayes, which is commonly used for continuous features."
  },
  {
    "objectID": "code/naive.html#gaussian-naive-bayes",
    "href": "code/naive.html#gaussian-naive-bayes",
    "title": "Naive Bayes Classifier",
    "section": "Gaussian Naive Bayes",
    "text": "Gaussian Naive Bayes\nGaussian Naive Bayes assumes that the continuous features in each class are normally distributed. It calculates the mean and standard deviation for each feature in each class and uses a Gaussian probability density function to estimate the likelihood of observing a particular feature value given a class. The formula for the Gaussian probability density function is:\n\\(P(x|y) = \\frac{1}{{\\sqrt{{2\\pi\\sigma_y^2}}}} \\cdot e^{-\\frac{{(x - \\mu_y)^2}}{{2\\sigma_y^2}}}\\)\nWhere:\n\n\\(x\\) is the feature value.\n\\(y\\) is the class label.\n\\(\\mu_y\\) is the mean of the feature values in class \\(y\\).\n\\(\\sigma_y\\) is the standard deviation of the feature values in class \\(y\\).\n\\(\\pi\\) is the mathematical constant pi."
  },
  {
    "objectID": "code/naive.html#implementation",
    "href": "code/naive.html#implementation",
    "title": "Naive Bayes Classifier",
    "section": "Implementation",
    "text": "Implementation\nNow let’s see the implementation of Gaussian Naive Bayes in Python using the sklearn library.\nStep 1: Import the required libraries.\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nStep 2: Load the dataset.\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nStep 3: Split the dataset into training and testing sets.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 4: Create a Gaussian Naive Bayes classifier and fit it to the training data.\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nStep 5: Make predictions on the test data.\n\ny_pred = clf.predict(X_test)\n\nStep 6: Evaluate the accuracy of the classifier.\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 1.0\n\n\nThat’s it! You have successfully implemented Gaussian Naive Bayes for classification in Python using sklearn.\nNote: This tutorial only covers the basic usage of Naive Bayes. There are many other aspects and variations of Naive Bayes that you can explore, such as Laplace smoothing, handling missing values, and dealing with categorical features."
  },
  {
    "objectID": "code/multin.html",
    "href": "code/multin.html",
    "title": "Multinomial Naive Bayes",
    "section": "",
    "text": "In machine learning, Naive Bayes is a probabilistic algorithm that is based on Bayes’ theorem. It is commonly used for classification problems and is known for its simplicity and efficiency. In particular, Multinomial Naive Bayes is a variation of the Naive Bayes algorithm that is specifically designed for discrete features, such as word counts in text documents."
  },
  {
    "objectID": "code/multin.html#bayes-theorem",
    "href": "code/multin.html#bayes-theorem",
    "title": "Multinomial Naive Bayes",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nTo understand how Multinomial Naive Bayes works, let’s first review Bayes’ theorem. Bayes’ theorem provides a way to calculate conditional probabilities. It can be formulated as follows:\n\\(P(A|B) = \\frac{{P(B|A) P(A)}}{{P(B)}}\\)\nWhere:\n\n\\(P(A|B)\\) is the probability of event A occurring given that event B has occurred.\n\\(P(B|A)\\) is the probability of event B occurring given that event A has occurred.\n\\(P(A)\\) is the probability of event A occurring.\n\\(P(B)\\) is the probability of event B occurring."
  },
  {
    "objectID": "code/multin.html#multinomial-naive-bayes",
    "href": "code/multin.html#multinomial-naive-bayes",
    "title": "Multinomial Naive Bayes",
    "section": "Multinomial Naive Bayes",
    "text": "Multinomial Naive Bayes\nMultinomial Naive Bayes is specifically designed for problems with discrete features. It assumes that the features are generated from a multinomial distribution and that the features are conditionally independent given the class label. This assumption simplifies the conditional probability calculation and makes the algorithm computationally efficient.\nThe formula for the multinomial distribution:\n\\(p(x_i | y) = \\frac{n_{yi} + \\alpha}{n_y + \\alpha n}\\)\nWhere\n\n\\(n_{yi}\\) is the number of times feature i appears in class y\n\\(n_y\\) is the number of time class y appears\n\\(\\alpha&gt;0\\) is the smoothing prior, which accounts for features not present in the learning samples and prevents zero probabilities in further computations.\n\nSetting \\(\\alpha=1\\) is called Laplace smoothing, while \\(\\alpha&lt;1\\) is called Lidstone smoothing.\n\n\nFor a comrehensive analysis of Naive Bayes algorithms, visit this link.\nHere’s a step-by-step overview of how Multinomial Naive Bayes works:\n\nPreparing the Dataset: First, we need a dataset consisting of samples with features and corresponding class labels. The features should be discrete, such as word counts in text documents.\nFeature Extraction: Next, we need to extract features from the dataset. This can involve techniques like tokenization, stemming, and vectorization.\nTraining: We then split the dataset into a training set and a test set. The training set is used to calculate the probabilities required for classification.\nCalculating Class Prior Probabilities: We calculate the prior probability of each class by counting the frequency of each class label in the training set.\nCalculating Conditional Probabilities: We calculate the conditional probability of each feature given the class label by counting the frequency of each feature in each class.\nClassifying New Instances: Finally, we use the calculated probabilities to classify new instances. For each new instance, we calculate the posterior probability of each class given the features and select the class with the highest probability as the predicted class.\n\nLet’s now implement Multinomial Naive Bayes in Python using the scikit-learn library."
  },
  {
    "objectID": "code/multin.html#implementation",
    "href": "code/multin.html#implementation",
    "title": "Multinomial Naive Bayes",
    "section": "Implementation",
    "text": "Implementation\nFirst, we need to import the necessary libraries:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nNext, let’s create an example dataset consisting of text documents and corresponding class labels:\n\ndocuments = ['The sun is shining',\n             'The weather is beautiful',\n             'I enjoy going for walks',\n             'I hate rainy days']\n\nlabels = ['positive', 'positive', 'negative', 'negative']\n\nNext, we need to split the dataset into a training set and a test set:\n\ndoc_train, doc_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\nprint(doc_train)\n\n['I hate rainy days', 'The sun is shining', 'I enjoy going for walks']\n\n\nNow, let’s create a CountVectorizer object to extract features from the text documents:\n\nvectorizer = CountVectorizer()\n\nWe can then use the fit_transform() method of the vectorizer to transform the documents into a feature matrix:\n\nx_train = vectorizer.fit_transform(doc_train)\nprint(x_train.todense())\nprint(vectorizer.vocabulary_)\n\ni2w = {i:w for w,i in vectorizer.vocabulary_.items()}\ndoc = x_train.toarray().copy().astype(str)\nfor i in range(doc.shape[0]):\n  for j in range(doc.shape[1]):\n    if doc[i,j]=='1':\n      doc[i,j] = str(i2w[j])\nprint(doc)\n\n[[1 0 0 0 1 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 1 1 1 0]\n [0 1 1 1 0 0 0 0 0 0 1]]\n{'hate': 4, 'rainy': 6, 'days': 0, 'the': 9, 'sun': 8, 'is': 5, 'shining': 7, 'enjoy': 1, 'going': 3, 'for': 2, 'walks': 10}\n[['days' '0' '0' '0' 'hate' '0' 'rainy' '0' '0' '0' '0']\n ['0' '0' '0' '0' '0' 'is' '0' 'shining' 'sun' 'the' '0']\n ['0' 'enjoy' 'for' 'going' '0' '0' '0' '0' '0' '0' 'walks']]\n\n\nNow, let’s create a MultinomialNB object and train it on the training set:\n\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nWe can then use the trained model to classify the instances in the test set:\n\nx_test = vectorizer.transform(doc_test)\ny_pred = model.predict(x_test)\nprint(doc_test)\nprint(x_test.todense())\ndoc = x_test.toarray().copy().astype(str)\nfor i in range(doc.shape[0]):\n  for j in range(doc.shape[1]):\n    if doc[i,j]=='1':\n      doc[i,j] = str(i2w[j])\nprint(doc)\nprint(y_pred)\n\n['The weather is beautiful']\n[[0 0 0 0 0 1 0 0 0 1 0]]\n[['0' '0' '0' '0' '0' 'is' '0' '0' '0' 'the' '0']]\n['positive']\n\n\nFinally, let’s calculate the accuracy of the model:\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n\nAccuracy: 1.0\n\n\nWe have successfully trained a Multinomial Naive Bayes classifier and used it to classify new instances."
  },
  {
    "objectID": "code/precis.html",
    "href": "code/precis.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "The precision-recall curve is a graphical representation of the trade-off between precision and recall for different threshold values. It is commonly used in binary classification problems where the goal is to classify data into one of two classes.\nLet’s start by understanding precision and recall:\nPrecision is defined as the number of true positives (TP) divided by the sum of true positives and false positives (FP): [ Precision = ]\nRecall is defined as the number of true positives (TP) divided by the sum of true positives and false negatives (FN): [ Recall = ]\nIn a classification problem, a high precision means that the classifier is making fewer false positive predictions, while a high recall means that it is making fewer false negative predictions.\nTo create a precision-recall curve, we need a classifier that can provide prediction probabilities or scores for each instance. Then, by varying the threshold on these scores, we can generate different points on the precision-recall curve.\nWe will demonstrate this process using the scikit-learn library and the Breast Cancer Wisconsin (Diagnostic) dataset. Let’s get started:\nStep 1: Import the necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\n\nStep 2: Load and prepare the dataset\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 3: Train a classifier and obtain prediction probabilities\n\n# Train a logistic regression classifier\nclassifier = LogisticRegression(max_iter=10_000)\nclassifier.fit(X_train, y_train)\n\n# Obtain prediction probabilities for the test set\ny_prob = classifier.predict_proba(X_test)[:, 1]\n\nStep 4: Calculate precision and recall for different threshold values\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n\nStep 5: Plot the precision-recall curve\n\nplt.plot(thresholds, precision[:-1], label='Precision')\nplt.plot(thresholds, recall[:-1], label='Recall')\nplt.xlabel('Threshold')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.show()\n\n\n\n\nIn this code, we first import the necessary libraries such as numpy, matplotlib, scikit-learn modules, and data from scikit-learn’s built-in Breast Cancer Wisconsin dataset.\nNext, we load and prepare the dataset. We split it into training and testing sets using the train_test_split function.\nThen, we train a logistic regression classifier on the training set and obtain prediction probabilities for the test set using the predict_proba method.\nFinally, we calculate precision and recall values for different threshold values using the precision_recall_curve function. We plot these values to visualize the precision-recall curve using the plt.plot function.\nThe resulting precision-recall curve shows how the precision and recall values change for different threshold values. A higher precision and recall value indicates a better classifier performance.\nThis curve can be useful in identifying an appropriate threshold value that balances precision and recall according to the specific problem requirements."
  },
  {
    "objectID": "code/confus.html",
    "href": "code/confus.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "A confusion matrix is a useful tool for evaluating the performance of a classification model. It provides a tabular representation of the predicted and actual classes of a binary classification problem. The matrix helps us understand how well the model is performing by showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\nLet’s define the confusion matrix for a binary classification problem using the following notations:\n\nTP: Number of true positive predictions\nTN: Number of true negative predictions\nFP: Number of false positive predictions\nFN: Number of false negative predictions\n\nTo illustrate this, let’s consider a dataset of 100 samples. Our binary classifier predicts whether a sample is positive or negative. After running the prediction, we obtain the following results:\n\nThere are 60 true positive predictions (TP = 60).\nThere are 30 true negative predictions (TN = 30).\nThere are 5 false positive predictions (FP = 5).\nThere are 5 false negative predictions (FN = 5).\n\nNow, let’s plot these values in a confusion matrix:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP = 60\nFN = 5\n\n\nActual Negative\nFP = 5\nTN = 30\n\n\n\nIn Python, we can use the scikit-learn library to calculate the confusion matrix. Here’s an example of how to compute the confusion matrix for binary classification:\nStep 1: Import the necessary libraries\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nStep 2: Define the actual and predicted classes\n\nactual = np.array([1, 1, 0, 0, 1, 0, 0, 1, 0, 1])\npredicted = np.array([1, 0, 0, 1, 1, 0, 1, 1, 0, 0])\n\nStep 3: Calculate the confusion matrix using the confusion_matrix function\n\ncm = confusion_matrix(actual, predicted)\nprint(cm)\n\n[[3 2]\n [2 3]]\n\n\nOutput:\n[[3 2]\n [2 3]]\nIn this example, we have 3 true positive predictions, 3 true negative predictions, 2 false positive predictions, and 2 false negative predictions. Hence, the confusion matrix is:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP = 3\nFN = 2\n\n\nActual Negative\nFP = 2\nTN = 3\n\n\n\nThe confusion matrix provides essential information for evaluating the performance of a binary classification model, such as accuracy, precision, recall, and F1 score. It helps us understand the model’s strengths and weaknesses, identify any imbalances in the predictions, and make informed decisions about improving the model."
  },
  {
    "objectID": "code/prf1.html",
    "href": "code/prf1.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "In the field of machine learning and classification, evaluating the model’s performance is crucial. One common way to evaluate classification models is by using a confusion matrix. The confusion matrix provides a detailed breakdown of the model’s predictions and their corresponding actual labels. From the confusion matrix, we can calculate several performance metrics, including precision, recall, and F1-score.\nLet’s start by understanding what a confusion matrix is.\nA confusion matrix is a table that visualizes the performance of a classification model. It consists of four different values:\n\nTrue Positive (TP): The number of positive instances that the model correctly predicted as positive.\nFalse Positive (FP): The number of negative instances that the model incorrectly predicted as positive.\nTrue Negative (TN): The number of negative instances that the model correctly predicted as negative.\nFalse Negative (FN): The number of positive instances that the model incorrectly predicted as negative.\n\nThe confusion matrix is typically presented in the following format:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\nNow, let’s calculate precision, recall, and F1-score using the confusion matrix.\nPrecision measures the accuracy of positive predictions. It is calculated using the formula:\n\\(\\text{{Precision}} = \\frac{TP}{{TP + FP}}\\)\nRecall, also known as the sensitivity or true positive rate, measures the proportion of actual positive instances that were correctly classified. It is calculated using the formula:\n\\(\\text{{Recall}} = \\frac{TP}{{TP + FN}}\\)\nF1-score is the harmonic mean of precision and recall. It provides a balanced measure between the two. F1-score is calculated using the formula:\n\\(F1 = \\frac{2 \\times \\text{{Precision}} \\times \\text{{Recall}}}{{\\text{{Precision}} + \\text{{Recall}}}}\\)"
  },
  {
    "objectID": "Introduction_to_ML.html",
    "href": "Introduction_to_ML.html",
    "title": "MATH60629A Fall 2022",
    "section": "",
    "text": "This tutorial provides a short introduction to the practice of machine learning.\nWe assume that the user already has an understanding of the basic concepts that underlie the field. We review both methodological concepts like supervised learning and also use software libraries such as scikit-learn, pandas, and numpy.\nIn particular, we will: 1. load some data, 2. fit different supervised models on variations of the data, 3. and compare results.\nThis tutorial is not meant to be exhaustive (references are provided throughout, and links to extra material are provided at the end).\n\n\n\nLaurent Charlin lcharlin@gmail.com\n\n\n\n\n\nSection 0. Introduction\nSection 1. Data Pre-Processing\nSection 2. Modelling\nSection 3. Concluding Remarks\n\n ### Section 0. Introduction We will use the example of a recommender system, i.e., a system which must recommend movies of interests to its users (e.g., Netflix). We will model user-movie preferences from a popular publicly available dataset (Movielens 1M). We will learn, from past user-movie ratings, to predict (missing/future) user-movie ratings from user socio-demographics and movie-tags data.\nMathematically, we are interested in learning the (parameters of the) following function:\n\\[ r_{um} = f_\\theta(x_u, x_m)\\] where - \\(u\\) indexes users - \\(m\\) indexes items - \\(r_{um}\\) is u’s rating for m (that user’s preference) – the dependent variable - \\(f_\\theta\\) is some model parametrized by \\(\\theta\\). For example, a linear regression with coefficients \\(\\theta\\) - \\(x_u\\) are user u’s covariates (e.g., age and occupation of this user) - \\(x_m\\) are movie m’s covariates (e.g., tags associated with the movie)\nThe function \\(f\\) can take several forms (in other words, we can use a variety of models for this task). In today’s tutorial we will assume that the problem is a regression one and we will experiment with several models ranging from a simple linear regression model to a more complicated two-hidden layer neural network.\n\n\n\nIt can be useful to think of machine learning as comprising three elements: 1. Task (T) 2. Experience (E) 3. Performance measure (P).\n(a good description of these concepts is provided in Ch. 5 of the Deep Learning Book)\nThe intuition is that the task (T) is “the type of problem you are trying the solve” (e.g., classification, regression, anomaly detection), the experience (E) is “how your data comes about” (e.g., does it come with labels or not, do you observe it all at once or as a stream), and the performance (P) is “how well your model does”. Standard performance measures include accuracy and mean-squared error.\nNote that the above terminology does not define the model used to learn (fit) the data nor does it define the fitting procedure (e.g., gradient descent).\nRelationship to the problem of rating prediction: - Task: Our task is to predict user-movie ratings. It can be modelled in different ways (more on this during week 11), but here we will model it as a regression problem. - Experience: The experience is a supervised learning one because we are predicting some dependent variable (rating) from a set of independent variables - Performance measure: We will be using the mean-squared error (MSE).\n\n\n\nFor supervised learning, it is customary, to construct two data matrix \\(X\\) and \\(Y\\). The former, \\(X\\), contains the covariates (features). It is a matrix of size \\(n \\times p\\) with \\(n\\) the number of examples and \\(p\\) the dimensionality of each example (in other words the number of covariates associated with each example). They are the input to the function.\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\n\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\\nx_{n1} & x_{12} & \\ldots & x_{np} \\\\\n\\end{bmatrix}\n\\]\nThe latter, \\(Y\\), is a (column) vector of length \\(n\\) which contains the labels (here ratings). \\(Y_1\\) corresponds to the rating of \\(X_1\\) (row contains the labels (here ratings).\n\\[\nY = \\begin{bmatrix}\nr_1 \\\\\nr_2 \\\\\n\\vdots \\\\\nr_n\n\\end{bmatrix}\\]\nOf course, in a real problem we will differentiate the train and test sets, e.g., with \\(X_\\text{train}\\) and \\(X_\\text{test}\\). Same for the labels using, e.g., \\(Y_\\text{train}\\) and \\(Y_\\text{test}\\).\n\n\n\nFollowing this brief introduction, we now dive into the problem.\n\n# We first download the repo to get access to data and some utility code (This is specifically for colab.)\n!rm -rf 80-629/\n!git clone https://github.com/lcharlin/80-629/\n\nCloning into '80-629'...\nremote: Enumerating objects: 25, done.\nremote: Counting objects: 100% (25/25), done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 77 (delta 16), reused 19 (delta 10), pack-reused 52\nUnpacking objects: 100% (77/77), done.\n\n\nWe begin by importing the packages that we will need: - reduce function will come in handy to iteratively process data - os standard packages for performing system operations (e.g., opening files) - re package for regex - sys package to deal with system-level operations (here used to change the search path) - time package we will use to measure the duration of certain operations\n\nmatplotlib for plotting\nnumpy for linear-algebra computations\npandas for data wrangling\nsklearn (scikit-learn) for machine learning models and useful machine learning related routines\n\n\nfrom functools import reduce\nimport os\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neural_network\n\nimport sys\nsys.path += ['80-629/week4-PracticalSession/']\nfrom local_utils import DrawNN\n\n # Section 1: Data Pre-Processing\nIn the following we load data from several csv files and pre-process it.\nWhile this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\nI suggest that you read this section, but that you spend most of your time on the other sections. If you have time at the end, you can come back and do this more thoroughly.\n\n\nWe will use the publically available movielens dataset. The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the ML-1M data (it contains 1M ratings) but we will also use movie tags from the ML-20M dataset (20M ratings).\nExcept for downloading the dataset (to save you some time), I have not processed nor modified the data in any way.\n\nROOT_DIR='80-629/week4-PracticalSession'\nDATA_DIR=os.path.join(ROOT_DIR, 'dat/ml-1m/') # this is where most of our data lives\nDATA_DIR_20ML=os.path.join(ROOT_DIR, 'dat/ml-20m/') # for the tags data\n\n\n\n\nWe begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format MovieID::Name::Genres.\nAfter loading into a pandas dataFrame structure, we will have movie names (mName), IDs (mid), and movie genres (mGenres)\n\nmovies_pd = pd.read_csv(os.path.join(DATA_DIR, 'movies.dat'),\n                        sep='::',\n                        names=['mid', 'mName', 'mGenres'], engine='python',\n                        encoding='latin-1')\n\n\nprint(f'The dataset contains {movies_pd.shape[0]} movies')\n\nThe dataset contains 3883 movies\n\n\n\ndisplay(movies_pd.head())\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n0\n1\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children's|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\nUsing pandas we can also search for movies by mid or by their name:\n\nmid = 10\ndisplay(movies_pd[movies_pd.mid==mid])\n\nname = 'Machine'\ndisplay(movies_pd[movies_pd.mName.str.contains(name,\n                                               regex=False, case=False)])\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n9\n10\nGoldenEye (1995)\nAction|Adventure|Thriller\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n1409\n1433\nMachine, The (1994)\nComedy|Horror\n\n\n\n\n\n\n\n\n\n\nUsing a similar routine as above, we load the ratings data which is in this format UserID::MovieID::Rating::Timestamp, and we will name the column UserID with uid, the column MovieID with mid, the rating with rating, and the time of the rating with timestamp.\n\nratings_pd = pd.read_csv(os.path.join(DATA_DIR, 'ratings.dat'),\n                         sep='::',\n                         names=['uid', 'mid', 'rating', 'timestamp'],\n                         parse_dates=['timestamp'],\n                         infer_datetime_format=True,\n                         engine='python')\n\ndisplay(ratings_pd.head())\n\n\n\n\n\n\n\n\nuid\nmid\nrating\ntimestamp\n\n\n\n\n0\n1\n1193\n5\n978300760\n\n\n1\n1\n661\n3\n978302109\n\n\n2\n1\n914\n3\n978301968\n\n\n3\n1\n3408\n4\n978300275\n\n\n4\n1\n2355\n5\n978824291\n\n\n\n\n\n\n\n\nprint(f\"\"\"The dataset contains {ratings_pd.shape[0]} ratings,\n      from {ratings_pd.uid.nunique()} users,\n      and {ratings_pd.mid.nunique()} items.\"\"\")\n\nThe dataset contains 1000209 ratings, \n      from 6040 users, \n      and 3706 items.\n\n\n\n\n\nThe file is in this format UserID::Gender::Age::Occupation::Zip-code, which we will load in a dataFrame with the following column names uid,gender,age,occupation,zip.\n\nusers_pd = pd.read_csv(os.path.join(DATA_DIR, 'users.dat'),\n                       sep='::',\n                       names=['uid', 'gender', 'age', 'occupation', 'zip'],\n                       engine=\"python\")\n\ndisplay(users_pd.head())\n\n\n\n\n\n\n\n\nuid\ngender\nage\noccupation\nzip\n\n\n\n\n0\n1\nF\n1\n10\n48067\n\n\n1\n2\nM\n56\n16\n70072\n\n\n2\n3\nM\n25\n15\n55117\n\n\n3\n4\nM\n45\n7\n02460\n\n\n4\n5\nM\n25\n20\n55455\n\n\n\n\n\n\n\n\nprint(f'This table contains {users_pd.shape[0]} users')\n\nThis table contains 6040 users\n\n\nFurther we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and &gt;3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature.\n\nprint(f'We originally have {users_pd.zip.nunique()} different zip codes')\nusers_pd['zip'] = users_pd['zip'].apply(lambda x: x[:2])\ndisplay(users_pd['zip'].head())\nprint(f'By only keep the first two digits of each zip code, \\\nwe reduced the unique number of zip codes to {users_pd.zip.nunique()}.')\n\nWe originally have 3439 different zip codes\nBy only keep the first two digits of each zip code, we reduced the unique number of zip codes to 100.\n\n\n0    48\n1    70\n2    55\n3    02\n4    55\nName: zip, dtype: object\n\n\n\n\n\nThe remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\nWe will load the csv data movieId,tagId,relevance into a dataFrame with the columns mid,tid,relevance.\n\n# load ml-20m tags\ntags_scores = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-scores.csv.gz'),\n                          skiprows=1,\n                          names=['mid', 'tid', 'relevance'])\ndisplay(tags_scores.head(10))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\n\n\n\n\n0\n1\n1\n0.02500\n\n\n1\n1\n2\n0.02500\n\n\n2\n1\n3\n0.05775\n\n\n3\n1\n4\n0.09675\n\n\n4\n1\n5\n0.14675\n\n\n5\n1\n6\n0.21700\n\n\n6\n1\n7\n0.06700\n\n\n7\n1\n8\n0.26275\n\n\n8\n1\n9\n0.26200\n\n\n9\n1\n10\n0.03200\n\n\n\n\n\n\n\n\nprint(f'The data contains {tags_scores.tid.nunique()} unique tags.')\nprint(f'Affinities (relevances) are contained in the {tags_scores.relevance.min()}--{tags_scores.relevance.max()} range.')\ndisplay(tags_scores.relevance.describe())\n\nThe data contains 1128 unique tags.\nAffinities (relevances) are contained in the 0.00024999999999997247--1.0 range.\n\n\ncount    1.170977e+07\nmean     1.164833e-01\nstd      1.542463e-01\nmin      2.500000e-04\n25%      2.425000e-02\n50%      5.650000e-02\n75%      1.415000e-01\nmax      1.000000e+00\nName: relevance, dtype: float64\n\n\nFrom above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12.\nWe also load the tag names. This will be useful for exploration purposes.\n\ntags_names = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-tags.csv'), skiprows=1, names=['tid', 'tName'])\ndisplay(tags_names.head(10))\n\n\n\n\n\n\n\n\ntid\ntName\n\n\n\n\n0\n1\n007\n\n\n1\n2\n007 (series)\n\n\n2\n3\n18th century\n\n\n3\n4\n1920s\n\n\n4\n5\n1930s\n\n\n5\n6\n1950s\n\n\n6\n7\n1960s\n\n\n7\n8\n1970s\n\n\n8\n9\n1980s\n\n\n9\n10\n19th century\n\n\n\n\n\n\n\nSince we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (mid) are the same across these two datasets (i.e., not need for messy string match).\n(Note: Pandas’ functionalities allow you to do operations similar to what you would do in SQL for relational databases.)\n\ntags_scores = tags_scores.loc[tags_scores['mid'].isin(ratings_pd['mid'].unique())]\nprint(tags_scores.mid.nunique())\n\n3470\n\n\nWe lost a few movies compared to the original count of 3706 but we can live with that.\nNext, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie.\n\n# Keep only high-relevance tags (here this is defined as having a relevance above 0.9)\nprint('unique tags:', tags_scores['tid'].nunique())\ntags_scores_high = tags_scores.loc[tags_scores['relevance'] &gt; 0.9]\nprint('unique tags w. high relevance:', tags_scores_high['tid'].nunique())\n\nunique tags: 1128\nunique tags w. high relevance: 968\n\n\n\n\n\nLet’s get some understanding of how these tags are used. To help, we first build a dataFrame that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names).\nPandas’ merge function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)\n\ntags_high_names_movies = pd.merge(tags_scores_high, tags_names, how='inner', on='tid')\ntags_high_names_movies = pd.merge(tags_high_names_movies, movies_pd, how='inner', on='mid')\ndisplay(tags_high_names_movies.head())\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n0\n1\n63\n0.93325\nanimated\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n1\n64\n0.98575\nanimation\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n2\n1\n186\n0.95650\ncartoon\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n3\n1\n203\n0.92625\nchildhood\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n4\n1\n204\n0.96425\nchildren\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n\n\n\n\n\nSimilarly as above we can search for top movies according to a particular tag:\n\ntag = 'scary' # This is the (sub) tag we search for\ndisplay(tags_high_names_movies[\n    tags_high_names_movies.tName.str.contains(tag,\n                                               regex=False, case=False)].sort_values(by=['relevance'],\n                                                                                    ascending=False))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n13151\n2710\n882\n0.96700\nscary\nBlair Witch Project, The (1999)\nHorror\n\n\n14005\n1342\n882\n0.96625\nscary\nCandyman (1992)\nHorror\n\n\n9205\n1347\n882\n0.96550\nscary\nNightmare on Elm Street, A (1984)\nHorror\n\n\n14658\n3892\n882\n0.96475\nscary\nAnatomy (Anatomie) (2000)\nHorror\n\n\n3187\n1997\n882\n0.96200\nscary\nExorcist, The (1973)\nHorror\n\n\n14021\n2550\n882\n0.95625\nscary\nHaunting, The (1963)\nHorror|Thriller\n\n\n11492\n1350\n882\n0.94675\nscary\nOmen, The (1976)\nHorror\n\n\n11546\n2841\n882\n0.94650\nscary\nStir of Echoes (1999)\nThriller\n\n\n9340\n1974\n882\n0.94625\nscary\nFriday the 13th (1980)\nHorror\n\n\n2976\n1387\n882\n0.94250\nscary\nJaws (1975)\nAction|Horror\n\n\n5121\n2460\n882\n0.93925\nscary\nTexas Chainsaw Massacre 2, The (1986)\nHorror\n\n\n2488\n1214\n882\n0.93875\nscary\nAlien (1979)\nAction|Horror|Sci-Fi|Thriller\n\n\n9478\n3273\n882\n0.93825\nscary\nScream 3 (2000)\nHorror|Mystery|Thriller\n\n\n13208\n1590\n882\n0.93525\nscary\nEvent Horizon (1997)\nAction|Mystery|Sci-Fi|Thriller\n\n\n13916\n1321\n882\n0.93475\nscary\nAmerican Werewolf in London, An (1981)\nHorror\n\n\n11518\n2160\n882\n0.93325\nscary\nRosemary's Baby (1968)\nHorror|Thriller\n\n\n4825\n2719\n882\n0.92450\nscary\nHaunting, The (1999)\nHorror|Thriller\n\n\n7636\n3499\n882\n0.92350\nscary\nMisery (1990)\nHorror\n\n\n3452\n2762\n882\n0.92075\nscary\nSixth Sense, The (1999)\nThriller\n\n\n2781\n1258\n882\n0.91675\nscary\nShining, The (1980)\nHorror\n\n\n12355\n3081\n882\n0.91425\nscary\nSleepy Hollow (1999)\nHorror|Romance\n\n\n2343\n1200\n882\n0.90475\nscary\nAliens (1986)\nAction|Sci-Fi|Thriller|War\n\n\n\n\n\n\n\nThis next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data.\nIn the current dataset, every movie-tag id pair is a separate entry (row of the dataFrame). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example.\nTo do so, we re-encode tids using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., “cats” and “dogs”) which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag scary is id 882) but tags cannot be compared using their numbers (e.g., tag 882 is not “bigger” than tag 880 or smaller than tag 900). 1-of-K encoding deals with this by encoding each tag as a binary vector of length \\(K\\) with a single non-zero value which corresponds to the tag. In the present case, \\(K=968\\) tags, and tag scary would have a 1 at position 882.\nBelow we see that our data now has 971 columns: 968 for tag ids, 1 for mid, and 1 for relevance, and 1 for the pandas index.\n\n#print(tags_scores_high.shape)\ntags_scores_high_dum = pd.get_dummies(tags_scores_high, columns=['tid'])\ntags_scores_high_dum = tags_scores_high_dum.reset_index()\n#print(tags_scores_high_dum.shape)\ndisplay(tags_scores_high_dum.head())\ntags_per_movie = tags_scores_high_dum.groupby(\"mid\").sum()\n\n\n\n\n\n\n\n\nindex\nmid\nrelevance\ntid_1\ntid_2\ntid_3\ntid_5\ntid_6\ntid_7\ntid_9\n...\ntid_1119\ntid_1120\ntid_1121\ntid_1122\ntid_1123\ntid_1124\ntid_1125\ntid_1126\ntid_1127\ntid_1128\n\n\n\n\n0\n62\n1\n0.93325\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n63\n1\n0.98575\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n185\n1\n0.95650\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n202\n1\n0.92625\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n203\n1\n0.96425\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 971 columns\n\n\n\nWith this data we can then explore the distribution of movies per tag (and tags per movie below).\n\nth = tags_scores_high.groupby(\"tid\").count()\nhists = th.hist(bins=50, column=\"mid\", xlabelsize=15, ylabelsize=15)[0][0]\nhists.set_ylabel(\"Tags\", size=15)\nhists.set_xlabel(\"Movies per tag\", size=15)\nhists.set_title(\"Histogram of Movies per tag\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (&lt;5). On the other hand, the most popular tag was used to tag 210 movies.\nWe note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies.\n\ntname = tags_names.at[tags_names.tid.eq(th.mid.idxmax()).idxmax(), 'tName']\nprint(f'The most popular tag \"{tname}\" has been used for {th.mid.max()} movies')\n\nThe most popular tag \"comedy\" has been used for 210 movies\n\n\nUsing the same recipe, we can do something similar for movies instead of tags\n\nhists = tags_scores_high.groupby(\"mid\").count().hist(bins=40, column=\"tid\", xlabelsize=15, ylabelsize=15)\nhists[0][0].set_ylabel(\"Movies\", size=15)\nhists[0][0].set_xlabel(\"Tags per movie\", size=15)\nhists[0][0].set_title(\"Histogram of Tags per movie\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags.\n\n\n\nWhat is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags?\n\nmh = ...\nmname = ...\nprint(f'The most popular movie \"{mname}\" has {mh.tid.max()} tags')\n\nAttributeError: 'ellipsis' object has no attribute 'tid'\n\n\nIn the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features.\n\n# Join users, ratings, and tags\ndata_pd = pd.merge(users_pd, ratings_pd, how='inner', on='uid')\ndata_pd = pd.merge(data_pd, tags_per_movie, how='inner', on='mid')\n\nFor the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes.\n\n# shuffle data and keep 2% of the ratings.\n# (this small percentage ensures that all computations in this tutorial are fast)\ndata_pd = data_pd.sample(frac=0.02, random_state=1234)\nprint(data_pd.shape)\n\nWe can have a look at our current dataset.\n\nprint('Final descriptive stats of our dataset.')\nprint('\\t- %d items'   % data_pd['mid'].nunique())\nprint('\\t- %d users'   % data_pd['uid'].nunique())\nprint('\\t- %d ratings' % data_pd.shape[0])\n\nNotice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags).\n\nprint(data_pd.shape)\ndisplay(data_pd[:10])\n\n\ncols = ['gender','occupation','zip','mid','uid']\ndata_pd_dum = pd.get_dummies(data_pd, columns=cols)\nprint(data_pd_dum.shape)\ndisplay(data_pd_dum.head(10))\n\nWhile we used pandas to create dummies, scikit-learn has similar capacities. The preprocessing module is detailed here. You can also checkout the section on Categorical features.\nWe are ready to construct our first dataset. We will first use a subset of the columns (not including tags).\nBelow you will also note that we split our data into train and test using train_test_split from scikit-learn.\n\nattributes = \"mid_*|uid_*|gender_*|age|zip_*|occupation_*\"\nX = data_pd_dum.filter(regex=('('+attributes+')'))\nprint(X.shape)\n\nrating = data_pd_dum['rating']\nprint(rating.shape)\n\n# Split Train/Test\n# Keep 20% of the data for testing.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, rating, test_size=0.2, random_state=1234, shuffle=False)\n\nRecommender Systems note: We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items).\nRecommender Systems note #2: Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings).\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n # Section 2: Modelling\nRatings (likert scale) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n\\[ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2\\]\nThe MSE can be understood as the average square distance between the predictor \\(f(x_i)\\) and the target \\(y_i\\). MSE returns a non-negative quantity and the perfect predictor has an MSE of \\(0\\). If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\nTrain/Test: Recall that while we estimate the parameters of the model using the train data, we evaluate the quality of the model (its performance) using test data.\n\n\n\nIt is often helpful to use a very simple benchmark to compare against the performance of our models.\nOur initial benchmark is a model which simply predicts the mean (train) rating.\nRecommender Systems note: We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean.\n\n# Check accuracy of constant predictor\n\nprint(\"Constant predictor\")\n\nprint(\"\\tTrain mean-squared error: %.3f\"\n      % mean_squared_error(y_train,\n                           np.full_like(y_train, y_train.mean())))\nprint(\"\\tTest mean-squared error: %.3f\"\n      % mean_squared_error(y_test,\n                           np.full_like(y_test, y_train.mean())))\n\nThe train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance).\nIn terms of absolute values these indicate that, on average, our predictions are 1.3 units (\\(\\sqrt{1.6}\\)) away from the true rating. This indicates that you shouldn’t be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods.\n\n\n\nFor our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users’ gender, age, zip, and occupation. We fit this model \\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\\(\\theta_{1:6}\\) are the parameters, \\(\\text{gender}_u\\) stands for the gender of user \\(u\\) and similarly for other covariates. Also, \\(x_{\\text{uid}_u}\\) represents the identity of the user and similarly for \\(x_{\\text{mid}_i}\\) and movies.\nNote that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so \\(\\theta_{\\text{zip}}\\) has 100 dimensions.\nTraining this model involves minimizing the train MSE, this is exactly what the LinearRegression class does. (This is a least-squares problem and it can be solved in closed form.)\n\n# Create linear regression object\nreg = linear_model.LinearRegression()\n\n# Train the model using the training sets\nreg.fit(X_train, y_train)\n\nprint(\"Number of parameters: \", reg.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = reg.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = reg.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nWe note that train error \\(&lt;&lt;\\) test error (\\(&lt;&lt;\\) stands for “much smaller”). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it’s a low bias and high variance model).\nDifferent methods can help prevent the model from overfitting, this is often referred to as regularizing the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: \\[  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 \\]\nInstead of the previous $ := $.\nNote: - \\(||\\cdot||_2\\) stands for the 2-Norm. That is, the square root of the sum of the operand’s squared elements. - \\(\\alpha\\) is a hyper-parameter which denotes the strength of the regularizer (if \\(\\alpha=0\\) the regularizer vanishes and if \\(\\alpha=\\infty\\) all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning \\(\\alpha\\) along with the \\(\\theta\\)s would lead to a \\(\\alpha=0\\)).\nDuring learning the model must then tradeoff performance (MSE) and complexity (high \\(\\theta\\)s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the Ridge class from the linear_model package to fit regularized linear regression models.\n\n# Create linear regression object\nregr = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr.fit(X_train, y_train)\nfit_time = time.time() - start\n\nprint(\"Fitting time: %.2f seconds\" % fit_time)\n\nprint(\"Number of parameters:\", regr.coef_.shape[0]+1)\n\nQuestion 3 Explain why there are 7,184 parameters.\nHint: Don’t forget the intercept/bias term\n\n# Make train predictions\ny_train_pred = regr.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nCompared to above, we see that with \\(\\alpha=10\\) the train and test errors are much closer (i.e., there’s less overfitting). Presumably different values of \\(\\alpha\\) would yield different generalizations.\nQuestion 4 How do you find the “best” value of \\(\\alpha\\) for a given model and dataset?\nHint: Have a look at the RidgeCV, a cross-validation enabled version of Ridge.\nAnswer: See below.\n\n# Create linear regression object\nregRCV = ...\n\n# Train the model using the training sets\n\n\nprint(\"Number of parameters: %d, estimated alpha: %d\" % (regRCV.coef_.shape[0], regRCV.alpha_))\n\nTechnical remark: since the optimization is often done in log space, it’s typical for the set of \\(\\alpha\\)’s to be powers of 10.\n\n# Make train predictions\ny_train_pred = ...\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = ...\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nThe advantage of doing cross validation (for example using RidgeCV) is clear. It automatically searches for the best values of hyperparameters (here \\(\\alpha\\)) from a given set (here \\(\\{ 1, 10, 100 \\}\\)).\nA validation set (or cross validation) should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement validation. In such cases you will likely need to split a separate validation set from your training data–in sklearn you can use the train_test_split function. It is typical for the validation set to be the same size as the test set.\nYou should also remember to never select model hyperparameters based on performance on test set, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models.\n\nArmed with a good model we can explore the learned model including some of its predictions\n\n# helper function to return non-zero columns\ndef non_zero(row, columns):\n    col_name = list(columns[~(row == 0)])[0]\n    #r = re.sub('mid_','',l)\n    return col_name\n\n\n# get number of ratings per movie (popularity)\nmids = X_test.filter(regex=('mid_*'))\ny_mid_cols = mids.apply(lambda x: non_zero(x, mids.columns), axis=1)\nmovie_popularity = X_train.filter(regex=('mid_*')).sum(axis=0)[ y_mid_cols ]\n\n# get number of ratings per user (activity)\nuids = X_test.filter(regex=('uid_*'))\ny_uid_cols = uids.apply(lambda x: non_zero(x, uids.columns), axis=1)\nuser_activity = X_train.filter(regex=('uid_*')).sum(axis=0)[ y_uid_cols ]\n\nerr = (y_test_pred-y_test)\n\n\n# only plot a subsample for higher readability\nsubn = 500\nfig, (ax0, ax1) = plt.subplots(ncols=2)\nfig.set_figwidth(15)\nax0.scatter(movie_popularity[:subn], err[:subn])\nax0.set_ylabel('Prediction error')\nax0.set_xlabel('Movie Popularity')\n\nax1.scatter(user_activity[:subn], err[:subn])\nax1.set_ylabel('Prediction error')\nax1.set_xlabel('User Activity');\n\nAbove we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n\nThis empirical distribution looks symmetrical so there doesn’t seem to be a bias toward lower or higher predictions\nThe prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a “triangle” pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters (\\(\\theta_{\\text{mid}}\\)). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process.\n\n\n\n\nWe use a linear regression model as above but also model movie tags:\n\\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n\\]\nThe last term on the right-hand side (bolded) is the only difference wrt to our previous model.\nQuestion 5: How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance?\nHint: One model is a special case of another.\n\n# This is very similar to how we constructed our dataset above except that we add the tags columns\nX_tags = data_pd_dum.filter(regex=('('+attributes+\"|tid_*\"+')'))\nprint(X_tags.shape)\n\n# Split Train/Test. Notice that we use the same seed as above to replicate that split.\nX_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(\n    X_tags, rating, test_size=0.2, random_state=1234, shuffle=False)\nprint(X_train_tags.shape)\n\n\n# Create linear regression object\nregr_tags = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr_tags.fit(X_train_tags, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", regr_tags.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = regr_tags.predict(X_train_tags)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_tags.predict(X_test_tags)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n\nRemarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n\n\n\n\nSo far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant.\nHere we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\nNeural network basics: (We will discuss these models in some depth over the next two weeks.) - A neural network is made up of interconnected neurons. Each neuron computes a few simple operations. - In a feed-forward network, neurons are organized into sets called layers. - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer. - The first layer is called the input layer it provides data to the next layer. The last layer is the output layer it provides a prediction \\(\\hat{y}\\). - Layers in between the input layer and the output layer are called hidden layers. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (activation function): \\(f(x) = \\sum_i x_i \\theta_i\\). - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions). - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers.\nMathematically for a regression task (with a single output), a one-hidden layer neural net is: \\[\nf(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) )\n\\] where - \\(\\theta_{ij}\\) are the parameters of input \\(i\\) and neuron \\(j\\) in the hidden layer. - \\(f_h\\) is the activation function of the hidden layer - \\(\\theta'_{j}\\) are the parameters that connect the neuron \\(j\\) in the hidden layer to the output layer. - \\(f_o\\) is the activation function of the output layer\nAn intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:\n\n# a 1 hidden layer neural net, where the input has 10 dimensions (p=10) and the output 1\ninput_dims = 10 # p\nhidden_layers_size = [4] # number of hidden neurons for each hidden layer (adding a dimension adds a layer)\noutput_dims = 1 # number of outputs\n\nnetwork = DrawNN( [input_dims] + hidden_layers_size + [output_dims] )\nnetwork.draw()\n\n\n# Fit a neural network on this data.\nregr_nn = neural_network.MLPRegressor(alpha=0.1, # l2-regularization (weight decay)\n                                      hidden_layer_sizes=tuple(hidden_layers_size),\n                                      early_stopping=True, # stop if validation performance decreases\n                                      verbose=True,\n                                      random_state=1234)\nstart = time.time()\nregr_nn.fit(X_train_tags.values, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", reduce(lambda x,y: x+y,\n                                       list(map(lambda x: x.size, regr_nn.coefs_+regr_nn.intercepts_)) ))\n\nMuch like previous models we can regularize a neural net to combat overfitting: - Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by \\(\\alpha\\). - In addition, we use a second regularizer called early-stopping. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In scikit-learn, the MLPRegressor class with early_stopping=True automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters.\nQuestion 6: Why does this model come with the possibility to set the random seed (i.e., random_state) while linear regression did not?\n\n# Make train predictions\ny_train_pred = regr_nn.predict(X_train_tags.values)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_nn.predict(X_test_tags.values)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n#Train Mean squared error: 0.6623\n#Test Mean squared error: 1.0465\n\nHere is our updated table of results\n\n\n\nModel\nTest MSE\n\n\n\n\n2.2 (Linear Reg. w. basic features)\n1.031\n\n\n2.3 (2.2 + movie tags)\n0.991\n\n\n2.4 (Neural Network w. features from 2.3)\n1.029\n\n\n\nAlthough neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better.\n ### Section 3. Concluding Remarks\nThe goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it’s coming from and how to model it.\nHere are a few more parting thoughts:\n\n\nAs you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n\n\n\nscikit-learn is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\nModel Selection, i.e., which model should I use for a particular dataset/task can be daunting. This page provides some tips particular to scikit-learn. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\nNote that scikit-learn does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n\n\n\nSoftware is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. scikit-learn is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more.\n\n\n\n\nIn our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don’t have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be:\n\n\\[\nf(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\nQuestion 7: What’s wrong with the above model? Try to think about it for a minute or two before looking at the answer.\nAs we will see during week 11 (on recommender systems), many models take ratings as output and as input. For example, one could take a user’s previous ratings and try to predict one’s future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist).\n\n\n\n\n\n\nScikit-learn - Documentation - Tutorials - Help for model selection"
  },
  {
    "objectID": "Introduction_to_ML.html#start",
    "href": "Introduction_to_ML.html#start",
    "title": "MATH60629A Fall 2022",
    "section": "",
    "text": "Following this brief introduction, we now dive into the problem.\n\n# We first download the repo to get access to data and some utility code (This is specifically for colab.)\n!rm -rf 80-629/\n!git clone https://github.com/lcharlin/80-629/\n\nCloning into '80-629'...\nremote: Enumerating objects: 25, done.\nremote: Counting objects: 100% (25/25), done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 77 (delta 16), reused 19 (delta 10), pack-reused 52\nUnpacking objects: 100% (77/77), done.\n\n\nWe begin by importing the packages that we will need: - reduce function will come in handy to iteratively process data - os standard packages for performing system operations (e.g., opening files) - re package for regex - sys package to deal with system-level operations (here used to change the search path) - time package we will use to measure the duration of certain operations\n\nmatplotlib for plotting\nnumpy for linear-algebra computations\npandas for data wrangling\nsklearn (scikit-learn) for machine learning models and useful machine learning related routines\n\n\nfrom functools import reduce\nimport os\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neural_network\n\nimport sys\nsys.path += ['80-629/week4-PracticalSession/']\nfrom local_utils import DrawNN\n\n # Section 1: Data Pre-Processing\nIn the following we load data from several csv files and pre-process it.\nWhile this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\nI suggest that you read this section, but that you spend most of your time on the other sections. If you have time at the end, you can come back and do this more thoroughly.\n\n\nWe will use the publically available movielens dataset. The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the ML-1M data (it contains 1M ratings) but we will also use movie tags from the ML-20M dataset (20M ratings).\nExcept for downloading the dataset (to save you some time), I have not processed nor modified the data in any way.\n\nROOT_DIR='80-629/week4-PracticalSession'\nDATA_DIR=os.path.join(ROOT_DIR, 'dat/ml-1m/') # this is where most of our data lives\nDATA_DIR_20ML=os.path.join(ROOT_DIR, 'dat/ml-20m/') # for the tags data\n\n\n\n\nWe begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format MovieID::Name::Genres.\nAfter loading into a pandas dataFrame structure, we will have movie names (mName), IDs (mid), and movie genres (mGenres)\n\nmovies_pd = pd.read_csv(os.path.join(DATA_DIR, 'movies.dat'),\n                        sep='::',\n                        names=['mid', 'mName', 'mGenres'], engine='python',\n                        encoding='latin-1')\n\n\nprint(f'The dataset contains {movies_pd.shape[0]} movies')\n\nThe dataset contains 3883 movies\n\n\n\ndisplay(movies_pd.head())\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n0\n1\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children's|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\nUsing pandas we can also search for movies by mid or by their name:\n\nmid = 10\ndisplay(movies_pd[movies_pd.mid==mid])\n\nname = 'Machine'\ndisplay(movies_pd[movies_pd.mName.str.contains(name,\n                                               regex=False, case=False)])\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n9\n10\nGoldenEye (1995)\nAction|Adventure|Thriller\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n1409\n1433\nMachine, The (1994)\nComedy|Horror\n\n\n\n\n\n\n\n\n\n\nUsing a similar routine as above, we load the ratings data which is in this format UserID::MovieID::Rating::Timestamp, and we will name the column UserID with uid, the column MovieID with mid, the rating with rating, and the time of the rating with timestamp.\n\nratings_pd = pd.read_csv(os.path.join(DATA_DIR, 'ratings.dat'),\n                         sep='::',\n                         names=['uid', 'mid', 'rating', 'timestamp'],\n                         parse_dates=['timestamp'],\n                         infer_datetime_format=True,\n                         engine='python')\n\ndisplay(ratings_pd.head())\n\n\n\n\n\n\n\n\nuid\nmid\nrating\ntimestamp\n\n\n\n\n0\n1\n1193\n5\n978300760\n\n\n1\n1\n661\n3\n978302109\n\n\n2\n1\n914\n3\n978301968\n\n\n3\n1\n3408\n4\n978300275\n\n\n4\n1\n2355\n5\n978824291\n\n\n\n\n\n\n\n\nprint(f\"\"\"The dataset contains {ratings_pd.shape[0]} ratings,\n      from {ratings_pd.uid.nunique()} users,\n      and {ratings_pd.mid.nunique()} items.\"\"\")\n\nThe dataset contains 1000209 ratings, \n      from 6040 users, \n      and 3706 items.\n\n\n\n\n\nThe file is in this format UserID::Gender::Age::Occupation::Zip-code, which we will load in a dataFrame with the following column names uid,gender,age,occupation,zip.\n\nusers_pd = pd.read_csv(os.path.join(DATA_DIR, 'users.dat'),\n                       sep='::',\n                       names=['uid', 'gender', 'age', 'occupation', 'zip'],\n                       engine=\"python\")\n\ndisplay(users_pd.head())\n\n\n\n\n\n\n\n\nuid\ngender\nage\noccupation\nzip\n\n\n\n\n0\n1\nF\n1\n10\n48067\n\n\n1\n2\nM\n56\n16\n70072\n\n\n2\n3\nM\n25\n15\n55117\n\n\n3\n4\nM\n45\n7\n02460\n\n\n4\n5\nM\n25\n20\n55455\n\n\n\n\n\n\n\n\nprint(f'This table contains {users_pd.shape[0]} users')\n\nThis table contains 6040 users\n\n\nFurther we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and &gt;3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature.\n\nprint(f'We originally have {users_pd.zip.nunique()} different zip codes')\nusers_pd['zip'] = users_pd['zip'].apply(lambda x: x[:2])\ndisplay(users_pd['zip'].head())\nprint(f'By only keep the first two digits of each zip code, \\\nwe reduced the unique number of zip codes to {users_pd.zip.nunique()}.')\n\nWe originally have 3439 different zip codes\nBy only keep the first two digits of each zip code, we reduced the unique number of zip codes to 100.\n\n\n0    48\n1    70\n2    55\n3    02\n4    55\nName: zip, dtype: object\n\n\n\n\n\nThe remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\nWe will load the csv data movieId,tagId,relevance into a dataFrame with the columns mid,tid,relevance.\n\n# load ml-20m tags\ntags_scores = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-scores.csv.gz'),\n                          skiprows=1,\n                          names=['mid', 'tid', 'relevance'])\ndisplay(tags_scores.head(10))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\n\n\n\n\n0\n1\n1\n0.02500\n\n\n1\n1\n2\n0.02500\n\n\n2\n1\n3\n0.05775\n\n\n3\n1\n4\n0.09675\n\n\n4\n1\n5\n0.14675\n\n\n5\n1\n6\n0.21700\n\n\n6\n1\n7\n0.06700\n\n\n7\n1\n8\n0.26275\n\n\n8\n1\n9\n0.26200\n\n\n9\n1\n10\n0.03200\n\n\n\n\n\n\n\n\nprint(f'The data contains {tags_scores.tid.nunique()} unique tags.')\nprint(f'Affinities (relevances) are contained in the {tags_scores.relevance.min()}--{tags_scores.relevance.max()} range.')\ndisplay(tags_scores.relevance.describe())\n\nThe data contains 1128 unique tags.\nAffinities (relevances) are contained in the 0.00024999999999997247--1.0 range.\n\n\ncount    1.170977e+07\nmean     1.164833e-01\nstd      1.542463e-01\nmin      2.500000e-04\n25%      2.425000e-02\n50%      5.650000e-02\n75%      1.415000e-01\nmax      1.000000e+00\nName: relevance, dtype: float64\n\n\nFrom above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12.\nWe also load the tag names. This will be useful for exploration purposes.\n\ntags_names = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-tags.csv'), skiprows=1, names=['tid', 'tName'])\ndisplay(tags_names.head(10))\n\n\n\n\n\n\n\n\ntid\ntName\n\n\n\n\n0\n1\n007\n\n\n1\n2\n007 (series)\n\n\n2\n3\n18th century\n\n\n3\n4\n1920s\n\n\n4\n5\n1930s\n\n\n5\n6\n1950s\n\n\n6\n7\n1960s\n\n\n7\n8\n1970s\n\n\n8\n9\n1980s\n\n\n9\n10\n19th century\n\n\n\n\n\n\n\nSince we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (mid) are the same across these two datasets (i.e., not need for messy string match).\n(Note: Pandas’ functionalities allow you to do operations similar to what you would do in SQL for relational databases.)\n\ntags_scores = tags_scores.loc[tags_scores['mid'].isin(ratings_pd['mid'].unique())]\nprint(tags_scores.mid.nunique())\n\n3470\n\n\nWe lost a few movies compared to the original count of 3706 but we can live with that.\nNext, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie.\n\n# Keep only high-relevance tags (here this is defined as having a relevance above 0.9)\nprint('unique tags:', tags_scores['tid'].nunique())\ntags_scores_high = tags_scores.loc[tags_scores['relevance'] &gt; 0.9]\nprint('unique tags w. high relevance:', tags_scores_high['tid'].nunique())\n\nunique tags: 1128\nunique tags w. high relevance: 968\n\n\n\n\n\nLet’s get some understanding of how these tags are used. To help, we first build a dataFrame that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names).\nPandas’ merge function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)\n\ntags_high_names_movies = pd.merge(tags_scores_high, tags_names, how='inner', on='tid')\ntags_high_names_movies = pd.merge(tags_high_names_movies, movies_pd, how='inner', on='mid')\ndisplay(tags_high_names_movies.head())\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n0\n1\n63\n0.93325\nanimated\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n1\n64\n0.98575\nanimation\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n2\n1\n186\n0.95650\ncartoon\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n3\n1\n203\n0.92625\nchildhood\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n4\n1\n204\n0.96425\nchildren\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n\n\n\n\n\nSimilarly as above we can search for top movies according to a particular tag:\n\ntag = 'scary' # This is the (sub) tag we search for\ndisplay(tags_high_names_movies[\n    tags_high_names_movies.tName.str.contains(tag,\n                                               regex=False, case=False)].sort_values(by=['relevance'],\n                                                                                    ascending=False))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n13151\n2710\n882\n0.96700\nscary\nBlair Witch Project, The (1999)\nHorror\n\n\n14005\n1342\n882\n0.96625\nscary\nCandyman (1992)\nHorror\n\n\n9205\n1347\n882\n0.96550\nscary\nNightmare on Elm Street, A (1984)\nHorror\n\n\n14658\n3892\n882\n0.96475\nscary\nAnatomy (Anatomie) (2000)\nHorror\n\n\n3187\n1997\n882\n0.96200\nscary\nExorcist, The (1973)\nHorror\n\n\n14021\n2550\n882\n0.95625\nscary\nHaunting, The (1963)\nHorror|Thriller\n\n\n11492\n1350\n882\n0.94675\nscary\nOmen, The (1976)\nHorror\n\n\n11546\n2841\n882\n0.94650\nscary\nStir of Echoes (1999)\nThriller\n\n\n9340\n1974\n882\n0.94625\nscary\nFriday the 13th (1980)\nHorror\n\n\n2976\n1387\n882\n0.94250\nscary\nJaws (1975)\nAction|Horror\n\n\n5121\n2460\n882\n0.93925\nscary\nTexas Chainsaw Massacre 2, The (1986)\nHorror\n\n\n2488\n1214\n882\n0.93875\nscary\nAlien (1979)\nAction|Horror|Sci-Fi|Thriller\n\n\n9478\n3273\n882\n0.93825\nscary\nScream 3 (2000)\nHorror|Mystery|Thriller\n\n\n13208\n1590\n882\n0.93525\nscary\nEvent Horizon (1997)\nAction|Mystery|Sci-Fi|Thriller\n\n\n13916\n1321\n882\n0.93475\nscary\nAmerican Werewolf in London, An (1981)\nHorror\n\n\n11518\n2160\n882\n0.93325\nscary\nRosemary's Baby (1968)\nHorror|Thriller\n\n\n4825\n2719\n882\n0.92450\nscary\nHaunting, The (1999)\nHorror|Thriller\n\n\n7636\n3499\n882\n0.92350\nscary\nMisery (1990)\nHorror\n\n\n3452\n2762\n882\n0.92075\nscary\nSixth Sense, The (1999)\nThriller\n\n\n2781\n1258\n882\n0.91675\nscary\nShining, The (1980)\nHorror\n\n\n12355\n3081\n882\n0.91425\nscary\nSleepy Hollow (1999)\nHorror|Romance\n\n\n2343\n1200\n882\n0.90475\nscary\nAliens (1986)\nAction|Sci-Fi|Thriller|War\n\n\n\n\n\n\n\nThis next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data.\nIn the current dataset, every movie-tag id pair is a separate entry (row of the dataFrame). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example.\nTo do so, we re-encode tids using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., “cats” and “dogs”) which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag scary is id 882) but tags cannot be compared using their numbers (e.g., tag 882 is not “bigger” than tag 880 or smaller than tag 900). 1-of-K encoding deals with this by encoding each tag as a binary vector of length \\(K\\) with a single non-zero value which corresponds to the tag. In the present case, \\(K=968\\) tags, and tag scary would have a 1 at position 882.\nBelow we see that our data now has 971 columns: 968 for tag ids, 1 for mid, and 1 for relevance, and 1 for the pandas index.\n\n#print(tags_scores_high.shape)\ntags_scores_high_dum = pd.get_dummies(tags_scores_high, columns=['tid'])\ntags_scores_high_dum = tags_scores_high_dum.reset_index()\n#print(tags_scores_high_dum.shape)\ndisplay(tags_scores_high_dum.head())\ntags_per_movie = tags_scores_high_dum.groupby(\"mid\").sum()\n\n\n\n\n\n\n\n\nindex\nmid\nrelevance\ntid_1\ntid_2\ntid_3\ntid_5\ntid_6\ntid_7\ntid_9\n...\ntid_1119\ntid_1120\ntid_1121\ntid_1122\ntid_1123\ntid_1124\ntid_1125\ntid_1126\ntid_1127\ntid_1128\n\n\n\n\n0\n62\n1\n0.93325\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n63\n1\n0.98575\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n185\n1\n0.95650\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n202\n1\n0.92625\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n203\n1\n0.96425\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 971 columns\n\n\n\nWith this data we can then explore the distribution of movies per tag (and tags per movie below).\n\nth = tags_scores_high.groupby(\"tid\").count()\nhists = th.hist(bins=50, column=\"mid\", xlabelsize=15, ylabelsize=15)[0][0]\nhists.set_ylabel(\"Tags\", size=15)\nhists.set_xlabel(\"Movies per tag\", size=15)\nhists.set_title(\"Histogram of Movies per tag\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (&lt;5). On the other hand, the most popular tag was used to tag 210 movies.\nWe note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies.\n\ntname = tags_names.at[tags_names.tid.eq(th.mid.idxmax()).idxmax(), 'tName']\nprint(f'The most popular tag \"{tname}\" has been used for {th.mid.max()} movies')\n\nThe most popular tag \"comedy\" has been used for 210 movies\n\n\nUsing the same recipe, we can do something similar for movies instead of tags\n\nhists = tags_scores_high.groupby(\"mid\").count().hist(bins=40, column=\"tid\", xlabelsize=15, ylabelsize=15)\nhists[0][0].set_ylabel(\"Movies\", size=15)\nhists[0][0].set_xlabel(\"Tags per movie\", size=15)\nhists[0][0].set_title(\"Histogram of Tags per movie\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags.\n\n\n\nWhat is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags?\n\nmh = ...\nmname = ...\nprint(f'The most popular movie \"{mname}\" has {mh.tid.max()} tags')\n\nAttributeError: 'ellipsis' object has no attribute 'tid'\n\n\nIn the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features.\n\n# Join users, ratings, and tags\ndata_pd = pd.merge(users_pd, ratings_pd, how='inner', on='uid')\ndata_pd = pd.merge(data_pd, tags_per_movie, how='inner', on='mid')\n\nFor the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes.\n\n# shuffle data and keep 2% of the ratings.\n# (this small percentage ensures that all computations in this tutorial are fast)\ndata_pd = data_pd.sample(frac=0.02, random_state=1234)\nprint(data_pd.shape)\n\nWe can have a look at our current dataset.\n\nprint('Final descriptive stats of our dataset.')\nprint('\\t- %d items'   % data_pd['mid'].nunique())\nprint('\\t- %d users'   % data_pd['uid'].nunique())\nprint('\\t- %d ratings' % data_pd.shape[0])\n\nNotice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags).\n\nprint(data_pd.shape)\ndisplay(data_pd[:10])\n\n\ncols = ['gender','occupation','zip','mid','uid']\ndata_pd_dum = pd.get_dummies(data_pd, columns=cols)\nprint(data_pd_dum.shape)\ndisplay(data_pd_dum.head(10))\n\nWhile we used pandas to create dummies, scikit-learn has similar capacities. The preprocessing module is detailed here. You can also checkout the section on Categorical features.\nWe are ready to construct our first dataset. We will first use a subset of the columns (not including tags).\nBelow you will also note that we split our data into train and test using train_test_split from scikit-learn.\n\nattributes = \"mid_*|uid_*|gender_*|age|zip_*|occupation_*\"\nX = data_pd_dum.filter(regex=('('+attributes+')'))\nprint(X.shape)\n\nrating = data_pd_dum['rating']\nprint(rating.shape)\n\n# Split Train/Test\n# Keep 20% of the data for testing.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, rating, test_size=0.2, random_state=1234, shuffle=False)\n\nRecommender Systems note: We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items).\nRecommender Systems note #2: Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings).\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n # Section 2: Modelling\nRatings (likert scale) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n\\[ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2\\]\nThe MSE can be understood as the average square distance between the predictor \\(f(x_i)\\) and the target \\(y_i\\). MSE returns a non-negative quantity and the perfect predictor has an MSE of \\(0\\). If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\nTrain/Test: Recall that while we estimate the parameters of the model using the train data, we evaluate the quality of the model (its performance) using test data.\n\n\n\nIt is often helpful to use a very simple benchmark to compare against the performance of our models.\nOur initial benchmark is a model which simply predicts the mean (train) rating.\nRecommender Systems note: We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean.\n\n# Check accuracy of constant predictor\n\nprint(\"Constant predictor\")\n\nprint(\"\\tTrain mean-squared error: %.3f\"\n      % mean_squared_error(y_train,\n                           np.full_like(y_train, y_train.mean())))\nprint(\"\\tTest mean-squared error: %.3f\"\n      % mean_squared_error(y_test,\n                           np.full_like(y_test, y_train.mean())))\n\nThe train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance).\nIn terms of absolute values these indicate that, on average, our predictions are 1.3 units (\\(\\sqrt{1.6}\\)) away from the true rating. This indicates that you shouldn’t be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods.\n\n\n\nFor our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users’ gender, age, zip, and occupation. We fit this model \\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\\(\\theta_{1:6}\\) are the parameters, \\(\\text{gender}_u\\) stands for the gender of user \\(u\\) and similarly for other covariates. Also, \\(x_{\\text{uid}_u}\\) represents the identity of the user and similarly for \\(x_{\\text{mid}_i}\\) and movies.\nNote that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so \\(\\theta_{\\text{zip}}\\) has 100 dimensions.\nTraining this model involves minimizing the train MSE, this is exactly what the LinearRegression class does. (This is a least-squares problem and it can be solved in closed form.)\n\n# Create linear regression object\nreg = linear_model.LinearRegression()\n\n# Train the model using the training sets\nreg.fit(X_train, y_train)\n\nprint(\"Number of parameters: \", reg.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = reg.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = reg.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nWe note that train error \\(&lt;&lt;\\) test error (\\(&lt;&lt;\\) stands for “much smaller”). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it’s a low bias and high variance model).\nDifferent methods can help prevent the model from overfitting, this is often referred to as regularizing the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: \\[  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 \\]\nInstead of the previous $ := $.\nNote: - \\(||\\cdot||_2\\) stands for the 2-Norm. That is, the square root of the sum of the operand’s squared elements. - \\(\\alpha\\) is a hyper-parameter which denotes the strength of the regularizer (if \\(\\alpha=0\\) the regularizer vanishes and if \\(\\alpha=\\infty\\) all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning \\(\\alpha\\) along with the \\(\\theta\\)s would lead to a \\(\\alpha=0\\)).\nDuring learning the model must then tradeoff performance (MSE) and complexity (high \\(\\theta\\)s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the Ridge class from the linear_model package to fit regularized linear regression models.\n\n# Create linear regression object\nregr = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr.fit(X_train, y_train)\nfit_time = time.time() - start\n\nprint(\"Fitting time: %.2f seconds\" % fit_time)\n\nprint(\"Number of parameters:\", regr.coef_.shape[0]+1)\n\nQuestion 3 Explain why there are 7,184 parameters.\nHint: Don’t forget the intercept/bias term\n\n# Make train predictions\ny_train_pred = regr.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nCompared to above, we see that with \\(\\alpha=10\\) the train and test errors are much closer (i.e., there’s less overfitting). Presumably different values of \\(\\alpha\\) would yield different generalizations.\nQuestion 4 How do you find the “best” value of \\(\\alpha\\) for a given model and dataset?\nHint: Have a look at the RidgeCV, a cross-validation enabled version of Ridge.\nAnswer: See below.\n\n# Create linear regression object\nregRCV = ...\n\n# Train the model using the training sets\n\n\nprint(\"Number of parameters: %d, estimated alpha: %d\" % (regRCV.coef_.shape[0], regRCV.alpha_))\n\nTechnical remark: since the optimization is often done in log space, it’s typical for the set of \\(\\alpha\\)’s to be powers of 10.\n\n# Make train predictions\ny_train_pred = ...\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = ...\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nThe advantage of doing cross validation (for example using RidgeCV) is clear. It automatically searches for the best values of hyperparameters (here \\(\\alpha\\)) from a given set (here \\(\\{ 1, 10, 100 \\}\\)).\nA validation set (or cross validation) should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement validation. In such cases you will likely need to split a separate validation set from your training data–in sklearn you can use the train_test_split function. It is typical for the validation set to be the same size as the test set.\nYou should also remember to never select model hyperparameters based on performance on test set, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models.\n\nArmed with a good model we can explore the learned model including some of its predictions\n\n# helper function to return non-zero columns\ndef non_zero(row, columns):\n    col_name = list(columns[~(row == 0)])[0]\n    #r = re.sub('mid_','',l)\n    return col_name\n\n\n# get number of ratings per movie (popularity)\nmids = X_test.filter(regex=('mid_*'))\ny_mid_cols = mids.apply(lambda x: non_zero(x, mids.columns), axis=1)\nmovie_popularity = X_train.filter(regex=('mid_*')).sum(axis=0)[ y_mid_cols ]\n\n# get number of ratings per user (activity)\nuids = X_test.filter(regex=('uid_*'))\ny_uid_cols = uids.apply(lambda x: non_zero(x, uids.columns), axis=1)\nuser_activity = X_train.filter(regex=('uid_*')).sum(axis=0)[ y_uid_cols ]\n\nerr = (y_test_pred-y_test)\n\n\n# only plot a subsample for higher readability\nsubn = 500\nfig, (ax0, ax1) = plt.subplots(ncols=2)\nfig.set_figwidth(15)\nax0.scatter(movie_popularity[:subn], err[:subn])\nax0.set_ylabel('Prediction error')\nax0.set_xlabel('Movie Popularity')\n\nax1.scatter(user_activity[:subn], err[:subn])\nax1.set_ylabel('Prediction error')\nax1.set_xlabel('User Activity');\n\nAbove we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n\nThis empirical distribution looks symmetrical so there doesn’t seem to be a bias toward lower or higher predictions\nThe prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a “triangle” pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters (\\(\\theta_{\\text{mid}}\\)). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process.\n\n\n\n\nWe use a linear regression model as above but also model movie tags:\n\\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n\\]\nThe last term on the right-hand side (bolded) is the only difference wrt to our previous model.\nQuestion 5: How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance?\nHint: One model is a special case of another.\n\n# This is very similar to how we constructed our dataset above except that we add the tags columns\nX_tags = data_pd_dum.filter(regex=('('+attributes+\"|tid_*\"+')'))\nprint(X_tags.shape)\n\n# Split Train/Test. Notice that we use the same seed as above to replicate that split.\nX_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(\n    X_tags, rating, test_size=0.2, random_state=1234, shuffle=False)\nprint(X_train_tags.shape)\n\n\n# Create linear regression object\nregr_tags = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr_tags.fit(X_train_tags, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", regr_tags.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = regr_tags.predict(X_train_tags)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_tags.predict(X_test_tags)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n\nRemarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n\n\n\n\nSo far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant.\nHere we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\nNeural network basics: (We will discuss these models in some depth over the next two weeks.) - A neural network is made up of interconnected neurons. Each neuron computes a few simple operations. - In a feed-forward network, neurons are organized into sets called layers. - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer. - The first layer is called the input layer it provides data to the next layer. The last layer is the output layer it provides a prediction \\(\\hat{y}\\). - Layers in between the input layer and the output layer are called hidden layers. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (activation function): \\(f(x) = \\sum_i x_i \\theta_i\\). - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions). - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers.\nMathematically for a regression task (with a single output), a one-hidden layer neural net is: \\[\nf(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) )\n\\] where - \\(\\theta_{ij}\\) are the parameters of input \\(i\\) and neuron \\(j\\) in the hidden layer. - \\(f_h\\) is the activation function of the hidden layer - \\(\\theta'_{j}\\) are the parameters that connect the neuron \\(j\\) in the hidden layer to the output layer. - \\(f_o\\) is the activation function of the output layer\nAn intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:\n\n# a 1 hidden layer neural net, where the input has 10 dimensions (p=10) and the output 1\ninput_dims = 10 # p\nhidden_layers_size = [4] # number of hidden neurons for each hidden layer (adding a dimension adds a layer)\noutput_dims = 1 # number of outputs\n\nnetwork = DrawNN( [input_dims] + hidden_layers_size + [output_dims] )\nnetwork.draw()\n\n\n# Fit a neural network on this data.\nregr_nn = neural_network.MLPRegressor(alpha=0.1, # l2-regularization (weight decay)\n                                      hidden_layer_sizes=tuple(hidden_layers_size),\n                                      early_stopping=True, # stop if validation performance decreases\n                                      verbose=True,\n                                      random_state=1234)\nstart = time.time()\nregr_nn.fit(X_train_tags.values, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", reduce(lambda x,y: x+y,\n                                       list(map(lambda x: x.size, regr_nn.coefs_+regr_nn.intercepts_)) ))\n\nMuch like previous models we can regularize a neural net to combat overfitting: - Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by \\(\\alpha\\). - In addition, we use a second regularizer called early-stopping. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In scikit-learn, the MLPRegressor class with early_stopping=True automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters.\nQuestion 6: Why does this model come with the possibility to set the random seed (i.e., random_state) while linear regression did not?\n\n# Make train predictions\ny_train_pred = regr_nn.predict(X_train_tags.values)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_nn.predict(X_test_tags.values)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n#Train Mean squared error: 0.6623\n#Test Mean squared error: 1.0465\n\nHere is our updated table of results\n\n\n\nModel\nTest MSE\n\n\n\n\n2.2 (Linear Reg. w. basic features)\n1.031\n\n\n2.3 (2.2 + movie tags)\n0.991\n\n\n2.4 (Neural Network w. features from 2.3)\n1.029\n\n\n\nAlthough neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better.\n ### Section 3. Concluding Remarks\nThe goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it’s coming from and how to model it.\nHere are a few more parting thoughts:\n\n\nAs you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n\n\n\nscikit-learn is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\nModel Selection, i.e., which model should I use for a particular dataset/task can be daunting. This page provides some tips particular to scikit-learn. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\nNote that scikit-learn does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n\n\n\nSoftware is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. scikit-learn is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more.\n\n\n\n\nIn our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don’t have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be:\n\n\\[\nf(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\nQuestion 7: What’s wrong with the above model? Try to think about it for a minute or two before looking at the answer.\nAs we will see during week 11 (on recommender systems), many models take ratings as output and as input. For example, one could take a user’s previous ratings and try to predict one’s future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist)."
  },
  {
    "objectID": "Introduction_to_ML.html#references",
    "href": "Introduction_to_ML.html#references",
    "title": "MATH60629A Fall 2022",
    "section": "",
    "text": "Scikit-learn - Documentation - Tutorials - Help for model selection"
  },
  {
    "objectID": "Neural_Networks_questions.html",
    "href": "Neural_Networks_questions.html",
    "title": "MATH60629A",
    "section": "",
    "text": "This tutorial explores neural networks.\n\nimport numpy as np\n\n# Code to obtain utils.py\n!wget https://raw.githubusercontent.com/lcharlin/80-629/master/week5-NeuralNetworks/utils.py -O utilities.py\n\n--2021-10-01 15:19:56--  https://raw.githubusercontent.com/lcharlin/80-629/master/week5-NeuralNetworks/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13166 (13K) [text/plain]\nSaving to: ‘utilities.py’\n\nutilities.py        100%[===================&gt;]  12.86K  --.-KB/s    in 0.002s  \n\n2021-10-01 15:19:56 (7.38 MB/s) - ‘utilities.py’ saved [13166/13166]\n\n\n\n\n\nIn order to classify the examples, we will use the following simple neural network:\n\nwhere \\(\\sigma\\) is the sigmoid function defined as:\n\\[\n    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n\\]\n\n\nAssume that the parameters of the neural network are as follows:\n\\[\\begin{aligned}\n& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n& b_1 = 25 & b_2 = 40 && b_3 = -30\n\\end{aligned}\\]\nWhat would be the predicted label for the following data points:\n\n\n\nx1\nx2\no\nlabel\n\n\n\n\n4\n-4\n\n\n\n\n-4\n4\n\n\n\n\n-4\n-4\n\n\n\n\n4\n4\n\n\n\n\n\nYou can use the following piece of code to evaluate the output of the network:\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef nn1(x1, x2, w1, w2, w3, w4, w5, w6, b1, b2, b3):\n    h1 = sigmoid(w1*x1 + w3*x2 + b1)\n    h2 = sigmoid(w2*x1 + w4*x2 + b2)\n    o = sigmoid(w5*h1 + w6*h2 + b3)\n    return o\n\n\n\n\n\nLet’s move to a slightly more realistic example. Here we focus on the task of (binary) classification. As always, we first load the data that we want to classify:\n\nfrom utilities import load_data, plot_boundaries, plot_data # we wrote some helper functions\nX_train, y_train, X_test, y_test = load_data()          # to help with data loading\n\nYou can plot the data using the helper function plot_data:\n\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nAs you can see, this data is not linearly separable. In other words, the positive and negative examples cannot be separated using a linear classifier. Our goal for the rest of this notebook-session is to learn the parameters of a neural-network model which can separate the positives from the negative examples.\nWhat do we mean by learning the parameters? Remember that our neural network has 9 parameters including three biases (\\(w_1, \\ldots, w_6, b_1, b_2, b_3\\)). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best.\nLet’s see how different choices of parameters changes the classifier. For a given set of parameters, the function plot_boundaries shows the regions of positive prediction (coloured blue) and negative prediction (coloured red):\n\nw1 = 1; w2 = 1; w3 = 1; w4 = 1; w5 = 1; w6 = 1\nb1 = 0; b2 = 0; b3 = -1\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n\n\n\n\nNow let’s project the plot of data on these decision boundaries:\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nIt appears that the classifier obtained using the above set of parameters does not match our data. (Of course, this is to be expected. This classifier with fixed weights a priori has a high bias and a low variance.)\n\n\nTry the alternatives below and see which one is a better match for our data:\n\nw1 = -1; w2 = -1; w3 = -1; w4 = -1; w5 = 4; w6 = -3\nb1 = -4; b2 = 4; b3 = 1\n\n\nw1 = 1; w2 = -1; w3 = -1; w4 = -1; w5 = -4; w6 = 3\nb1 = 4; b2 = -4; b3 = 2\n\n\nw1 = -1; w2 = 2; w3 = 1; w4 = -2; w5 = 4; w6 = 4\nb1 = 5; b2 = 8; b3 = -6\n\nObviously, we need a better way than trial and error to find the best parameters. The way that we do this is by minimizing a loss function.\n\n\n\n\nA loss function evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the binary cross-entropy loss. Let’s represent our training data by the set \\(\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}\\) and our neural network function by \\(f\\). Then the binary cross-entropy loss function will be defined as:\n\\[\\begin{equation}\n    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n\\end{equation}\\]\nThe binary cross-entropy relates to the Bernoulli distribution (maximizing the Bernoulli likelihood is equivalent to minimizing the binary cross-entropy). It is the loss function that should be used for binary classification problems. It can be generalized to multiclass classification problems, see cross entropy.\n\n\nLet’s see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of \\(X, f(X), y\\) of those four examples are as follows:\n\n\n\nX\nf(X)\ny\n\n\n\n\n(5.4, 1.6)\n1\n1\n\n\n(1.4, -0.5)\n0.3679\n1\n\n\n(3.5, -3)\n0.8647\n0\n\n\n(-3.5, 1.1)\n0\n0\n\n\n\nCalculate the loss function using the equation above. You can calculate the log using this function:\n\nnp.log(0.5)\n\n-0.6931471805599453\n\n\nIt is important to remember that the loss function \\(l\\) is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n\\[\\begin{equation}\n    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n\\end{equation}\\]\nIn principle, we want to find the set of parameters \\(\\mathbf{w}, \\mathbf{b}\\) for which \\(\\ell(\\mathbf{w}, \\mathbf{b})\\) has the smallest value. We will use gradient descent to find these values.\n\n\n\nThe plot below shows the function \\(f(x_1, x_2) = x_1^2 + x_2^2\\):\n\n\n\n\nPoint A on the plot has coordinates \\((1, 1, 3)\\). The blue vector AB shows the direction \\((-1, -1)\\), and the green vector AC shows the direction \\((0, -1)\\). Assume that we are at initial point \\((1, 1)\\) and we want to move in a direction that minimizes the function \\(f\\). Which of these two directions moves faster towards the minimum: \\((-1, -1)\\) or \\((0, -1)\\)?\n\n\n\nCalculate the gradient of function \\(f\\) in the point \\((1, 1)\\). How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?\n\n\n\n\nWe now understand the theory of training neural networks. But how do we do this in practice? We will now develop our practical skills using the scikit-learn library to train our tiny network. Let’s first define the network:\n\nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(2,),\n                    activation='logistic',\n                    solver='lbfgs',\n                    random_state=0,\n                    max_iter=500,\n                    tol=1e-7)\n\nThe argument hidden_layer_sizes=(2,) states that we only have one hidden layer with two neurons, and the argument activation='logistic' shows that we use the sigmoid activation function (Let’s ignore the other arguments for now).\nWe will now train the network using our training data:\n\nclf.fit(X_train, y_train)\n\nMLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(2,), learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=0, shuffle=True, solver='lbfgs',\n              tol=1e-07, validation_fraction=0.1, verbose=False,\n              warm_start=False)\n\n\nOnce the network is trained, use the helper function tiny_net_parameters to get the parameters of the trained network (tiny_net_parameters is a wrapper around clf.coefs_ and clf.intercepts_):\n\nfrom utils import tiny_net_parameters\nw1, w2, w3, w4, w5, w6, b1, b2, b3 = tiny_net_parameters(clf)\n\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nThe learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data).\n\nIn addition to the decision boundaries in the original data space, we can also visualize how the data are transformed through the neural networks. Since we use a hidden layer with two neurons, we can visualize its “output” in two dimensions.\nEn plus des frontières de décisions dans l’espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions.\n(for better visibility, we changed the color of the yellow class to blue.)\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_train, y_train, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')\n\n\n\n\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_test, y_test, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')\n\n\n\n\n\n\n\nWe will now investigate a few properties of neural networks using tensorflow playground. Take a few minutes to familiarize yourself with the playground:\n\nChange the number of hidden layers to one\nChange the data distribution to exclusive OR\nPush the run button and see how the network is trained\nStop training after epoch 500 (each epoch involves doing gradient descent using the complete dataset)\nHover over the neurons in the hidden layer and see the vizualization of their outputs.\n\n\n\nOpen this example on tensorflow playground.\n\nPush the run button and see the learning process for 500 epochs. What do you observe?\nStop training and press the restart button. Change the learning rate from 3 to 0.1, and press the run button again. What is different from the previous run?\nTry these steps using three learning rates: 0.3, 0.03, and 0.003:\nPress the reset button\nChange the learning rate\nPress the step button (located at the right of run button) a few times, and observe how the training/test loss changes in each step.\n\nWhich of those three rates would you use?\n\n\n\nOpen this example on playground.\nLet’s first observe a few things about this example. Check the box titled Show test data. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting. - Press the run button and let the training proceed for 500 epochs, then pause the training. - What do you think about the decision boundary of the classifier? - What causes the difference between the training error and test error? (Check the Show test data box again) - Write down the test error\nWe will now see how we can avoid overfitting using \\(L_2\\) regularization. - Press the restart button - Change regularization from None to L2 - Change Regularization rate from 0 to 0.3 - Press the run button and run the model for 500 epochs - What is different from the previous setting? - Write down the test error\nJust like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003: - Press the restart button - Change Regularization rate - Press the run button and run the model for 500 epochs - Write down the test error\nWhich of these regularization rates would you use?"
  },
  {
    "objectID": "Neural_Networks_questions.html#a-tiny-neural-network-classifier",
    "href": "Neural_Networks_questions.html#a-tiny-neural-network-classifier",
    "title": "MATH60629A",
    "section": "",
    "text": "In order to classify the examples, we will use the following simple neural network:\n\nwhere \\(\\sigma\\) is the sigmoid function defined as:\n\\[\n    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n\\]\n\n\nAssume that the parameters of the neural network are as follows:\n\\[\\begin{aligned}\n& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n& b_1 = 25 & b_2 = 40 && b_3 = -30\n\\end{aligned}\\]\nWhat would be the predicted label for the following data points:\n\n\n\nx1\nx2\no\nlabel\n\n\n\n\n4\n-4\n\n\n\n\n-4\n4\n\n\n\n\n-4\n-4\n\n\n\n\n4\n4\n\n\n\n\n\nYou can use the following piece of code to evaluate the output of the network:\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef nn1(x1, x2, w1, w2, w3, w4, w5, w6, b1, b2, b3):\n    h1 = sigmoid(w1*x1 + w3*x2 + b1)\n    h2 = sigmoid(w2*x1 + w4*x2 + b2)\n    o = sigmoid(w5*h1 + w6*h2 + b3)\n    return o"
  },
  {
    "objectID": "Neural_Networks_questions.html#finding-good-parameters-for-our-network",
    "href": "Neural_Networks_questions.html#finding-good-parameters-for-our-network",
    "title": "MATH60629A",
    "section": "",
    "text": "Let’s move to a slightly more realistic example. Here we focus on the task of (binary) classification. As always, we first load the data that we want to classify:\n\nfrom utilities import load_data, plot_boundaries, plot_data # we wrote some helper functions\nX_train, y_train, X_test, y_test = load_data()          # to help with data loading\n\nYou can plot the data using the helper function plot_data:\n\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nAs you can see, this data is not linearly separable. In other words, the positive and negative examples cannot be separated using a linear classifier. Our goal for the rest of this notebook-session is to learn the parameters of a neural-network model which can separate the positives from the negative examples.\nWhat do we mean by learning the parameters? Remember that our neural network has 9 parameters including three biases (\\(w_1, \\ldots, w_6, b_1, b_2, b_3\\)). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best.\nLet’s see how different choices of parameters changes the classifier. For a given set of parameters, the function plot_boundaries shows the regions of positive prediction (coloured blue) and negative prediction (coloured red):\n\nw1 = 1; w2 = 1; w3 = 1; w4 = 1; w5 = 1; w6 = 1\nb1 = 0; b2 = 0; b3 = -1\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n\n\n\n\nNow let’s project the plot of data on these decision boundaries:\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nIt appears that the classifier obtained using the above set of parameters does not match our data. (Of course, this is to be expected. This classifier with fixed weights a priori has a high bias and a low variance.)\n\n\nTry the alternatives below and see which one is a better match for our data:\n\nw1 = -1; w2 = -1; w3 = -1; w4 = -1; w5 = 4; w6 = -3\nb1 = -4; b2 = 4; b3 = 1\n\n\nw1 = 1; w2 = -1; w3 = -1; w4 = -1; w5 = -4; w6 = 3\nb1 = 4; b2 = -4; b3 = 2\n\n\nw1 = -1; w2 = 2; w3 = 1; w4 = -2; w5 = 4; w6 = 4\nb1 = 5; b2 = 8; b3 = -6\n\nObviously, we need a better way than trial and error to find the best parameters. The way that we do this is by minimizing a loss function."
  },
  {
    "objectID": "Neural_Networks_questions.html#loss-function",
    "href": "Neural_Networks_questions.html#loss-function",
    "title": "MATH60629A",
    "section": "",
    "text": "A loss function evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the binary cross-entropy loss. Let’s represent our training data by the set \\(\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}\\) and our neural network function by \\(f\\). Then the binary cross-entropy loss function will be defined as:\n\\[\\begin{equation}\n    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n\\end{equation}\\]\nThe binary cross-entropy relates to the Bernoulli distribution (maximizing the Bernoulli likelihood is equivalent to minimizing the binary cross-entropy). It is the loss function that should be used for binary classification problems. It can be generalized to multiclass classification problems, see cross entropy.\n\n\nLet’s see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of \\(X, f(X), y\\) of those four examples are as follows:\n\n\n\nX\nf(X)\ny\n\n\n\n\n(5.4, 1.6)\n1\n1\n\n\n(1.4, -0.5)\n0.3679\n1\n\n\n(3.5, -3)\n0.8647\n0\n\n\n(-3.5, 1.1)\n0\n0\n\n\n\nCalculate the loss function using the equation above. You can calculate the log using this function:\n\nnp.log(0.5)\n\n-0.6931471805599453\n\n\nIt is important to remember that the loss function \\(l\\) is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n\\[\\begin{equation}\n    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n\\end{equation}\\]\nIn principle, we want to find the set of parameters \\(\\mathbf{w}, \\mathbf{b}\\) for which \\(\\ell(\\mathbf{w}, \\mathbf{b})\\) has the smallest value. We will use gradient descent to find these values.\n\n\n\nThe plot below shows the function \\(f(x_1, x_2) = x_1^2 + x_2^2\\):\n\n\n\n\nPoint A on the plot has coordinates \\((1, 1, 3)\\). The blue vector AB shows the direction \\((-1, -1)\\), and the green vector AC shows the direction \\((0, -1)\\). Assume that we are at initial point \\((1, 1)\\) and we want to move in a direction that minimizes the function \\(f\\). Which of these two directions moves faster towards the minimum: \\((-1, -1)\\) or \\((0, -1)\\)?\n\n\n\nCalculate the gradient of function \\(f\\) in the point \\((1, 1)\\). How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?"
  },
  {
    "objectID": "Neural_Networks_questions.html#training-the-neural-network",
    "href": "Neural_Networks_questions.html#training-the-neural-network",
    "title": "MATH60629A",
    "section": "",
    "text": "We now understand the theory of training neural networks. But how do we do this in practice? We will now develop our practical skills using the scikit-learn library to train our tiny network. Let’s first define the network:\n\nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(2,),\n                    activation='logistic',\n                    solver='lbfgs',\n                    random_state=0,\n                    max_iter=500,\n                    tol=1e-7)\n\nThe argument hidden_layer_sizes=(2,) states that we only have one hidden layer with two neurons, and the argument activation='logistic' shows that we use the sigmoid activation function (Let’s ignore the other arguments for now).\nWe will now train the network using our training data:\n\nclf.fit(X_train, y_train)\n\nMLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(2,), learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=0, shuffle=True, solver='lbfgs',\n              tol=1e-07, validation_fraction=0.1, verbose=False,\n              warm_start=False)\n\n\nOnce the network is trained, use the helper function tiny_net_parameters to get the parameters of the trained network (tiny_net_parameters is a wrapper around clf.coefs_ and clf.intercepts_):\n\nfrom utils import tiny_net_parameters\nw1, w2, w3, w4, w5, w6, b1, b2, b3 = tiny_net_parameters(clf)\n\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nThe learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data).\n\nIn addition to the decision boundaries in the original data space, we can also visualize how the data are transformed through the neural networks. Since we use a hidden layer with two neurons, we can visualize its “output” in two dimensions.\nEn plus des frontières de décisions dans l’espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions.\n(for better visibility, we changed the color of the yellow class to blue.)\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_train, y_train, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')\n\n\n\n\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_test, y_test, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')"
  },
  {
    "objectID": "Neural_Networks_questions.html#tensorflow-playground-questions",
    "href": "Neural_Networks_questions.html#tensorflow-playground-questions",
    "title": "MATH60629A",
    "section": "",
    "text": "We will now investigate a few properties of neural networks using tensorflow playground. Take a few minutes to familiarize yourself with the playground:\n\nChange the number of hidden layers to one\nChange the data distribution to exclusive OR\nPush the run button and see how the network is trained\nStop training after epoch 500 (each epoch involves doing gradient descent using the complete dataset)\nHover over the neurons in the hidden layer and see the vizualization of their outputs.\n\n\n\nOpen this example on tensorflow playground.\n\nPush the run button and see the learning process for 500 epochs. What do you observe?\nStop training and press the restart button. Change the learning rate from 3 to 0.1, and press the run button again. What is different from the previous run?\nTry these steps using three learning rates: 0.3, 0.03, and 0.003:\nPress the reset button\nChange the learning rate\nPress the step button (located at the right of run button) a few times, and observe how the training/test loss changes in each step.\n\nWhich of those three rates would you use?\n\n\n\nOpen this example on playground.\nLet’s first observe a few things about this example. Check the box titled Show test data. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting. - Press the run button and let the training proceed for 500 epochs, then pause the training. - What do you think about the decision boundary of the classifier? - What causes the difference between the training error and test error? (Check the Show test data box again) - Write down the test error\nWe will now see how we can avoid overfitting using \\(L_2\\) regularization. - Press the restart button - Change regularization from None to L2 - Change Regularization rate from 0 to 0.3 - Press the run button and run the model for 500 epochs - What is different from the previous setting? - Write down the test error\nJust like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003: - Press the restart button - Change Regularization rate - Press the run button and run the model for 500 epochs - Write down the test error\nWhich of these regularization rates would you use?"
  },
  {
    "objectID": "code/gradie2.html",
    "href": "code/gradie2.html",
    "title": "Gradient Descent with PyTorch",
    "section": "",
    "text": "In this tutorial, we will learn how to implement the Gradient Descent algorithm using PyTorch. Gradient Descent is an optimization algorithm commonly used in machine learning to find the optimal parameters of a model by iteratively updating them based on the gradient of a cost function.\nWe will start by explaining the theory behind Gradient Descent, then move on to implementing it in PyTorch."
  },
  {
    "objectID": "code/gradie2.html#theory",
    "href": "code/gradie2.html#theory",
    "title": "Gradient Descent with PyTorch",
    "section": "Theory",
    "text": "Theory\nGradient Descent works by iteratively updating the parameters of a model in the opposite direction of the gradient of a cost function. The goal is to find the minimum of the cost function, which corresponds to the optimal parameters for the model.\nThe update rule for Gradient Descent is given by:\n\\(\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)\\)\nwhere: - \\(\\theta\\) represents the current parameters of the model, - \\(\\alpha\\) is the learning rate (step size), - \\(J(\\theta)\\) is the cost function, and - \\(\\nabla J(\\theta)\\) is the gradient of the cost function with respect to \\(\\theta\\).\nThe learning rate \\(\\alpha\\) determines the size of the steps taken in each iteration. Too small of a learning rate may cause the algorithm to converge slowly, while too large of a learning rate may cause it to overshoot the minimum."
  },
  {
    "objectID": "code/gradie2.html#implementation",
    "href": "code/gradie2.html#implementation",
    "title": "Gradient Descent with PyTorch",
    "section": "Implementation",
    "text": "Implementation\nNow let’s implement Gradient Descent using PyTorch. We will start by generating some sample data and defining a linear regression model. Then, we will define the cost function and gradient descent function.\n\nStep 1: Generate Sample Data\n\nimport torch\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Generate some dummy data\nX = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\ny = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\nprint(f\"X is {X}\")\nprint(f\"y is {y}\")\n\nX is tensor([[1.],\n        [2.],\n        [3.],\n        [4.]])\ny is tensor([[2.],\n        [4.],\n        [6.],\n        [8.]])\n\n\nIn this step, we import the torch module, set the random seed for reproducibility, and generate some dummy data. Here, X represents the input features and y represents the corresponding output values.\n\n\nStep 2: Define Linear Regression Model\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super(LinearRegression, self).__init__()\n        self.linear = torch.nn.Linear(1, 1)\n        \n    def forward(self, x):\n        y_pred = self.linear(x)\n        return y_pred\n    \nmodel = LinearRegression()\nprint(f\"weight: {model.linear.weight.data}\")\nprint(f\"bias: {model.linear.bias.data}\")\n\nweight: tensor([[-0.0075]])\nbias: tensor([0.5364])\n\n\nIn this step, we define a linear regression model using PyTorch’s torch.nn.Module class. The model consists of a single linear layer self.linear, which maps the input features to the output values. The forward method is used to define the forward pass of the model.\n\n\nStep 3: Define Cost Function\n\ncriterion = torch.nn.MSELoss()\n\nIn this step, we define the Mean Squared Error (MSE) loss function using PyTorch’s torch.nn.MSELoss class. The loss function measures the difference between the predicted values and the actual values.\n\n\nStep 4: Define Gradient Descent Function\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\ndef gradient_descent(X, y, model, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        # Forward pass\n        y_pred = model(X)\n        \n        # Compute loss\n        loss = criterion(y_pred, y)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch+1) % 200 == 0:\n            print(f'Epoch: {epoch+1}, Loss: {loss.item():.3f}, Weight: {model.linear.weight.data.item():.3f}, bias: {model.linear.bias.data.item():.3f}')\n\nIn this step, we define the gradient descent function gradient_descent. The function takes the input features X, target values y, model, criterion (loss function), optimizer, and the number of epochs as input.\nIn each epoch, we perform the following steps: 1. Compute the forward pass of the model to obtain the predicted values. 2. Compute the loss between the predicted values and the actual values. 3. Perform backpropagation to compute the gradients. 4. Update the parameters of the model using the optimizer.\nWe also print the loss value every 10 epochs to track the progress of the algorithm.\n\n\nStep 5: Run Gradient Descent\n\nnum_epochs = 2000\ngradient_descent(X, y, model, criterion, optimizer, num_epochs)\n\nEpoch: 200, Loss: 0.060, Weight: 1.796, bias: 0.600\nEpoch: 400, Loss: 0.018, Weight: 1.888, bias: 0.329\nEpoch: 600, Loss: 0.005, Weight: 1.939, bias: 0.181\nEpoch: 800, Loss: 0.002, Weight: 1.966, bias: 0.099\nEpoch: 1000, Loss: 0.000, Weight: 1.981, bias: 0.054\nEpoch: 1200, Loss: 0.000, Weight: 1.990, bias: 0.030\nEpoch: 1400, Loss: 0.000, Weight: 1.994, bias: 0.016\nEpoch: 1600, Loss: 0.000, Weight: 1.997, bias: 0.009\nEpoch: 1800, Loss: 0.000, Weight: 1.998, bias: 0.005\nEpoch: 2000, Loss: 0.000, Weight: 1.999, bias: 0.003\n\n\nFinally, we run the gradient descent function with the specified number of epochs. The function will update the parameters of the model in each epoch to minimize the loss."
  },
  {
    "objectID": "code/gradie2.html#conclusion",
    "href": "code/gradie2.html#conclusion",
    "title": "Gradient Descent with PyTorch",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we learned how to implement the Gradient Descent algorithm using PyTorch. Gradient Descent is a fundamental optimization algorithm used to find the optimal parameters of a model. By iteratively updating the parameters in the direction of the negative gradient, we can find the minimum of a cost function."
  },
  {
    "objectID": "code/pytorch_intro.html",
    "href": "code/pytorch_intro.html",
    "title": "Tensors and Arrays",
    "section": "",
    "text": "Sure! PyTorch is a popular open-source machine learning library for Python. It provides an efficient and flexible framework for building deep learning models. Many beginners in PyTorch are already familiar with Numpy, which is another Python library used for numerical computing. In this tutorial, we will introduce PyTorch using the similarities between PyTorch and Numpy.\nTo begin, let’s start by installing PyTorch using the following command:\n!pip install torch\nAfter installing PyTorch, we can import it into our Python code using the following line:\nimport torch\nNow, let’s draw some comparisons between PyTorch and Numpy.\nPyTorch uses tensors to store and manipulate data, while Numpy uses arrays. Tensors and arrays behave similarly and share many common operations. For example, creating a tensor/array, accessing elements, and performing basic operations are done in a similar way.\nLet’s compare how we can create a tensor and an array in PyTorch and Numpy respectively:\n# Creating a PyTorch tensor\ntensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(tensor)\n\n# Creating a Numpy array\nimport numpy as np\narray = np.array([[1, 2, 3], [4, 5, 6]])\nprint(array)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n[[1 2 3]\n [4 5 6]]\nBoth the PyTorch tensor and Numpy array store data in a multi-dimensional structure."
  },
  {
    "objectID": "code/pytorch_intro.html#accessing-elements",
    "href": "code/pytorch_intro.html#accessing-elements",
    "title": "Tensors and Arrays",
    "section": "Accessing Elements",
    "text": "Accessing Elements\nAccessing elements in PyTorch tensors and Numpy arrays is similar, as both use indexing and slicing methods.\n\n# Accessing elements in PyTorch tensor\nprint(tensor[1, 2])  # Output: 6\n\n# Accessing elements in Numpy array\nprint(array[1, 2])  # Output: 6\n\ntensor(6)\n6"
  },
  {
    "objectID": "code/pytorch_intro.html#basic-operations",
    "href": "code/pytorch_intro.html#basic-operations",
    "title": "Tensors and Arrays",
    "section": "Basic Operations",
    "text": "Basic Operations\nBasic operations such as addition, subtraction, multiplication, and division can be easily performed on PyTorch tensors and Numpy arrays.\n\n# Perform addition on PyTorch tensor\ntensor_sum = tensor + tensor\nprint(tensor_sum)\n\n# Perform addition on Numpy array\narray_sum = array + array\nprint(array_sum)\n\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12]])\n[[ 2  4  6]\n [ 8 10 12]]\n\n\nBoth the tensor_sum and array_sum will output the element-wise sum of their respective objects."
  },
  {
    "objectID": "code/pytorch_intro.html#shape-and-reshaping",
    "href": "code/pytorch_intro.html#shape-and-reshaping",
    "title": "Tensors and Arrays",
    "section": "Shape and Reshaping",
    "text": "Shape and Reshaping\nThe shape and reshaping of tensors and arrays are crucial operations in both libraries. Here’s how we can determine the shape of a tensor/array and reshape it:\n\n# Shape of PyTorch tensor\nprint(tensor.shape)  # Output: torch.Size([2, 3])\n\n# Shape of Numpy array\nprint(array.shape)  # Output: (2, 3)\n\n# Reshaping PyTorch tensor\ntensor_reshaped = tensor.view(3, 2)\nprint(tensor_reshaped)\n\n# Reshaping Numpy array\narray_reshaped = array.reshape(3, 2)\nprint(array_reshaped)\n\ntorch.Size([2, 3])\n(2, 3)\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n[[1 2]\n [3 4]\n [5 6]]"
  },
  {
    "objectID": "code/pytorch_intro.html#automatic-differentiation",
    "href": "code/pytorch_intro.html#automatic-differentiation",
    "title": "Tensors and Arrays",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nOne of the key advantages of PyTorch over Numpy is its ability to perform automatic differentiation. It makes training deep learning models much easier by computing gradients automatically.\n\n# Enable automatic differentiation in PyTorch\ntensor = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n\n# Perform a forward operation\noutput = tensor.sum()\n\n# Perform automatic differentiation\noutput.backward()\n\n# Access the gradients\nprint(tensor.grad)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\nThe requires_grad=True argument enables the computation of gradients for the tensor. The output.backward() function computes the gradients of output with respect to all tensors that have requires_grad=True. Finally, tensor.grad retrieves the gradients.\nThese are some of the similarities between PyTorch and Numpy. Understanding these similarities can make it easier for beginners to transition from Numpy to PyTorch."
  },
  {
    "objectID": "code/MLP_pytorch.html",
    "href": "code/MLP_pytorch.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To demonstrate the implementation of Multilayer Perceptron (MLP) using Pytorch, we will use the classic Iris dataset. MLP is a type of artificial neural network that is widely used in machine learning for classification tasks. In this tutorial, we will build a simple MLP model to classify the Iris flowers into different species.\nBut first, let’s import the necessary libraries:\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nNow let’s load the Iris dataset and split it into training and testing sets:\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nIn the code above, we first import the required libraries: torch for building and training the MLP model, torch.nn for defining the network architecture, torch.optim for optimizing the model parameters, sklearn.datasets for loading the Iris dataset, and sklearn.model_selection for splitting the dataset into training and testing sets.\nNext, we load the Iris dataset using load_iris() function and assign the feature data to X and the target labels to y. Then, we split the dataset into training and testing sets using train_test_split() function, where we specify the test size as 0.2 (20% of the data) and set the random state for reproducibility.\nNow, let’s define our MLP model:\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\nIn the code above, we define a class MLP inheriting from nn.Module. The constructor (__init__()) takes three arguments: input_size which is the number of input features, hidden_size which is the number of neurons in the hidden layer, and num_classes which is the number of output classes. Inside the constructor, we define the layers of our MLP model using nn.Linear (fully connected) and nn.ReLU (rectified linear unit) activation function.\nThe forward() method defines the forward pass of the model. It takes an input x, passes it through the layers, and returns the output.\nNow let’s create an instance of our MLP model:\n\ninput_size = X.shape[1]\nhidden_size = 64\nnum_classes = len(set(y))\n\nmodel = MLP(input_size, hidden_size, num_classes)\n\nIn the code above, we set input_size as the number of features in the input data (X.shape[1]), hidden_size as 64 (you can change this value to experiment), and num_classes as the number of unique classes in the target labels (len(set(y))).\nNow let’s define the loss function and optimizer:\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nWe use nn.CrossEntropyLoss() as our loss function, which is suitable for multi-class classification problems. We also define the optimizer using optim.Adam and pass the model parameters (model.parameters()) along with the learning rate (lr=0.001).\nNext, let’s train the MLP model:\n\nnum_epochs = 100\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    for i in range(0, X_train.shape[0], batch_size):\n        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n        batch_y = torch.LongTensor(y_train[i:i+batch_size])\n        \n        # Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nEpoch [10/100], Loss: 0.6263\nEpoch [20/100], Loss: 0.4618\nEpoch [30/100], Loss: 0.3765\nEpoch [40/100], Loss: 0.3074\nEpoch [50/100], Loss: 0.2480\nEpoch [60/100], Loss: 0.1996\nEpoch [70/100], Loss: 0.1622\nEpoch [80/100], Loss: 0.1339\nEpoch [90/100], Loss: 0.1126\nEpoch [100/100], Loss: 0.0964\n\n\nIn the code above, we define the number of epochs (num_epochs) and the batch size (batch_size) for training. We iterate over each epoch and split the training data into batches. For each batch, we convert the numpy arrays to tensors using torch.FloatTensor and torch.LongTensor for the input features and target labels respectively.\nIn the forward pass, we pass the input batch (batch_X) through the model and compute the predicted outputs. We then calculate the loss between the predicted outputs and the target labels using the defined loss function.\nIn the backward pass, we zero the gradients with optimizer.zero_grad(), compute the gradients of the loss with respect to the model parameters using loss.backward(), and update the model parameters using the optimizer with optimizer.step().\nFinally, we print the loss value every 10 epochs for tracking the training progress.\nAfter training, let’s evaluate the MLP model on the test set:\n\nwith torch.no_grad():\n    outputs = model(torch.FloatTensor(X_test))\n    _, predicted = torch.max(outputs.data, 1)\n\naccuracy = (predicted == torch.LongTensor(y_test)).sum().item() / len(y_test)\nprint(f'Accuracy: {accuracy:.4f}')\n\nAccuracy: 0.9667\n\n\nIn the code above, we use torch.no_grad() to turn off gradient calculation and save memory. We pass the test features (X_test) through the trained model and obtain the predicted outputs. We then use torch.max() to get the class labels with the highest probability and compare them with the ground truth labels (y_test) to calculate the accuracy.\nThat’s it! You have successfully implemented an MLP model using Pytorch for classification on the Iris dataset."
  },
  {
    "objectID": "code/RNN_timeseries.html",
    "href": "code/RNN_timeseries.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "Sure! In this tutorial, I will explain how to use a Recurrent Neural Network (RNN) to predict time series data using the PyTorch library.\nFirst, let’s start by installing the required packages. We need to install PyTorch and matplotlib for visualization.\n!pip install torch\n!pip install matplotlib\nNow that we have the necessary packages installed, let’s import them into our code.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe next step is to generate some synthetic time series data for training our model. We will generate a sine wave with some added noise.\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Generate time points\nnum_points = 1000\nt = torch.arange(0, num_points)\n\n# Generate sine wave\nvalue = torch.sin(0.1*t)\n\n# Add noise\nvalue += 0.2*torch.randn(num_points)\n\nLet’s visualize the generated time series using matplotlib.\n\nplt.figure(figsize=(10, 6))\nplt.plot(t, value)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Time Series Data')\nplt.show()\n\n\n\n\nNow, we need to prepare our data for training the RNN. We will split the data into input sequences and corresponding output sequences. Each input sequence will contain a certain number of time steps (e.g., 10), and the corresponding output sequence will be the next time step.\n\n# Define number of time steps for input sequence\nnum_steps = 10\n\n# Split data into input and output sequences\nx = []\ny = []\nfor i in range(len(value) - num_steps):\n    # creating list of x; each item has num_steps values\n    x.append(value[i:i+num_steps])\n    # creating list of y; each item has the next value\n    y.append(value[i+num_steps])\n\n# Convert data to PyTorch tensors\n# turning list x to tensor & adding new dimension (input size) at the end\nx = torch.stack(x)[..., None]\n# turning list y to tensor & adding new dimension at the end\ny = torch.stack(y)[..., None]\n\nx_train = x[0:100]\nx_test = x[100:]\n\ny_train = y[0:100]\ny_test = y[100:]\n\nprint(x_train.shape)\nprint(y_train.shape)\n\ntorch.Size([100, 10, 1])\ntorch.Size([100, 1])\n\n\nNext, we need to define our RNN model using the nn.RNN module from PyTorch.\n\n# Define RNN model\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        # If batch_first=True, then the input and output tensors are provided as (batch, seq, feature)\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        # nn.Linear to transform hidden state from hidden_size to output_size\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def init_h(self, x):\n        '''\n        To initialize the hidden state for the first iteration\n        shape: d x num_layers, N, H if input is batched (our case)\n        shape: d x num_layers, H if input not batched\n        d: 2 if bidirectional else 1\n        N: batch size\n        H: hidden_size\n        num_layers: number of RNN layers\n        '''\n        batch_size = x.shape[0]\n        self.h0 = torch.zeros(1, batch_size, self.hidden_size, requires_grad=True)\n        return self.h0\n\n    def forward(self, x, hidden):\n        # shape: N, L, DxH \n        # shape: 990, 10, 16\n        out, hidden = self.rnn(x, hidden)\n        # choose the last output of out: out[:, -1, :]\n        # pass it to the fc layer\n        out = self.fc(out[:, -1, :])\n        return out, hidden\n\n# Create an instance of the RNN model\ninput_size = 1\nhidden_size = 16\noutput_size = 1\nmodel = RNN(input_size, hidden_size, output_size)\n\nNote that the self.fc = nn.Linear(hidden_size, output_size) transforms the hidden state from hidden_size to output_size. This is a useful use case of fully connected layers, which helps us transform our feature space from \\(n\\) dimensions to \\(n\\prime\\) dimensions.\nBefore training our model, we need to define the loss function and optimizer.\n\n# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nNow, we can train our RNN model using a loop. We will iterate over the data for a certain number of epochs and update the model parameters based on the computed loss.\n\n# Set number of epochs\nnum_epochs = 100\n# initialize the hidden state\nh = model.init_h(x_train)\n# Train the model\nfor epoch in range(num_epochs):\n    # Forward pass\n    output, h = model(x_train, h)\n    loss = criterion(output, y_train)\n    \n    # Backward and optimize\n    loss.backward()\n    optimizer.step()\n    # set the gradients of the model tensors to zero\n    optimizer.zero_grad()\n    # detach h from the computation graph\n    h.detach_()\n    \n    # Print progress\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nEpoch [10/100], Loss: 0.1675\nEpoch [20/100], Loss: 0.0974\nEpoch [30/100], Loss: 0.0782\nEpoch [40/100], Loss: 0.0725\nEpoch [50/100], Loss: 0.0709\nEpoch [60/100], Loss: 0.0692\nEpoch [70/100], Loss: 0.0683\nEpoch [80/100], Loss: 0.0675\nEpoch [90/100], Loss: 0.0666\nEpoch [100/100], Loss: 0.0657\n\n\nFinally, let’s visualize the predicted values and compare them with the ground truth.\n\nh = model.init_h(x_test)\n# Generate predictions\nwith torch.no_grad():\n    y_pred, h = model(x_test, h)\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(t, value, label='Ground Truth')\nplt.plot(t[10:110], output.detach().numpy(), label='Predicted train')\nplt.plot(t[110:], y_pred.detach().numpy(), label='Predicted test')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Time Series Prediction')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "code/backpropagation_pytorch.html",
    "href": "code/backpropagation_pytorch.html",
    "title": "Backpropagation with PyTorch",
    "section": "",
    "text": "In this tutorial, we will learn about backpropagation and how to implement it using PyTorch. Backpropagation is a key algorithm for training neural networks. It allows us to compute gradients of the loss function with respect to the network’s parameters, which in turn allows us to update the parameters and improve the model’s performance."
  },
  {
    "objectID": "code/backpropagation_pytorch.html#what-is-backpropagation",
    "href": "code/backpropagation_pytorch.html#what-is-backpropagation",
    "title": "Backpropagation with PyTorch",
    "section": "What is Backpropagation?",
    "text": "What is Backpropagation?\nBackpropagation is a technique for training neural networks by computing the gradient of the loss function with respect to the parameters of the network. It is based on the chain rule of calculus, which allows us to compute the gradient of a function composed of multiple nested functions.\nLet’s denote a neural network with parameters \\(\\theta\\), and a loss function \\(J(\\theta)\\). The backpropagation algorithm computes the gradient \\(\\nabla J(\\theta)\\) with respect to the parameters by applying the chain rule, as follows:\n\\[\n\\nabla J(\\theta) = \\frac{{\\partial J(\\theta)}}{{\\partial \\theta}} = \\frac{{\\partial J(\\theta)}}{{\\partial \\hat{y}}} \\cdot \\frac{{\\partial \\hat{y}}}{{\\partial h}} \\cdot \\frac{{\\partial h}}{{\\partial \\theta}}\n\\]\nwhere \\(\\hat{y}\\) is the predicted output of the neural network, \\(h\\) is the pre-activation (weighted sum) of the final layer, and \\(\\theta\\) represents the weights and biases of all the layers.\nTo update the parameters of the network, we typically use an optimization algorithm such as Stochastic Gradient Descent (SGD) or Adam, which leverages the computed gradients to perform parameter updates in the direction of minimizing the loss."
  },
  {
    "objectID": "code/backpropagation_pytorch.html#implementing-backpropagation-with-pytorch",
    "href": "code/backpropagation_pytorch.html#implementing-backpropagation-with-pytorch",
    "title": "Backpropagation with PyTorch",
    "section": "Implementing Backpropagation with PyTorch",
    "text": "Implementing Backpropagation with PyTorch\nPyTorch is a popular deep learning library that provides an automatic differentiation engine, which can compute gradients of any function with respect to its input. This makes implementing backpropagation much easier.\nHere are the steps we will follow to implement backpropagation with PyTorch:\n\nDefine the neural network structure.\nDefine the loss function.\nCreate an optimizer object.\nWrite the training loop.\n\nLet’s dive into the code implementation."
  },
  {
    "objectID": "code/backpropagation_pytorch.html#step-1-define-the-neural-network-structure",
    "href": "code/backpropagation_pytorch.html#step-1-define-the-neural-network-structure",
    "title": "Backpropagation with PyTorch",
    "section": "Step 1: Define the Neural Network Structure",
    "text": "Step 1: Define the Neural Network Structure\nFirst, we need to define the structure of our neural network using the torch.nn module in PyTorch. We will create a simple feedforward neural network with two hidden layers.\n\nimport torch\nimport torch.nn as nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        return out\n\nIn this code, we define a class NeuralNetwork which inherits from nn.Module. This allows us to use all of PyTorch’s built-in capabilities for neural networks.\nWe define three fully connected layers (nn.Linear) with ReLU activation between them. ReLU is a commonly used activation function in neural networks.\nThe forward method defines the forward pass, which computes the output of the neural network given an input x."
  },
  {
    "objectID": "code/backpropagation_pytorch.html#step-2-define-the-loss-function",
    "href": "code/backpropagation_pytorch.html#step-2-define-the-loss-function",
    "title": "Backpropagation with PyTorch",
    "section": "Step 2: Define the Loss Function",
    "text": "Step 2: Define the Loss Function\nNext, we need to define the loss function that we will use to measure how well our neural network is performing. In this example, we will use the Mean Squared Error (MSE) loss.\n\ncriterion = nn.MSELoss()"
  },
  {
    "objectID": "code/backpropagation_pytorch.html#step-3-create-an-optimizer-object",
    "href": "code/backpropagation_pytorch.html#step-3-create-an-optimizer-object",
    "title": "Backpropagation with PyTorch",
    "section": "Step 3: Create an Optimizer Object",
    "text": "Step 3: Create an Optimizer Object\nTo update the parameters of the neural network during training, we need to use an optimizer. PyTorch provides various optimizers, such as SGD, Adam, and RMSprop.\nHere, we will use the Adam optimizer.\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
  },
  {
    "objectID": "code/backpropagation_pytorch.html#step-4-write-the-training-loop",
    "href": "code/backpropagation_pytorch.html#step-4-write-the-training-loop",
    "title": "Backpropagation with PyTorch",
    "section": "Step 4: Write the Training Loop",
    "text": "Step 4: Write the Training Loop\nFinally, we can write the training loop. The training loop consists of the following steps:\n\nCompute the predicted output of the neural network.\nCompute the loss between the predicted output and the target output.\nCompute the gradients of the loss with respect to the network’s parameters using the backward method.\nUpdate the parameters of the network using the optimizer’s step method.\nReset the gradients to zero.\n\n\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute loss\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nIn this code snippet, inputs and targets represent the input and target output for each training example, respectively. We compute the predicted outputs using model(inputs), and then compute the loss using criterion(outputs, targets).\nWe zero out the gradients using optimizer.zero_grad() to avoid accumulating gradients from previous iterations. Then, we call loss.backward() to compute the gradients using backpropagation.\nFinally, we update the parameters of the network using optimizer.step() and repeat this process for each epoch.\nThat’s it! You have now implemented backpropagation using PyTorch. By running this code and providing appropriate inputs and targets, you can train your neural network on your specific problem.\nRemember to preprocess your data, normalize it if necessary, split it into training and validation sets, and track the training and validation loss to monitor the progress of your neural network.\nI hope this tutorial was helpful in understanding backpropagation and its implementation with PyTorch. Happy learning!"
  },
  {
    "objectID": "code/datasets_pytorch.html",
    "href": "code/datasets_pytorch.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "In PyTorch, the torch.utils.data.Dataset class is used to represent a dataset. It provides an interface to access individual samples in the dataset, which can be used to train and evaluate machine learning models. This class is an abstract class, so we need to create a custom class that inherits from it and implements two methods: __len__ and __getitem__.\nTo create a custom randomly generated dataset, we need to define a class that generates random samples and labels. Let’s assume that we want to generate random 2D points along with their corresponding labels. The labels can be either 0 or 1, indicating two different classes.\nHere’s an example implementation of the custom dataset class:\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass RandomDataset(Dataset):\n    def __init__(self, num_samples, num_features):\n        self.num_samples = num_samples\n        self.num_features = num_features\n        self.data = torch.randn(num_samples, num_features)\n        # Randomly assign labels of 0 or 1\n        self.targets = torch.randint(0, 2, (num_samples,))\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, index):\n        sample = self.data[index]\n        label = self.targets[index]\n        return sample, label\n\nLet’s break down the code step by step:\n\nImport the necessary modules: torch and torch.utils.data.Dataset.\nDefine a custom class called RandomDataset that inherits from Dataset.\nIn the class constructor (__init__ method), we pass the number of samples (num_samples) and the number of features (num_features) as parameters.\nInside the constructor, we generate random data using torch.randn with dimensions (num_samples, num_features). This will create a tensor of random floating-point values.\nWe also generate random labels using torch.randint with values between 0 and 1 (exclusive), and shape (num_samples,). This will create a tensor with integer labels.\nImplement the __len__ method, which returns the number of samples in the dataset (num_samples).\nImplement the __getitem__ method, which takes an index as input and returns the corresponding sample and label. This method is used to access individual samples in the dataset. Inside this method, we use the index to retrieve the sample and label from the data and targets tensors, respectively.\n\nNow, we can create an instance of the RandomDataset class and use it as a regular dataset in PyTorch.\n\n# Instantiate the custom dataset\ndataset = RandomDataset(num_samples=1000, num_features=2)\n\n# Access an individual sample and its label\nsample, label = dataset[0]\nprint(\"Sample:\", sample)\nprint(\"Label:\", label)\n\nSample: tensor([ 0.9420, -0.5163])\nLabel: tensor(1)\n\n\nThis will output the first sample and its corresponding label from the dataset. You can access any sample by using the index in the same way.\nBy defining a custom dataset class, we can generate random datasets with specific characteristics and use them for training and evaluating machine learning models in PyTorch."
  },
  {
    "objectID": "code/dataloader_pytorch.html",
    "href": "code/dataloader_pytorch.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To work with custom datasets in PyTorch, we need to create a custom dataset class and then use a data loader to load the data in mini-batches during training or evaluation. In this tutorial, we will create a custom randomly generated dataset and load it using a data loader.\nFirst, let’s install the necessary packages and import the required libraries:\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nNext, we will create a custom dataset class by subclassing the Dataset class provided by PyTorch. In this example, let’s create a dataset of randomly generated 2D points with corresponding labels:\n\nclass CustomDataset(Dataset):\n    def __init__(self, num_samples):\n        self.num_samples = num_samples\n        # data is from 0 to num_samples-1\n        self.data = torch.arange(num_samples)\n        self.labels = (self.data%2==0).long()\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n        return sample, label\n\nIn the __init__ method of our CustomDataset class, we initialize the number of samples and number of features. We then generate random data points and labels using the torch.randn and torch.randint functions, respectively.\nThe __len__ method is used to return the total number of samples in the dataset.\nThe __getitem__ method is used to access a particular sample and its label given an index. We retrieve the corresponding data point and label from the pre-generated data and labels tensors, and return them as a tuple.\nNow, let’s create an instance of our custom dataset:\n\ndataset = CustomDataset(num_samples=20)\nprint(dataset[0])\nprint(dataset[1])\n\n(tensor(0), tensor(1))\n(tensor(1), tensor(0))\n\n\nLet’s visualize the dataset.\n\nimport matplotlib.pyplot as plt\nplt.scatter(dataset.data, dataset.labels)\nplt.xlabel('inputs')\nplt.ylabel('labels')\n\nText(0, 0.5, 'labels')\n\n\n\n\n\nWe have created a dataset with 20 samples.\nFinally, we can create a data loader to load our custom dataset in mini-batches:\n\ndataloader = DataLoader(dataset, batch_size=8, shuffle=False, drop_last=False)\n\nThe DataLoader class takes our custom dataset as input along with the desired batch size.\n\nfor batch in dataloader:\n    inputs, labels = batch\n    print(f'inputs: {inputs}')\n    print(f'labels: {labels}\\n')\n\ninputs: tensor([0, 1, 2, 3, 4, 5, 6, 7])\nlabels: tensor([1, 0, 1, 0, 1, 0, 1, 0])\n\ninputs: tensor([ 8,  9, 10, 11, 12, 13, 14, 15])\nlabels: tensor([1, 0, 1, 0, 1, 0, 1, 0])\n\ninputs: tensor([16, 17, 18, 19])\nlabels: tensor([1, 0, 1, 0])\n\n\n\nIn each iteration, the data loader returns a batch of samples, where inputs contains the features of the samples and labels contains their corresponding labels. We can now perform any required operations on the mini-batch, such as passing it through a model for training or evaluation.\nYou can see that by default, the Dataloader class returns the dataset in mini-batches without changing the order of the data.\nWe can set the shuffle parameter to True if we want to shuffle the data before each epoch. Suffling the training data can prevent the model from relying on the order of the data points to predict the labels.\n\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=False)\nfor batch in dataloader:\n    inputs, labels = batch\n    print(f'inputs: {inputs}')\n    print(f'labels: {labels}\\n')\n\ninputs: tensor([15,  1,  5, 10,  2,  3,  8, 16])\nlabels: tensor([0, 0, 0, 1, 1, 0, 1, 1])\n\ninputs: tensor([13,  7,  4, 18, 12,  6, 19,  0])\nlabels: tensor([0, 0, 1, 1, 1, 1, 0, 1])\n\ninputs: tensor([17, 14,  9, 11])\nlabels: tensor([0, 1, 0, 0])\n\n\n\nFor specific models that rely on the batch_size to construct their parameters (see RNNs), we need to set drop_last to True to prevent batches having different shapes.\n\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)\nfor batch in dataloader:\n    inputs, labels = batch\n    print(f'inputs: {inputs}')\n    print(f'labels: {labels}\\n')\n\ninputs: tensor([ 0, 19, 15, 16, 11, 17, 12,  6])\nlabels: tensor([1, 0, 0, 1, 0, 0, 1, 1])\n\ninputs: tensor([14,  2,  9,  5,  4, 18,  7,  1])\nlabels: tensor([1, 1, 0, 0, 1, 1, 0, 0])\n\n\n\nYou can see that by setting drop_last=True, the final mini-batch of size \\(4\\) is dropped.\nThat’s it! You have learned how to create a custom dataset and load it using a data loader in PyTorch."
  },
  {
    "objectID": "Introduction_to_PyTorch.html",
    "href": "Introduction_to_PyTorch.html",
    "title": "Linear Regression and Gradient Descent",
    "section": "",
    "text": "!git clone \"https://github.com/davoodwadi/davoodwadi.github.io.git\"\n\nCloning into 'davoodwadi.github.io'...\nremote: Enumerating objects: 755, done.\nremote: Counting objects: 100% (19/19), done.\nremote: Compressing objects: 100% (16/16), done.\nremote: Total 755 (delta 4), reused 5 (delta 1), pack-reused 736\nReceiving objects: 100% (755/755), 89.73 MiB | 32.12 MiB/s, done.\nResolving deltas: 100% (432/432), done.\nUpdating files: 100% (239/239), done.\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np, matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor, nn\ntorch.manual_seed(42)\n\nmpl.rcParams['image.cmap'] = 'gray'\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\nX = torch.randn(100, 1)\ny = X * 4 + torch.randn(100, 1)\nX.shape, y.shape\n\n(torch.Size([100, 1]), torch.Size([100, 1]))\nplt.scatter(X,y)\n\n&lt;matplotlib.collections.PathCollection at 0x7bfff9287970&gt;\nWs = torch.linspace(0,8, 50)\nWs\n\ntensor([0.00, 0.16, 0.33, 0.49, 0.65, 0.82, 0.98, 1.14, 1.31, 1.47, 1.63, 1.80, 1.96, 2.12, 2.29, 2.45, 2.61, 2.78, 2.94,\n        3.10, 3.27, 3.43, 3.59, 3.76, 3.92, 4.08, 4.24, 4.41, 4.57, 4.73, 4.90, 5.06, 5.22, 5.39, 5.55, 5.71, 5.88, 6.04,\n        6.20, 6.37, 6.53, 6.69, 6.86, 7.02, 7.18, 7.35, 7.51, 7.67, 7.84, 8.00])\nX.shape\n\ntorch.Size([100, 1])\nlosses = []\nfor w in Ws:\n  y_pred = X*w\n  loss = (y-y_pred).pow(2).mean() #mse\n  losses.append(loss.item())\nplt.plot(Ws, losses)\nw = tensor([[1.]], requires_grad=True)\nw\n\ntensor([[1.]], requires_grad=True)\ny_hat = X @ w\nloss_fn = nn.MSELoss()\nloss = loss_fn(y_hat, y)\nloss\n\ntensor(9.56, grad_fn=&lt;MseLossBackward0&gt;)\nloss.backward()\nw.grad\n\ntensor([[-5.83]])\ngradient descent step\nlr = 0.01\nwith torch.no_grad():\n  w = w - w.grad * lr\nw\n\ntensor([[1.06]])\nIn a loop"
  },
  {
    "objectID": "Introduction_to_PyTorch.html#crossentropy-loss",
    "href": "Introduction_to_PyTorch.html#crossentropy-loss",
    "title": "Linear Regression and Gradient Descent",
    "section": "CrossEntropy loss",
    "text": "CrossEntropy loss\n\nce_loss = torch.nn.CrossEntropyLoss()\ninp = tensor([[.9,0.1]])\nlogInp = inp.log()\nprint(logInp)\ntar = tensor([1])\n\nce_loss(logInp, tar).item()\n\ntensor([[-0.11, -2.30]])\n\n\n2.3025851249694824\n\n\n\np = tensor(0.1)\n(-1) * (tar * p.log() + (1-tar) * (1-p).log())\n\ntensor([2.30])"
  },
  {
    "objectID": "Introduction_to_PyTorch.html#classes",
    "href": "Introduction_to_PyTorch.html#classes",
    "title": "Linear Regression and Gradient Descent",
    "section": "10 classes",
    "text": "10 classes\n\nce_loss = torch.nn.CrossEntropyLoss()\ninp = tensor([[.5,0.1, 0.1, 0.1, 0.1, 0., 0., 0., 0., 0.1 ]])\nlogInp = inp.log()\nprint(logInp)\ntar = tensor([1])\nprint(inp.shape, tar.shape)\nce_loss(logInp, tar).item()\n\ntensor([[-0.69, -2.30, -2.30, -2.30, -2.30,  -inf,  -inf,  -inf,  -inf, -2.30]])\ntorch.Size([1, 10]) torch.Size([1])\n\n\n2.3025851249694824\n\n\n\np = tensor(0.1)\n(-1) * (tar * p.log())\n\ntensor([2.30])"
  },
  {
    "objectID": "code/RNN_LM.html",
    "href": "code/RNN_LM.html",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "",
    "text": "In this tutorial, we will explore how to use Recurrent Neural Networks (RNNs) for language modeling using PyTorch. Language modeling is the task of predicting the probability of a sequence of words occurring in a given language. RNNs are particularly well-suited for this task, as they can capture the sequential nature of language."
  },
  {
    "objectID": "code/RNN_LM.html#setting-up-the-environment",
    "href": "code/RNN_LM.html#setting-up-the-environment",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "1. Setting up the Environment",
    "text": "1. Setting up the Environment\nBefore we begin, make sure you have PyTorch installed. You can install PyTorch by following the instructions on the official website (pytorch.org).\nNext, let’s import the required libraries and set the random seed for reproducibility.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport random\n\n# Set the random seed for reproducibility\nrandom.seed(42)"
  },
  {
    "objectID": "code/RNN_LM.html#preparing-the-dataset",
    "href": "code/RNN_LM.html#preparing-the-dataset",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "2. Preparing the Dataset",
    "text": "2. Preparing the Dataset\nTo train our language model, we need a dataset consisting of sentences or sequences of words. In this tutorial, we will use a simplified version of the WikiText-2. Created by Merity et al. at 2016, the WikiText-103 & 2 Dataset contains word and character level tokens extracted from Wikipedia, in English language. Containing 100M+ in TOKENS file format.\n\npath = '../wikitext-2/'\ntrain_path = path + 'wiki.train.tokens'\nvalid_path = path + 'wiki.valid.tokens'\ntest_path = path + 'wiki.test.tokens'\n\ndef read_file(path):\n    # Open the file in read mode\n    with open(file_path, 'r') as file:\n        # Read the contents of the file\n        file_contents = file.read()\n    return file_contents\n\ntrain_string = read_file(train_path)\nvalid_string = read_file(valid_path)\ntest_string = read_file(test_path)\nprint(train_string[:100])\n\nNow, we need to preprocess our dataset. We will take care of tokenization, converting words to numerical indices, and creating batches of sequences for training."
  },
  {
    "objectID": "code/RNN_LM.html#tokenization",
    "href": "code/RNN_LM.html#tokenization",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "Tokenization",
    "text": "Tokenization\nWe first need to turn our very large string into a list of strings.\n\ntrain_list = train_string.split(\"\\n\")\nvalid_list = valid_string.split(\"\\n\")\ntest_list = test_string.split(\"\\n\")\ntrain_list[:5]\n\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp('''\"Let's go to N.Y.!\"''')\n\nfor token in doc:\n    print(token.text)\n\nnlp.vocab.strings['Let']"
  },
  {
    "objectID": "code/RNN_LM.html#defining-the-rnn-language-model",
    "href": "code/RNN_LM.html#defining-the-rnn-language-model",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "3. Defining the RNN Language Model",
    "text": "3. Defining the RNN Language Model\nNext, we define our language model using an RNN. Here, we will use a simple LSTM (Long Short-Term Memory) architecture. The LSTM cell has been shown to be particularly effective in capturing long-range dependencies in sequential data.\n\nclass RNNLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(RNNLanguageModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.lstm(embedded)\n        output = self.fc(output.reshape(-1, output.size(2)))\n        return output\n\nIn the __init__ method, we define the components of our model: - embedding: an embedding layer to convert word indices to dense word representations. - lstm: an LSTM layer to process the word embeddings, capturing the sequential dependencies. - fc: a fully connected linear layer to map the LSTM output to the vocabulary size, predicting the next word in the sequence.\nIn the forward method, we pass the input sequence through the layers. The output of the LSTM layer is reshaped and passed through the fully connected layer to obtain the predicted probabilities for each word in the vocabulary."
  },
  {
    "objectID": "code/RNN_LM.html#training-the-rnn-language-model",
    "href": "code/RNN_LM.html#training-the-rnn-language-model",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "4. Training the RNN Language Model",
    "text": "4. Training the RNN Language Model\nNow, let’s define a function to train our RNN Language Model.\n\ndef train_model(model, train_iter, val_iter, num_epochs, lr):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        \n        for batch in train_iter:\n            optimizer.zero_grad()\n            \n            x = batch.text[:, :-1]\n            y = batch.text[:, 1:].flatten()\n            \n            output = model(x)\n            loss = criterion(output, y)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        train_loss /= len(train_iter)\n        \n        model.eval()\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in val_iter:\n                x = batch.text[:, :-1]\n                y = batch.text[:, 1:].flatten()\n                \n                output = model(x)\n                loss = criterion(output, y)\n                \n                val_loss += loss.item()\n        \n        val_loss /= len(val_iter)\n        \n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'language_model.pt')\n        \n        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\nIn the train_model function, we define the training loop: - We initialize the optimizer and the loss function. - For each epoch, we iterate over the training data. - For each batch, we compute the forward pass, calculate the loss, compute the gradients, and update the model’s parameters using the optimizer. - After each epoch, we evaluate the model on the validation data and save the model if the validation loss improves. - Finally, we print the epoch number, training loss, and validation loss."
  },
  {
    "objectID": "code/RNN_LM.html#generating-text-with-the-rnn-language-model",
    "href": "code/RNN_LM.html#generating-text-with-the-rnn-language-model",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "5. Generating Text with the RNN Language Model",
    "text": "5. Generating Text with the RNN Language Model\nLastly, let’s define a function to generate text using our trained language model.\n\ndef generate_text(model, seed_text, max_length):\n    model.eval()\n    \n    with torch.no_grad():\n        tokens = seed_text.split()\n        current_length = len(tokens)\n        \n        while current_length &lt; max_length:\n            x = torch.tensor([[TEXT.vocab.stoi[token] for token in tokens]]).to(device)\n            \n            output = model(x)\n            last_word_logits = output[0, -1]\n            \n            probabilities = F.softmax(last_word_logits, dim=0).numpy()\n            predicted_index = np.random.choice(len(probabilities), p=probabilities)\n            predicted_word = TEXT.vocab.itos[predicted_index]\n            \n            tokens.append(predicted_word)\n            current_length += 1\n            \n    generated_text = ' '.join(tokens)\n    return generated_text\n\nIn the generate_text function, we generate text of a given maximum length using the trained language model: - We initialize the model in evaluation mode. - We tokenize the seed text into a list of words. - We repeatedly generate the next word in the sequence until we reach the maximum length. - For each step, we pass the input sequence through the model to obtain the logits for the next word. - We apply softmax to the logits to obtain word probabilities and sample the next word using np.random.choice(). - We append the predicted word to the list of tokens and update the current length accordingly. - Finally, we concatenate the tokens and return the generated text."
  },
  {
    "objectID": "code/RNN_LM.html#putting-it-all-together",
    "href": "code/RNN_LM.html#putting-it-all-together",
    "title": "Recurrent Neural Networks (RNNs) for Language Modeling in PyTorch",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nNow, let’s put everything together and train our RNN language model.\n\n# Set the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Preprocess the dataset\ntrain_iter, val_iter, test_iter = preprocess_dataset()\n\n# Define the model\nvocab_size = len(train_iter.dataset.fields['text'].vocab)\nembedding_dim = 100\nhidden_dim = 128\nnum_layers = 2\nmodel = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n\n# Train the model\nnum_epochs = 10\nlearning_rate = 0.001\ntrain_model(model, train_iter, val_iter, num_epochs, learning_rate)\n\n# Generate text using the trained model\nseed_text = \"The weather is\"\nmax_length = 20\ngenerated_text = generate_text(model, seed_text, max_length)\nprint(generated_text)\n\nIn this code snippet, we perform the following steps: - We set the device to CUDA if available, otherwise fallback to CPU. - We preprocess the dataset using the preprocess_dataset() function, which returns iterators for training, validation, and testing. - We define the model architecture using the RNNLanguageModel class. - We train the model using the train_model() function. - We generate text using the generate_text() function, providing a seed text and maximum length.\nThat’s it! You have now learned how to use RNNs for language modeling in PyTorch. You can further experiment by changing the model architecture, hyperparameters, or using different datasets. Happy coding!"
  },
  {
    "objectID": "Introduction_to_PyTorch.html#note.-resetting-the-gradient-must-be-inside-the-no_grad-context-manager.",
    "href": "Introduction_to_PyTorch.html#note.-resetting-the-gradient-must-be-inside-the-no_grad-context-manager.",
    "title": "Linear Regression and Gradient Descent",
    "section": "Note. Resetting the gradient must be inside the no_grad context manager.",
    "text": "Note. Resetting the gradient must be inside the no_grad context manager.\n\nw = tensor([[1.]], requires_grad=True)\nfor iteration in range(200):\n  y_hat = X @ w\n  loss = loss_fn(y_hat, y)\n  loss.backward()\n  with torch.no_grad():\n    w.data = w.data - w.grad * lr\n    # resetting the gradient must be inside the no_grad context manager\n    w.grad = None\n  if iteration%10==0:\n    print(f'loss`: {loss:.2f}, w: {w.item():.2f}')\n\n\n\n\nloss`: 9.56, w: 1.06\nloss`: 6.72, w: 1.58\nloss`: 4.80, w: 2.01\nloss`: 3.50, w: 2.37\nloss`: 2.62, w: 2.66\nloss`: 2.03, w: 2.90\nloss`: 1.63, w: 3.10\nloss`: 1.35, w: 3.26\nloss`: 1.17, w: 3.39\nloss`: 1.04, w: 3.50\nloss`: 0.96, w: 3.59\nloss`: 0.90, w: 3.67\nloss`: 0.86, w: 3.73\nloss`: 0.84, w: 3.78\nloss`: 0.82, w: 3.82\nloss`: 0.81, w: 3.86\nloss`: 0.80, w: 3.88\nloss`: 0.79, w: 3.91\nloss`: 0.79, w: 3.93\nloss`: 0.79, w: 3.94\n\n\n\nw = tensor([[1.]], requires_grad=True)\noptimizer = torch.optim.SGD([w], lr = lr)\n\n\nfor iteration in range(20):\n  y_hat = X @ w\n  loss = loss_fn(y_hat, y)\n  loss.backward()\n  optimizer.step()\n  optimizer.zero_grad()\n  print(f'loss`: {loss:.2f}, w: {w}')\n\nloss`: 9.56, w: tensor([[1.06]], requires_grad=True)\nloss`: 9.23, w: tensor([[1.12]], requires_grad=True)\nloss`: 8.90, w: tensor([[1.17]], requires_grad=True)\nloss`: 8.59, w: tensor([[1.23]], requires_grad=True)\nloss`: 8.29, w: tensor([[1.28]], requires_grad=True)\nloss`: 8.01, w: tensor([[1.33]], requires_grad=True)\nloss`: 7.73, w: tensor([[1.39]], requires_grad=True)\nloss`: 7.46, w: tensor([[1.44]], requires_grad=True)\nloss`: 7.21, w: tensor([[1.49]], requires_grad=True)\nloss`: 6.96, w: tensor([[1.53]], requires_grad=True)\nloss`: 6.72, w: tensor([[1.58]], requires_grad=True)\nloss`: 6.50, w: tensor([[1.63]], requires_grad=True)\nloss`: 6.28, w: tensor([[1.68]], requires_grad=True)\nloss`: 6.07, w: tensor([[1.72]], requires_grad=True)\nloss`: 5.87, w: tensor([[1.77]], requires_grad=True)\nloss`: 5.67, w: tensor([[1.81]], requires_grad=True)\nloss`: 5.48, w: tensor([[1.85]], requires_grad=True)\nloss`: 5.30, w: tensor([[1.89]], requires_grad=True)\nloss`: 5.13, w: tensor([[1.93]], requires_grad=True)\nloss`: 4.96, w: tensor([[1.97]], requires_grad=True)"
  },
  {
    "objectID": "groups/grouping.html",
    "href": "groups/grouping.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "import pandas as pd\n\n\ndata = pd.read_csv(\"./MATH80629A Project Team Registration.csv\")\n\n\ndata\n\n\n\n\n\n\n\n\nTimestamp\nTeam name\nTeam size (between 2-3)\nList members name and their email address separated by comma\nContact person: Full name (select one team member as a contact person for your team)\n\n\n\n\n0\n2023/08/29 6:18:49 PM AST\nCyrille & Gabriel & Pierre-Chanel\n3\ngabriel jobert: gabriel.jobert@hec.ca | Cyrill...\nPierre-Chanel MANGIN\n\n\n1\n2023/08/29 6:19:36 PM AST\nGamma A - team\n3\nSebastian Cabrera sebastian.cabrera-malik@hec....\nSebastian Cabrera\n\n\n2\n2023/08/29 6:22:12 PM AST\nGPT wannabes\n3\nFarah Atieh, farah.atieh@hec.ca,Yubo Guo, yubo...\nFarah Atieh\n\n\n3\n2023/08/29 6:22:50 PM AST\nRough Diamands\n3\nAndy Mai, andy.mai@hec.ca, Marie-Gloire Soku m...\nMarie-Gloire Soku\n\n\n4\n2023/08/29 6:23:41 PM AST\nNST\n3\nNastaran Behzadpour, nastaran.behzadpour@hec.c...\nTaha Varol\n\n\n5\n2023/08/29 6:24:23 PM AST\nRisk Minds Data Guardians\n3\nShayan Nayemi seyed-shayan.nazemi@hec.ca, Sahe...\nRaphaël Radzuweit\n\n\n6\n2023/08/29 6:31:08 PM AST\nData Divers\n3\nHassen Kammoun, hassen.kammoun@hec.ca, Amal Ko...\nHassen Kammoun\n\n\n7\n2023/08/30 5:28:47 PM AST\nKHY\n3\nYassir Benkouira, yassir.benkouira@hec.ca ; Ho...\nYassir Benkouira\n\n\n8\n2023/08/31 5:49:33 PM AST\nDasen Xun Jordan\n3\nDasen Ye, Dasen.Ye@hec.ca, Xun.Lu@hec.ca, Jord...\nJordan Dionne\n\n\n\n\n\n\n\n\ndat = data[['Team name', 'List members name and their email address separated by comma']]\ndat\n\n\n\n\n\n\n\n\nTeam name\nList members name and their email address separated by comma\n\n\n\n\n0\nCyrille & Gabriel & Pierre-Chanel\ngabriel jobert: gabriel.jobert@hec.ca | Cyrill...\n\n\n1\nGamma A - team\nSebastian Cabrera sebastian.cabrera-malik@hec....\n\n\n2\nGPT wannabes\nFarah Atieh, farah.atieh@hec.ca,Yubo Guo, yubo...\n\n\n3\nRough Diamands\nAndy Mai, andy.mai@hec.ca, Marie-Gloire Soku m...\n\n\n4\nNST\nNastaran Behzadpour, nastaran.behzadpour@hec.c...\n\n\n5\nRisk Minds Data Guardians\nShayan Nayemi seyed-shayan.nazemi@hec.ca, Sahe...\n\n\n6\nData Divers\nHassen Kammoun, hassen.kammoun@hec.ca, Amal Ko...\n\n\n7\nKHY\nYassir Benkouira, yassir.benkouira@hec.ca ; Ho...\n\n\n8\nDasen Xun Jordan\nDasen Ye, Dasen.Ye@hec.ca, Xun.Lu@hec.ca, Jord...\n\n\n\n\n\n\n\n\nemail_pattern = r'(\\b[\\w\\.-]+@[\\w\\.-]+\\b)'\ndat['emails'] = data['List members name and their email address separated by comma']\ndat['emails'].str.extractall(email_pattern).groupby(level=0).agg(', '.join)\n\n/var/folders/jv/ppbxly7j7vzgcr8sdv78s2hr0000gn/T/ipykernel_3414/546319782.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dat['emails'] = data['List members name and their email address separated by comma']\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\ngabriel.jobert@hec.ca, kouassi-cyrille.kouassi...\n\n\n1\nsebastian.cabrera-malik@hec.ca, nannan.du@hec....\n\n\n2\nfarah.atieh@hec.ca, yubo.guo@hec.ca, seraphine...\n\n\n3\nandy.mai@hec.ca, marie-gloire-ayowa.soku@hec.c...\n\n\n4\nnastaran.behzadpour@hec.ca, taha.varol@hec.ca,...\n\n\n5\nseyed-shayan.nazemi@hec.ca, sahel.hajivand@hec...\n\n\n6\nhassen.kammoun@hec.ca, amal.koodoruth@hec.ca, ...\n\n\n7\nyassir.benkouira@hec.ca, hong.zhao@hec.ca, kam...\n\n\n8\nDasen.Ye@hec.ca, Xun.Lu@hec.ca, Jordan.Dionne@...\n\n\n\n\n\n\n\n\nemails = dat['emails'].str.extractall(email_pattern).reset_index()\nemails\n\n\n\n\n\n\n\n\nlevel_0\nmatch\n0\n\n\n\n\n0\n0\n0\ngabriel.jobert@hec.ca\n\n\n1\n0\n1\nkouassi-cyrille.kouassi@hec.ca\n\n\n2\n0\n2\npierre-chanel.mangin@hec.ca\n\n\n3\n1\n0\nsebastian.cabrera-malik@hec.ca\n\n\n4\n1\n1\nnannan.du@hec.ca\n\n\n5\n1\n2\nyue.4.zhang@hec.ca\n\n\n6\n2\n0\nfarah.atieh@hec.ca\n\n\n7\n2\n1\nyubo.guo@hec.ca\n\n\n8\n2\n2\nseraphine.ogmen@hec.ca\n\n\n9\n3\n0\nandy.mai@hec.ca\n\n\n10\n3\n1\nmarie-gloire-ayowa.soku@hec.ca\n\n\n11\n3\n2\nrebecca.ngalula@hec.ca\n\n\n12\n4\n0\nnastaran.behzadpour@hec.ca\n\n\n13\n4\n1\ntaha.varol@hec.ca\n\n\n14\n4\n2\nsilvana.libreros@hec.ca\n\n\n15\n5\n0\nseyed-shayan.nazemi@hec.ca\n\n\n16\n5\n1\nsahel.hajivand@hec.ca\n\n\n17\n5\n2\nraphael.radzuweit@hec.ca\n\n\n18\n6\n0\nhassen.kammoun@hec.ca\n\n\n19\n6\n1\namal.koodoruth@hec.ca\n\n\n20\n6\n2\naicha.camara@hec.ca\n\n\n21\n7\n0\nyassir.benkouira@hec.ca\n\n\n22\n7\n1\nhong.zhao@hec.ca\n\n\n23\n7\n2\nkamel-yousri.amrouche@hec.ca\n\n\n24\n8\n0\nDasen.Ye@hec.ca\n\n\n25\n8\n1\nXun.Lu@hec.ca\n\n\n26\n8\n2\nJordan.Dionne@hec.ca\n\n\n\n\n\n\n\n\nfinal = pd.merge(dat[\"Team name\"],emails, right_on=\"level_0\", left_index=True)\nfinal\n\n\n\n\n\n\n\n\nTeam name\nlevel_0\nmatch\n0\n\n\n\n\n0\nCyrille & Gabriel & Pierre-Chanel\n0\n0\ngabriel.jobert@hec.ca\n\n\n1\nCyrille & Gabriel & Pierre-Chanel\n0\n1\nkouassi-cyrille.kouassi@hec.ca\n\n\n2\nCyrille & Gabriel & Pierre-Chanel\n0\n2\npierre-chanel.mangin@hec.ca\n\n\n3\nGamma A - team\n1\n0\nsebastian.cabrera-malik@hec.ca\n\n\n4\nGamma A - team\n1\n1\nnannan.du@hec.ca\n\n\n5\nGamma A - team\n1\n2\nyue.4.zhang@hec.ca\n\n\n6\nGPT wannabes\n2\n0\nfarah.atieh@hec.ca\n\n\n7\nGPT wannabes\n2\n1\nyubo.guo@hec.ca\n\n\n8\nGPT wannabes\n2\n2\nseraphine.ogmen@hec.ca\n\n\n9\nRough Diamands\n3\n0\nandy.mai@hec.ca\n\n\n10\nRough Diamands\n3\n1\nmarie-gloire-ayowa.soku@hec.ca\n\n\n11\nRough Diamands\n3\n2\nrebecca.ngalula@hec.ca\n\n\n12\nNST\n4\n0\nnastaran.behzadpour@hec.ca\n\n\n13\nNST\n4\n1\ntaha.varol@hec.ca\n\n\n14\nNST\n4\n2\nsilvana.libreros@hec.ca\n\n\n15\nRisk Minds Data Guardians\n5\n0\nseyed-shayan.nazemi@hec.ca\n\n\n16\nRisk Minds Data Guardians\n5\n1\nsahel.hajivand@hec.ca\n\n\n17\nRisk Minds Data Guardians\n5\n2\nraphael.radzuweit@hec.ca\n\n\n18\nData Divers\n6\n0\nhassen.kammoun@hec.ca\n\n\n19\nData Divers\n6\n1\namal.koodoruth@hec.ca\n\n\n20\nData Divers\n6\n2\naicha.camara@hec.ca\n\n\n21\nKHY\n7\n0\nyassir.benkouira@hec.ca\n\n\n22\nKHY\n7\n1\nhong.zhao@hec.ca\n\n\n23\nKHY\n7\n2\nkamel-yousri.amrouche@hec.ca\n\n\n24\nDasen Xun Jordan\n8\n0\nDasen.Ye@hec.ca\n\n\n25\nDasen Xun Jordan\n8\n1\nXun.Lu@hec.ca\n\n\n26\nDasen Xun Jordan\n8\n2\nJordan.Dionne@hec.ca\n\n\n\n\n\n\n\n\n# final[['Team name', '0']]\nfinal.columns\n\nIndex(['Team name', 'level_0', 'match', 0], dtype='object')\n\n\n\nfinal[['Team name', 0]].to_csv(\"./groups.csv\", index=False, header=False)"
  },
  {
    "objectID": "Minibatch_PyTorch.html",
    "href": "Minibatch_PyTorch.html",
    "title": "MNIST dataset",
    "section": "",
    "text": "!git clone \"https://github.com/davoodwadi/davoodwadi.github.io.git\"\n\nCloning into 'davoodwadi.github.io'...\nremote: Enumerating objects: 805, done.\nremote: Counting objects: 100% (69/69), done.\nremote: Compressing objects: 100% (47/47), done.\nremote: Total 805 (delta 31), reused 42 (delta 17), pack-reused 736\nReceiving objects: 100% (805/805), 89.88 MiB | 13.12 MiB/s, done.\nResolving deltas: 100% (459/459), done.\nUpdating files: 100% (244/244), done.\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np, matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor, nn\ntorch.manual_seed(42)\n\nmpl.rcParams['image.cmap'] = 'gray'\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\npath_data = Path('./davoodwadi.github.io/data/')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\nx_train.shape\n\ntorch.Size([50000, 784])\ny_train[0]\n\ntensor(5)\ni=0\nplt.imshow(x_train[i].view(28,28))\nplt.title(y_train[i].item())\n\nText(0.5, 1.0, '5')"
  },
  {
    "objectID": "Minibatch_PyTorch.html#define-the-model",
    "href": "Minibatch_PyTorch.html#define-the-model",
    "title": "MNIST dataset",
    "section": "Define the model",
    "text": "Define the model\n\nclass FCModel(nn.Module):\n  def __init__(self, feature_size, hidden_size):\n    super(FCModel, self).__init__()\n\n    self.fc1 = nn.Linear(feature_size, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, 10)\n    self.relu = nn.ReLU()\n  def forward(self, x):\n    out1 = self.fc1(x)\n    out2 = self.relu(out1)\n    out3 = self.fc2(out2)\n    return out3"
  },
  {
    "objectID": "Minibatch_PyTorch.html#fully-connected-layer",
    "href": "Minibatch_PyTorch.html#fully-connected-layer",
    "title": "MNIST dataset",
    "section": "Fully Connected layer",
    "text": "Fully Connected layer\n\nbx.shape\n\ntorch.Size([64, 784])\n\n\n\nhidden_size = 256\nfc = nn.Linear(bx.shape[1], hidden_size)\n\n\nactivation = fc(bx)\nactivation.shape\n\ntorch.Size([64, 256])\n\n\n\nfc.weight.shape, fc.bias.shape\n\n(torch.Size([256, 784]), torch.Size([256]))\n\n\n\nactivation_manual = bx @ fc.weight.T + fc.bias\nactivation_manual.shape\n\ntorch.Size([64, 256])"
  },
  {
    "objectID": "Minibatch_PyTorch.html#create-dataset-and-dataloader",
    "href": "Minibatch_PyTorch.html#create-dataset-and-dataloader",
    "title": "MNIST dataset",
    "section": "Create dataset and dataloader",
    "text": "Create dataset and dataloader\n\ntrainset = dataset(x_train, y_train)\ntestset = dataset(x_test, y_test)\n\n\ntrain_loader = DataLoader(trainset, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(testset, batch_size=64)\n\n\ncifar_train.classes\n\n['airplane',\n 'automobile',\n 'bird',\n 'cat',\n 'deer',\n 'dog',\n 'frog',\n 'horse',\n 'ship',\n 'truck']\n\n\n\nindex = 0\nbx, by = next(iter(train_loader))\nplt.imshow(bx[index])\nplt.title(cifar_train.classes[by[index]])\nbx[index].shape\n\ntorch.Size([32, 32, 3])\n\n\n\n\n\n\nclass CNN(nn.Module):\n  def __init__(self, features):\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=3, out_channels=features, kernel_size=(3,3), stride=(1,1), padding='same')\n    self.conv2 = nn.Conv2d(in_channels=features, out_channels=features*2, kernel_size=(3,3), stride=(1,1), padding='same')\n    self.conv3 = nn.Conv2d(in_channels=features*2, out_channels=features*4, kernel_size=(3,3), stride=(1,1), padding='same')\n    self.activation_fn = nn.ReLU()\n    self.pooling = nn.MaxPool2d((2, 2))\n    self.global_pooling = nn.AdaptiveAvgPool2d()\n    self.fc = nn.Linear(features*4, 10)\n\n  def forward(self, x):\n    out = self.conv1(x)\n    out = self.activation_fn(out)\n    out = self.pooling(out)\n    out = self.conv2(out)\n    out = self.activation_fn(out)\n    out = self.pooling(out)\n    out = self.conv3(out)\n    out = self.activation_fn(out)\n    out = self.global_pooling(out)\n    out = out.view(out.shape[0], -1)\n    out = self.fc(out)\n    return out\n\n\nfeatures = 8\nlr = 0.001\nepochs = 100\n\nmodel = CNN(features)\noptimizer = torch.optim.AdamW(model.parameters(), lr)\nloss_fn = nn.CrossEntropyLoss()\n\nTypeError: ignored\n\n\n\ntrain_eval(model, optimizer, train_loader, valid_loader, im_shape=32, n_channels=3, epoch=epochs, display_freq=10)\n\nepoch 0; loss 1.2450; accuracy 0.49\n********************\nepoch 1; loss 1.2443; accuracy 0.48\n********************\nepoch 2; loss 1.2408; accuracy 0.49\n********************\nepoch 3; loss 1.2400; accuracy 0.49\n********************\nepoch 4; loss 1.2362; accuracy 0.49\n********************\nepoch 5; loss 1.2355; accuracy 0.50\n********************\nepoch 6; loss 1.2315; accuracy 0.50\n********************\nepoch 7; loss 1.2293; accuracy 0.50\n********************\nepoch 8; loss 1.2254; accuracy 0.50\n********************\nepoch 9; loss 1.2242; accuracy 0.50\n********************\nepoch 10; loss 1.2188; accuracy 0.49\n********************\nepoch 11; loss 1.2236; accuracy 0.49\n********************\nepoch 12; loss 1.2185; accuracy 0.50\n********************\nepoch 13; loss 1.2158; accuracy 0.49\n********************\nepoch 14; loss 1.2105; accuracy 0.50\n********************\nepoch 15; loss 1.2142; accuracy 0.50\n********************\nepoch 16; loss 1.2112; accuracy 0.49\n********************\nepoch 17; loss 1.2078; accuracy 0.49\n********************\nepoch 18; loss 1.2036; accuracy 0.49\n********************\nepoch 19; loss 1.2043; accuracy 0.50\n********************\nepoch 20; loss 1.2057; accuracy 0.50\n********************\nepoch 21; loss 1.2014; accuracy 0.50\n********************\nepoch 22; loss 1.1988; accuracy 0.50\n********************\nepoch 23; loss 1.2005; accuracy 0.50\n********************\nepoch 24; loss 1.1956; accuracy 0.49\n********************\nepoch 25; loss 1.1943; accuracy 0.49\n********************\nepoch 26; loss 1.1931; accuracy 0.50\n********************\nepoch 27; loss 1.1945; accuracy 0.49\n********************\nepoch 28; loss 1.1922; accuracy 0.50\n********************\nepoch 29; loss 1.1891; accuracy 0.50\n********************\nepoch 30; loss 1.1911; accuracy 0.50\n********************\nepoch 31; loss 1.1864; accuracy 0.49\n********************\nepoch 32; loss 1.1842; accuracy 0.50\n********************\nepoch 33; loss 1.1858; accuracy 0.50\n********************\nepoch 34; loss 1.1837; accuracy 0.50\n********************\nepoch 35; loss 1.1837; accuracy 0.50\n********************\nepoch 36; loss 1.1831; accuracy 0.49\n********************\nepoch 37; loss 1.1818; accuracy 0.49\n********************\nepoch 38; loss 1.1815; accuracy 0.50\n********************\nepoch 39; loss 1.1793; accuracy 0.49\n********************\nepoch 40; loss 1.1748; accuracy 0.50\n********************\nepoch 41; loss 1.1751; accuracy 0.49\n********************\nepoch 42; loss 1.1773; accuracy 0.50\n********************\nepoch 43; loss 1.1748; accuracy 0.50\n********************\nepoch 44; loss 1.1737; accuracy 0.50\n********************\nepoch 45; loss 1.1706; accuracy 0.50\n********************\nepoch 46; loss 1.1731; accuracy 0.51\n********************\nepoch 47; loss 1.1669; accuracy 0.50\n********************\nepoch 48; loss 1.1720; accuracy 0.50\n********************\nepoch 49; loss 1.1657; accuracy 0.50\n********************\nepoch 50; loss 1.1687; accuracy 0.51\n********************\nepoch 51; loss 1.1663; accuracy 0.50\n********************\nepoch 52; loss 1.1666; accuracy 0.50\n********************\nepoch 53; loss 1.1655; accuracy 0.50\n********************\nepoch 54; loss 1.1634; accuracy 0.49\n********************\nepoch 55; loss 1.1648; accuracy 0.50\n********************\nepoch 56; loss 1.1619; accuracy 0.50\n********************\nepoch 57; loss 1.1613; accuracy 0.50\n********************\nepoch 58; loss 1.1614; accuracy 0.51\n********************\nepoch 59; loss 1.1615; accuracy 0.50\n********************\nepoch 60; loss 1.1541; accuracy 0.50\n********************\nepoch 61; loss 1.1559; accuracy 0.50\n********************\nepoch 62; loss 1.1574; accuracy 0.50\n********************\nepoch 63; loss 1.1555; accuracy 0.50\n********************\nepoch 64; loss 1.1575; accuracy 0.49\n********************\nepoch 65; loss 1.1580; accuracy 0.50\n********************\nepoch 66; loss 1.1594; accuracy 0.50\n********************\nepoch 67; loss 1.1529; accuracy 0.51\n********************\nepoch 68; loss 1.1565; accuracy 0.50\n********************\nepoch 69; loss 1.1536; accuracy 0.50\n********************\nepoch 70; loss 1.1511; accuracy 0.51\n********************\nepoch 71; loss 1.1517; accuracy 0.50\n********************\nepoch 72; loss 1.1473; accuracy 0.50\n********************\nepoch 73; loss 1.1529; accuracy 0.50\n********************\nepoch 74; loss 1.1502; accuracy 0.50\n********************\nepoch 75; loss 1.1526; accuracy 0.51\n********************\nepoch 76; loss 1.1487; accuracy 0.51\n********************\nepoch 77; loss 1.1517; accuracy 0.50\n********************\nepoch 78; loss 1.1458; accuracy 0.50\n********************\nepoch 79; loss 1.1505; accuracy 0.50\n********************\nepoch 80; loss 1.1497; accuracy 0.50\n********************\nepoch 81; loss 1.1479; accuracy 0.50\n********************\nepoch 82; loss 1.1457; accuracy 0.50\n********************\nepoch 83; loss 1.1487; accuracy 0.51\n********************\nepoch 84; loss 1.1455; accuracy 0.50\n********************\nepoch 85; loss 1.1448; accuracy 0.51\n********************\nepoch 86; loss 1.1437; accuracy 0.51\n********************\nepoch 87; loss 1.1436; accuracy 0.50\n********************\nepoch 88; loss 1.1425; accuracy 0.49\n********************\nepoch 89; loss 1.1411; accuracy 0.51\n********************\nepoch 90; loss 1.1440; accuracy 0.50\n********************\nepoch 91; loss 1.1404; accuracy 0.50\n********************\nepoch 92; loss 1.1421; accuracy 0.50\n********************\nepoch 93; loss 1.1427; accuracy 0.50\n********************\nepoch 94; loss 1.1384; accuracy 0.51\n********************\nepoch 95; loss 1.1390; accuracy 0.50\n********************\nepoch 96; loss 1.1395; accuracy 0.51\n********************\nepoch 97; loss 1.1397; accuracy 0.51\n********************\nepoch 98; loss 1.1410; accuracy 0.50\n********************\nepoch 99; loss 1.1402; accuracy 0.50\n********************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex = 0\nimg=bx[index]\nplt.imshow(bx[index])\nplt.title(cifar_train.classes[by[index]])\nbx[index].shape\n\ntorch.Size([32, 32, 3])\n\n\n\n\n\n\nfirst_conv_activation = model.conv1(img.view(3, 32, 32).cuda())\nshow_images(first_conv_activation.cpu())\nfirst_conv_activation.shape\n\ntorch.Size([8, 32, 32])\n\n\n\n\n\n\nsecond_conv_activation = model.conv2(first_conv_activation.cuda())\nshow_images(second_conv_activation.cpu())\nsecond_conv_activation.shape\n\ntorch.Size([16, 32, 32])"
  },
  {
    "objectID": "SVD.html",
    "href": "SVD.html",
    "title": "Get ActiveSGD algorithm",
    "section": "",
    "text": "import torch\n\n\na = torch.tensor([[1,0,0],\n                  [0,1,1],\n                  [0,-1,1],\n                  [0,1,1]],\n                 ).float()\na\n\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  1.],\n        [ 0., -1.,  1.],\n        [ 0.,  1.,  1.]])\n\n\n\nk = 20\nu = torch.randn(a.shape[0],k,\n                 requires_grad=True)\n\ne = torch.randn(k,k,\n                 requires_grad=True)\nv = torch.randn(k,a.shape[1],\n                 requires_grad=True)\nv\n\ntensor([[ 0.3727,  0.4020, -0.5528],\n        [-0.2050, -0.4838, -0.2506],\n        [-1.5798, -0.2937,  1.2738],\n        [ 1.0125,  0.5758,  0.0430],\n        [ 0.1004,  0.9923,  0.5611],\n        [-1.2531, -0.7247,  1.5790],\n        [ 1.6918,  2.4901, -0.6655],\n        [ 0.1845, -1.2979,  0.3447],\n        [-0.8393,  0.1415,  1.4891],\n        [-0.2766, -1.1580,  0.7771],\n        [-0.4322,  0.8322,  0.2769],\n        [-0.7442,  0.1456, -0.2111],\n        [ 0.6675, -0.0240, -0.6332],\n        [-2.3263, -0.6030,  0.5034],\n        [ 0.5356, -0.0642, -0.4217],\n        [-0.5647,  0.3019, -0.6941],\n        [ 1.7027, -0.6539, -0.9952],\n        [-0.0654,  0.4789, -0.3537],\n        [-0.1381,  0.1470,  0.8018],\n        [-2.5433,  0.3890,  0.0380]], requires_grad=True)\n\n\n\nwith torch.no_grad():\n  print((u @ e) @ v)\n\ntensor([[ 13.9237,  15.2582,  -9.2116],\n        [  6.6544, -12.3802,  -4.2403],\n        [-16.9099, -32.8751,  11.2544],\n        [ 30.6219,  15.0554,  -2.8704]])\n\n\n\na_hat = (u @ e) @ v\nloss = loss_fn(a_hat[mask], a[mask])\nloss.backward()\n\n\nu.grad\n\ntensor([[-3.1585e+01, -2.1295e+00,  1.7229e+01, -2.9118e+01,  2.1858e+01,\n         -1.0075e+01,  6.5761e+00,  1.2604e+01,  2.8173e+01, -7.5108e+00,\n         -1.0504e+01,  6.2597e+00, -1.5025e+01, -2.1357e+01, -1.5101e+01,\n          2.9773e+00,  5.2052e+00, -8.2106e+00,  3.6537e+00,  6.6837e+00],\n        [-1.3986e+01,  2.6096e+00,  2.2352e+01,  2.4575e+01,  5.3397e+01,\n         -2.3649e+01, -5.5750e+01,  2.2495e+01,  1.6939e+01, -3.0122e+01,\n          2.9761e+01,  2.5503e+01,  2.6977e+01,  4.1092e+01,  3.7109e+00,\n          1.0263e+01, -2.2510e+01,  2.8549e+01,  2.1062e+01,  2.0827e+01],\n        [ 6.2770e+00,  1.3564e+01,  1.2677e+01,  3.1640e+01,  2.7399e+01,\n         -1.6760e+01, -4.6806e+01,  5.5829e+00,  7.8993e-02, -2.5555e+01,\n          2.7347e+01,  9.4444e+00,  1.7262e+01,  4.3731e+01,  1.8627e+01,\n          2.1008e+01, -2.2945e+01,  1.8907e+01,  1.4966e+01,  6.5067e+00],\n        [ 2.3790e+01,  4.6461e+00, -6.8382e+00,  3.5006e+01, -9.4395e-01,\n         -3.7348e-02, -2.6268e+01, -4.1966e+00, -1.9225e+01, -5.1356e+00,\n          2.0174e+01,  2.3740e+00,  2.1545e+01,  3.4501e+01,  1.6427e+01,\n          3.4800e+00, -1.3387e+01,  1.6582e+01,  4.4162e+00,  4.4221e-01]])\n\n\n\nu.shape\n\ntorch.Size([4, 20])\n\n\n\nk = 20\nu = torch.randn(a.shape[0],k,\n                 requires_grad=True)\n\ne = torch.randn(k,k,\n                 requires_grad=True)\nv = torch.randn(k,a.shape[1],\n                 requires_grad=True)\n\n\noptim = torch.optim.SGD([u, e, v], lr = 0.001, momentum=0)\nloss_fn = torch.nn.MSELoss()\nepoch = 20\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = (u @ e) @ v # added sigmoid to force (0, 1)\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%1==0:\n    print(f'epoch {ep}: loss {loss.detach().item():0.4f}')\n\nepoch 0: loss 269.1609\nepoch 1: loss 146.8698\nepoch 2: loss 87.0954\nepoch 3: loss 54.5714\nepoch 4: loss 35.6558\nepoch 5: loss 24.1037\nepoch 6: loss 16.7645\nepoch 7: loss 11.9434\nepoch 8: loss 8.6836\nepoch 9: loss 6.4236\nepoch 10: loss 4.8222\nepoch 11: loss 3.6658\nepoch 12: loss 2.8168\nepoch 13: loss 2.1845\nepoch 14: loss 1.7076\nepoch 15: loss 1.3441\nepoch 16: loss 1.0642\nepoch 17: loss 0.8469\nepoch 18: loss 0.6770\nepoch 19: loss 0.5433\n\n\n\nwith torch.no_grad():\n  print((u @ e) @ v)\n\ntensor([[ 1.0000e+00, -7.4953e-06, -1.2398e-05],\n        [-1.4305e-06,  1.0000e+00,  1.0000e+00],\n        [-1.9073e-06,  3.4807e+00,  9.9999e-01],\n        [ 5.7220e-06,  1.0000e+00,  9.9999e-01]])\n\n\n\na\n\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  1.],\n        [ 0., -1.,  1.],\n        [ 0.,  1.,  1.]])\n\n\n“Read The Signs” paper\nPyTorch optimizer\n\n!wget https://raw.githubusercontent.com/davoodwadi/active-lr/main/optimizers/ActiveSGD.py\n\n--2023-09-25 12:09:51--  https://raw.githubusercontent.com/davoodwadi/active-lr/main/optimizers/ActiveSGD.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4148 (4.1K) [text/plain]\nSaving to: ‘ActiveSGD.py’\n\nActiveSGD.py          0%[                    ]       0  --.-KB/s               ActiveSGD.py        100%[===================&gt;]   4.05K  --.-KB/s    in 0s      \n\n2023-09-25 12:09:51 (68.1 MB/s) - ‘ActiveSGD.py’ saved [4148/4148]\n\n\n\n\nfrom ActiveSGD import ActiveSGD\n\n\nk = 20\nu = torch.randn(a.shape[0],k,\n                 requires_grad=True)\n\ne = torch.randn(k,k,\n                 requires_grad=True)\nv = torch.randn(k,a.shape[1],\n                 requires_grad=True)\n\n\nstepSize = 1\noptim = ActiveSGD([u, e, v], stepSize=1, lr = 0.001, momentum=0, lrLow=0.5, lrHigh=2.)\nloss_fn = torch.nn.MSELoss()\nepoch = 20\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = (u @ e) @ v # added sigmoid to force (0, 1)\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%1==0:\n    print(f'epoch {ep}: loss {loss.detach().item():0.4f}')\n\nepoch 0: loss 401.8856\nepoch 1: loss 204.4616\nepoch 2: loss 121.5371\nepoch 3: loss 74.8861\nepoch 4: loss 46.9087\nepoch 5: loss 29.4424\nepoch 6: loss 18.3647\nepoch 7: loss 11.3499\nepoch 8: loss 6.9511\nepoch 9: loss 4.2248\nepoch 10: loss 2.5522\nepoch 11: loss 1.5333\nepoch 12: loss 0.9155\nepoch 13: loss 0.5422\nepoch 14: loss 0.3176\nepoch 15: loss 0.1835\nepoch 16: loss 0.1043\nepoch 17: loss 0.0581\nepoch 18: loss 0.0317\nepoch 19: loss 0.0169\n\n\nadd a neural netwrok in the middle\n\nclass NN(torch.nn.Module):\n  def __init__(self,a, hidden_size):\n    super(NN, self).__init__()\n    self.u = torch.nn.Linear(a.shape[0], hidden_size, bias=False)\n    self.e1 = torch.nn.Linear(hidden_size, hidden_size)\n    self.v = torch.nn.Linear(hidden_size, a.shape[1])\n    self.relu = torch.nn.ReLU()\n  def forward(self):\n    x = self.e1(self.u.weight.T)\n    x = self.relu(x)\n    x = self.v(x)\n    return x\n\n\nhidden_size = 20\nlr = 0.001\nmodel = NN(a, hidden_size)\noptim = torch.optim.SGD(model.parameters(), lr=lr)\nloss_fn = torch.nn.MSELoss()\n\n\nepoch = 100\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = model()\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%10==0:\n    print(f'epoch {ep}: loss {loss.detach().item():0.4f}')\n\nepoch 0: loss 0.7275\nepoch 10: loss 0.7156\nepoch 20: loss 0.7040\nepoch 30: loss 0.6926\nepoch 40: loss 0.6815\nepoch 50: loss 0.6707\nepoch 60: loss 0.6601\nepoch 70: loss 0.6498\nepoch 80: loss 0.6397\nepoch 90: loss 0.6298\n\n\nActiveSGD\n\nhidden_size = 20\nlr = 0.001\nmodel = NN(a, hidden_size)\noptim = ActiveSGD(model.parameters(), 1, lr=lr, lrHigh=2., lrLow=0.5)\nloss_fn = torch.nn.MSELoss()\n\n\nepoch = 100\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = model()\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%10==0:\n    print(f'epoch {ep}: loss {loss.detach().item():0.4f}')\n\nepoch 0: loss 0.6306\nepoch 10: loss 0.5806\nepoch 20: loss 0.4504\nepoch 30: loss 0.3030\nepoch 40: loss 0.1950\nepoch 50: loss 0.1383\nepoch 60: loss 0.1081\nepoch 70: loss 0.0785\nepoch 80: loss 0.0492\nepoch 90: loss 0.0243\n\n\n\nwith torch.no_grad():\n  a_hat = model()\n\n\na_hat\n\ntensor([[ 0.9203,  0.1739,  0.1112],\n        [-0.0391,  0.8130,  0.9532],\n        [ 0.0461,  0.4488,  0.9737],\n        [ 0.0257,  1.0491,  0.9791]])\n\n\n\na\n\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  1.],\n        [ 0., -1.,  1.],\n        [ 0.,  1.,  1.]])\n\n\nTime series\n\na = torch.tensor([[1,-1],\n                  [2,6],\n                  [3,7],\n                  [4,8]],\n                 ).float()\na\n\ntensor([[ 1., -1.],\n        [ 2.,  6.],\n        [ 3.,  7.],\n        [ 4.,  8.]])\n\n\n\nhidden_size = 20\nlr = 0.001\nmodel = NN(a, hidden_size)\noptim = torch.optim.SGD(model.parameters(), lr=lr)\nloss_fn = torch.nn.MSELoss()\n\n\nepoch = 100\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = model()\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%10==0:\n    print(f'epoch {ep}: loss {loss.detach().item():0.4f}')\n\nepoch 0: loss 27.2724\nepoch 10: loss 26.6480\nepoch 20: loss 26.0484\nepoch 30: loss 25.4727\nepoch 40: loss 24.9193\nepoch 50: loss 24.3772\nepoch 60: loss 23.8444\nepoch 70: loss 23.3177\nepoch 80: loss 22.7949\nepoch 90: loss 22.2743\n\n\nActiveSGD\n\nhidden_size = 20\nlr = 0.001\nmodel = NN(a, hidden_size)\noptim = ActiveSGD(model.parameters(), 1, lr=lr, lrHigh=2., lrLow=0.5)\nloss_fn = torch.nn.MSELoss()\n\n\nepoch = 100\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = model()\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%10==0:\n    print(f'epoch {ep}: loss {loss.detach().item():0.4f}')\n\nepoch 0: loss 25.9642\nepoch 10: loss 21.8671\nepoch 20: loss 8.9271\nepoch 30: loss 2.8119\nepoch 40: loss 0.4167\nepoch 50: loss 0.2853\nepoch 60: loss 0.1742\nepoch 70: loss 0.0756\nepoch 80: loss 0.0228\nepoch 90: loss 0.0072\n\n\n\nwith torch.no_grad():\n  a_hat = model()\na_hat\n\ntensor([[0.9897, 3.7320],\n        [2.0361, 6.0216],\n        [3.0824, 6.9881],\n        [3.9730, 8.0825]])\n\n\n\na\n\ntensor([[ 1., -1.],\n        [ 2.,  6.],\n        [ 3.,  7.],\n        [ 4.,  8.]])"
  },
  {
    "objectID": "code/DataParallel.html",
    "href": "code/DataParallel.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To train the CIFAR-10 dataset using PyTorch’s DataParallel, we first need to have a basic understanding of what CIFAR-10 is and what PyTorch’s DataParallel does.\nCIFAR-10 is a popular computer vision dataset that consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.\nPyTorch’s DataParallel is a module that allows us to use multiple GPUs for training our models. It splits the input data across the GPUs, performs forward and backward passes on each GPU, and then aggregates the gradients to update the model parameters. This helps in speeding up the training process and achieving better performance.\nNow, let’s dive into the code.\nFirst, we need to import the necessary libraries and modules:\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nNext, we define the CIFAR-10 dataset and its transformations:\n\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n                                             download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n                                            download=True, transform=transform)\n\nThen, we define the data loaders for our training and testing datasets:\n\nbatch_size = 128\nnum_workers = 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, \n                          shuffle=True, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, \n                         shuffle=False, num_workers=num_workers)\n\nNow, let’s define our model:\n\nclass CIFAR10Net(nn.Module):\n    def __init__(self):\n        super(CIFAR10Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout(x)\n        x = x.view(-1, 64 * 16 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CIFAR10Net()\n\nAfter defining our model, we need to define the loss function and the optimizer:\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nNow, we are ready to train our model:\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nif torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    \n    for input, target in train_loader:\n        input, target = input.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for input, target in test_loader:\n            input, target = input.to(device), target.to(device)\n            \n            output = model(input)\n            _, predictions = torch.max(output, 1)\n            \n            total_correct += (predictions == target).sum().item()\n            total_samples += target.size(0)\n    \n    accuracy = total_correct / total_samples\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}\")\n\nFinally, we can evaluate our model on the test set:\n\nmodel.eval()\n\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for input, target in test_loader:\n        input, target = input.to(device), target.to(device)\n\n        output = model(input)\n        _, predictions = torch.max(output, 1)\n\n        total_correct += (predictions == target).sum().item()\n        total_samples += target.size(0)\n\naccuracy = total_correct / total_samples\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\nThat’s it! We have successfully trained the CIFAR-10 dataset using PyTorch’s DataParallel module."
  },
  {
    "objectID": "SVD_movielens100k.html",
    "href": "SVD_movielens100k.html",
    "title": "SVD",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport torch\n!wget 'https://files.grouplens.org/datasets/movielens/ml-100k.zip' './'\n\n--2023-09-25 12:22:25--  https://files.grouplens.org/datasets/movielens/ml-100k.zip\nResolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\nConnecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4924029 (4.7M) [application/zip]\nSaving to: ‘ml-100k.zip’\n\nml-100k.zip         100%[===================&gt;]   4.70M  9.91MB/s    in 0.5s    \n\n2023-09-25 12:22:26 (9.91 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n\n--2023-09-25 12:22:26--  http://./\nResolving . (.)... failed: No address associated with hostname.\nwget: unable to resolve host address ‘.’\nFINISHED --2023-09-25 12:22:26--\nTotal wall clock time: 1.0s\nDownloaded: 1 files, 4.7M in 0.5s (9.91 MB/s)\n!unzip './ml-100k.zip'\n\nArchive:  ./ml-100k.zip\n   creating: ml-100k/\n  inflating: ml-100k/allbut.pl       \n  inflating: ml-100k/mku.sh          \n  inflating: ml-100k/README          \n  inflating: ml-100k/u.data          \n  inflating: ml-100k/u.genre         \n  inflating: ml-100k/u.info          \n  inflating: ml-100k/u.item          \n  inflating: ml-100k/u.occupation    \n  inflating: ml-100k/u.user          \n  inflating: ml-100k/u1.base         \n  inflating: ml-100k/u1.test         \n  inflating: ml-100k/u2.base         \n  inflating: ml-100k/u2.test         \n  inflating: ml-100k/u3.base         \n  inflating: ml-100k/u3.test         \n  inflating: ml-100k/u4.base         \n  inflating: ml-100k/u4.test         \n  inflating: ml-100k/u5.base         \n  inflating: ml-100k/u5.test         \n  inflating: ml-100k/ua.base         \n  inflating: ml-100k/ua.test         \n  inflating: ml-100k/ub.base         \n  inflating: ml-100k/ub.test\nmovie_columns = ['movie id' , 'movie title' , 'release date' , 'video release date' ,\n              'IMDb URL' , 'unknown' , 'Action' , 'Adventure' , 'Animation' ,\n              \"Children's\" , 'Comedy' , 'Crime' , 'Documentary' , 'Drama' , 'Fantasy' ,\n              'Film-Noir' , 'Horror' , 'Musical' , 'Mystery' , 'Romance' , 'Sci-Fi' ,\n              'Thriller' , 'War' , 'Western' ]\nmovies = pd.read_csv('./ml-100k/u.item', sep='|', encoding='latin-1', names = movie_columns)\nmovies\n\n\n  \n    \n\n\n\n\n\n\nmovie id\nmovie title\nrelease date\nvideo release date\nIMDb URL\nunknown\nAction\nAdventure\nAnimation\nChildren's\n...\nFantasy\nFilm-Noir\nHorror\nMusical\nMystery\nRomance\nSci-Fi\nThriller\nWar\nWestern\n\n\n\n\n0\n1\nToy Story (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Toy%20Story%2...\n0\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n2\nGoldenEye (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?GoldenEye%20(...\n0\n1\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n3\nFour Rooms (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Four%20Rooms%...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n4\nGet Shorty (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Get%20Shorty%...\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n5\nCopycat (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Copycat%20(1995)\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1677\n1678\nMat' i syn (1997)\n06-Feb-1998\nNaN\nhttp://us.imdb.com/M/title-exact?Mat%27+i+syn+...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1678\n1679\nB. Monkey (1998)\n06-Feb-1998\nNaN\nhttp://us.imdb.com/M/title-exact?B%2E+Monkey+(...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n1679\n1680\nSliding Doors (1998)\n01-Jan-1998\nNaN\nhttp://us.imdb.com/Title?Sliding+Doors+(1998)\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n1680\n1681\nYou So Crazy (1994)\n01-Jan-1994\nNaN\nhttp://us.imdb.com/M/title-exact?You%20So%20Cr...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1681\n1682\nScream of Stone (Schrei aus Stein) (1991)\n08-Mar-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Schrei%20aus%...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n1682 rows × 24 columns\ngenres = movies.columns[6:].to_numpy()\ndef create_genre(row):\n  genre_int = row[6:]\n  index = np.where(genre_int==1)\n  # return genres[index[0]]\n  return ' '.join(genres[index[0]])\nmovies['Genre'] = movies.apply(create_genre, axis=1)\nmovies\n\n\n  \n    \n\n\n\n\n\n\nmovie id\nmovie title\nrelease date\nvideo release date\nIMDb URL\nunknown\nAction\nAdventure\nAnimation\nChildren's\n...\nFilm-Noir\nHorror\nMusical\nMystery\nRomance\nSci-Fi\nThriller\nWar\nWestern\nGenre\n\n\n\n\n0\n1\nToy Story (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Toy%20Story%2...\n0\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nAnimation Children's Comedy\n\n\n1\n2\nGoldenEye (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?GoldenEye%20(...\n0\n1\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\nAction Adventure Thriller\n\n\n2\n3\nFour Rooms (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Four%20Rooms%...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\nThriller\n\n\n3\n4\nGet Shorty (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Get%20Shorty%...\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nAction Comedy Drama\n\n\n4\n5\nCopycat (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Copycat%20(1995)\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\nCrime Drama Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1677\n1678\nMat' i syn (1997)\n06-Feb-1998\nNaN\nhttp://us.imdb.com/M/title-exact?Mat%27+i+syn+...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nDrama\n\n\n1678\n1679\nB. Monkey (1998)\n06-Feb-1998\nNaN\nhttp://us.imdb.com/M/title-exact?B%2E+Monkey+(...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n1\n0\n0\nRomance Thriller\n\n\n1679\n1680\nSliding Doors (1998)\n01-Jan-1998\nNaN\nhttp://us.imdb.com/Title?Sliding+Doors+(1998)\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\nDrama Romance\n\n\n1680\n1681\nYou So Crazy (1994)\n01-Jan-1994\nNaN\nhttp://us.imdb.com/M/title-exact?You%20So%20Cr...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nComedy\n\n\n1681\n1682\nScream of Stone (Schrei aus Stein) (1991)\n08-Mar-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Schrei%20aus%...\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nDrama\n\n\n\n\n\n1682 rows × 25 columns\nusers = pd.read_csv('./ml-100k/u.user', sep='|', encoding='latin-1', names = ['user id' , 'age' , 'gender' , 'occupation' , 'zip code'])\nusers\n\n\n  \n    \n\n\n\n\n\n\nuser id\nage\ngender\noccupation\nzip code\n\n\n\n\n0\n1\n24\nM\ntechnician\n85711\n\n\n1\n2\n53\nF\nother\n94043\n\n\n2\n3\n23\nM\nwriter\n32067\n\n\n3\n4\n24\nM\ntechnician\n43537\n\n\n4\n5\n33\nF\nother\n15213\n\n\n...\n...\n...\n...\n...\n...\n\n\n938\n939\n26\nF\nstudent\n33319\n\n\n939\n940\n32\nM\nadministrator\n02215\n\n\n940\n941\n20\nM\nstudent\n97229\n\n\n941\n942\n48\nF\nlibrarian\n78209\n\n\n942\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n943 rows × 5 columns\nratings = pd.read_csv('./ml-100k/u.data', sep='\\t', names = ['uid', 'mid', 'rating', 'timestamp'])\nratings\n\n\n  \n    \n\n\n\n\n\n\nuid\nmid\nrating\ntimestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n...\n...\n...\n...\n...\n\n\n99995\n880\n476\n3\n880175444\n\n\n99996\n716\n204\n5\n879795543\n\n\n99997\n276\n1090\n1\n874795795\n\n\n99998\n13\n225\n2\n882399156\n\n\n99999\n12\n203\n3\n879959583\n\n\n\n\n\n100000 rows × 4 columns\nratings.rating.value_counts()\n\n4    34174\n3    27145\n5    21201\n2    11370\n1     6110\nName: rating, dtype: int64\na_pivot = pd.pivot_table(ratings, values = 'rating', columns = 'mid', index = 'uid', fill_value = -1)\na_pivot\n\n\n  \n    \n\n\n\n\n\nmid\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n1673\n1674\n1675\n1676\n1677\n1678\n1679\n1680\n1681\n1682\n\n\nuid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n5\n3\n4\n3\n3\n5\n4\n1\n5\n3\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n2\n4\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n2\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n3\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n4\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n5\n4\n3\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n939\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n5\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n940\n-1\n-1\n-1\n2\n-1\n-1\n4\n5\n3\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n941\n5\n-1\n-1\n-1\n-1\n-1\n4\n-1\n-1\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n942\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n943\n-1\n5\n-1\n-1\n-1\n-1\n-1\n-1\n3\n-1\n...\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\n\n\n\n943 rows × 1682 columns\na = a_pivot.to_numpy()\na = torch.tensor(a).float()\na\n\ntensor([[ 5.,  3.,  4.,  ..., -1., -1., -1.],\n        [ 4., -1., -1.,  ..., -1., -1., -1.],\n        [-1., -1., -1.,  ..., -1., -1., -1.],\n        ...,\n        [ 5., -1., -1.,  ..., -1., -1., -1.],\n        [-1., -1., -1.,  ..., -1., -1., -1.],\n        [-1.,  5., -1.,  ..., -1., -1., -1.]])\na.numel()\n\n1586126\n(a==(-1)).sum()\n\ntensor(1486126)\na.shape\n\ntorch.Size([943, 1682])\nk = 20\nu = torch.randn(a.shape[0], k, requires_grad=True)\nv = torch.randn(k, a.shape[1],  requires_grad=True)\nu.shape, v.shape\n\n(torch.Size([943, 20]), torch.Size([20, 1682]))\noptim = torch.optim.SGD([u, v], lr = 0.5)\nloss_fn = torch.nn.MSELoss()\nepoch = 1000\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = u @ v\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%100==0:\n    print(f'epoch {ep}: rmse loss {loss.detach().sqrt().item():0.4f}')\n\nepoch 0: rmse loss 5.8354\nepoch 100: rmse loss 4.8373\nepoch 200: rmse loss 4.3531\nepoch 300: rmse loss 4.0140\nepoch 400: rmse loss 3.6677\nepoch 500: rmse loss 3.2592\nepoch 600: rmse loss 2.8526\nepoch 700: rmse loss 2.5214\nepoch 800: rmse loss 2.2663\nepoch 900: rmse loss 2.0665"
  },
  {
    "objectID": "SVD_movielens100k.html#activesgd",
    "href": "SVD_movielens100k.html#activesgd",
    "title": "SVD",
    "section": "activeSGD",
    "text": "activeSGD\n\n!wget https://raw.githubusercontent.com/davoodwadi/active-lr/main/optimizers/ActiveSGD.py\n\n--2023-09-25 12:23:21--  https://raw.githubusercontent.com/davoodwadi/active-lr/main/optimizers/ActiveSGD.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4148 (4.1K) [text/plain]\nSaving to: ‘ActiveSGD.py’\n\nActiveSGD.py          0%[                    ]       0  --.-KB/s               ActiveSGD.py        100%[===================&gt;]   4.05K  --.-KB/s    in 0s      \n\n2023-09-25 12:23:21 (43.4 MB/s) - ‘ActiveSGD.py’ saved [4148/4148]\n\n\n\n\nfrom ActiveSGD import ActiveSGD\n\n\nk = 200\nu = torch.randn(a.shape[0], k, requires_grad=True)\nv = torch.randn(k, a.shape[1],  requires_grad=True)\nu.shape, v.shape\n\n(torch.Size([943, 200]), torch.Size([200, 1682]))\n\n\n\nstepSize = 1\noptim = ActiveSGD([u, v], stepSize=stepSize, lr = 0.5)\nloss_fn = torch.nn.MSELoss()\nepoch = 1000\nmask = a &gt;= 0\nfor ep in range(epoch):\n  a_hat = u @ v\n  loss = loss_fn(a_hat[mask], a[mask])\n  loss.backward()\n  optim.step()\n  optim.zero_grad()\n  if ep%100==0:\n    print(f'epoch {ep}: rmse loss {loss.detach().sqrt().item():0.4f}')\n\nepoch 0: rmse loss 14.6202\nepoch 100: rmse loss 0.3853\nepoch 200: rmse loss 0.1878\nepoch 300: rmse loss 0.1045\nepoch 400: rmse loss 0.0629\nepoch 500: rmse loss 0.0414\nepoch 600: rmse loss 0.0290\nepoch 700: rmse loss 0.0211\nepoch 800: rmse loss 0.0159\nepoch 900: rmse loss 0.0123\n\n\n\nwith torch.no_grad():\n  a_hat = u @ v\na_hat\n\ntensor([[ 5.0043e+00,  2.9998e+00,  4.0002e+00,  ..., -2.2038e+00,\n          4.1306e-03,  6.7976e+00],\n        [ 3.9989e+00,  4.1398e+00, -7.4116e-02,  ..., -5.7194e+00,\n          9.1199e+00, -1.6044e+01],\n        [ 4.5649e+00, -1.6729e+01, -3.5799e+00,  ...,  1.4431e+01,\n         -2.1896e-01, -9.5088e+00],\n        ...,\n        [ 4.9977e+00, -9.2435e-01,  2.1641e+01,  ..., -1.7008e+01,\n          8.9838e+00, -1.4964e+01],\n        [ 4.7857e+00, -6.4135e+00,  5.1219e+00,  ...,  1.3930e+00,\n         -3.8581e+00, -1.4237e+01],\n        [ 6.6980e+00,  5.0004e+00,  5.6941e+00,  ...,  1.2654e+01,\n          1.2119e+01,  1.2553e+01]])"
  },
  {
    "objectID": "code/DistributedDataParallel.html",
    "href": "code/DistributedDataParallel.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To train the CIFAR-10 dataset using PyTorch’s DistributedDataParallel, we need to perform the following steps:\n\nImport the necessary libraries:\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nimport torch.multiprocessing as mp\nimport torchvision\nimport torchvision.transforms as transforms\n\n\nDefine the model architecture:\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 16 * 16, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(-1, 32 * 16 * 16)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CNN()\n\n\nDefine the training function:\n\n\ndef train(rank, world_size):\n    torch.manual_seed(0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize distributed training\n    dist.init_process_group(backend='nccl', init_method='env://', rank=rank, world_size=world_size)\n\n    # Load CIFAR-10 dataset\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n                                        transform=transforms.ToTensor())\n    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset, num_replicas=world_size, rank=rank)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=False, sampler=train_sampler)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n    # Transfer model to GPU\n    model.to(device)\n\n    # Create DistributedDataParallel model\n    model = DistributedDataParallel(model)\n\n    # Training loop\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        train_correct = 0\n        total = 0\n\n        for inputs, labels in trainloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Compute metrics\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            train_loss += loss.item()\n\n        # Print epoch results\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {(train_correct/total)*100:.2f}%\")\n\n    # Cleanup\n    dist.destroy_process_group()\n\n\nDefine the main function for multi-processing:\n\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == '__main__':\n    main()\n\n\nRun the main function to start the training process:\n\n\npython train_cifar10_distributed.py\n\nThis code will train a convolutional neural network (CNN) on the CIFAR-10 dataset using PyTorch’s DistributedDataParallel. The DistributedDataParallel module enables us to train models on multiple GPUs or machines.\nThe CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat, etc.). The dataset is divided into 50,000 training images and 10,000 testing images.\nIn the code above, we define a CNN model with two convolutional layers and two fully connected layers. We then load the CIFAR-10 dataset, define the loss function and optimizer, and transfer the model to the GPU if available.\nWe create a DistributedDataParallel model from the base model to enable distributed training. The DistributedDataParallel wrapper will automatically scatter the input data to the available GPUs and gather the gradients during the backward pass.\nNext, we define the training loop, where we iterate over the batches of the training dataset. For each batch, we compute the forward pass, the loss, and perform the backward pass and optimization. We also keep track of the training loss and accuracy.\nAfter each epoch, we print the epoch’s training loss and accuracy. Finally, we clean up the distributed training environment and destroy the process group.\nTo run the code, we use the mp.spawn() function to parallelize the training process across multiple processes, where each process trains on a separate GPU. The world_size parameter is set to the number of available GPUs."
  },
  {
    "objectID": "code/value iteration.html",
    "href": "code/value iteration.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "The Value Iteration Algorithm is a method used in reinforcement learning to find the optimal value function for a given Markov Decision Process (MDP). It is an iterative algorithm that aims to converge to the optimal value function by iteratively updating the values of states based on the Bellman Optimality Equation.\nThe Bellman Optimality Equation for a state s in an MDP is given by:\n\\(V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^*(s')]\\)\nwhere: - \\(V^*(s)\\) is the optimal value function for state s. - \\(\\max_a\\) represents the selection of the action that maximizes the equation. - \\(P(s'|s,a)\\) is the probability of transitioning to state s’ from state s given action a. - \\(R(s,a,s')\\) is the reward for transitioning from state s to state s’ using action a. - \\(\\gamma\\) is the discount factor that determines the importance of future rewards.\nThe algorithm starts with an initial estimate of the optimal value function and iteratively updates the values of states until convergence. The value of each state is updated by selecting the action that maximizes the equation.\nNow, let’s implement the Value Iteration Algorithm in Python:\n\nimport numpy as np\n\ndef value_iteration(env, gamma, epsilon):\n    # Initialize the value function\n    V = np.zeros(env.n_states)\n    \n    # Iterate until convergence\n    while True:\n        delta = 0\n        # Update the value of each state\n        for s in range(env.n_states):\n            # Find the maximum Q-value for the state\n            max_value = -np.inf\n            for a in range(env.n_actions):\n                q_value = 0\n                # Calculate the Q-value for each action\n                for s_prime in range(env.n_states):\n                    transition_prob = env.transition_probabilities[s,a,s_prime]\n                    reward = env.rewards[s,a,s_prime]\n                    q_value += transition_prob * (reward + gamma * V[s_prime])\n                max_value = max(max_value, q_value)\n            # Update the value function\n            delta = max(delta, np.abs(max_value - V[s]))\n            V[s] = max_value\n        \n        # Check for convergence\n        if delta &lt; epsilon:\n            break\n    \n    return V\n\nLet’s go through the code step by step:\n\nThe code starts by importing the necessary libraries, including numpy for numerical operations.\nThe value_iteration function takes three arguments: env (the environment), gamma (the discount factor), and epsilon (the convergence threshold).\nWe initialize the value function V as an array of zeros with the same size as the number of states in the environment.\nThe algorithm enters a loop that continues until convergence.\nInside the loop, we set the delta variable to 0. delta is used to keep track of the maximum change in the value function during an iteration.\nWe iterate over all the states in the environment.\nFor each state, we iterate over all the possible actions.\nFor each action, we calculate the Q-value using nested loops. The Q-value is calculated as the sum of the transition probabilities multiplied by the sum of the rewards and the discounted value of the next state.\nWe update the max_value variable if the current Q-value is greater than the current maximum value.\nAfter all actions have been considered, we update the value of the state in the value function array.\nWe update the delta variable with the maximum change in any state during the iteration.\nWe check if the delta is less than the convergence threshold epsilon. If it is, the algorithm breaks out of the loop and returns the optimal value function.\nIf the convergence condition is not met, the algorithm continues to the next iteration.\n\nThat’s it! The value_iteration function returns the optimal value function for the given MDP. This value function can be used to determine the optimal policy for taking actions in the environment."
  },
  {
    "objectID": "code/policy iteration.html",
    "href": "code/policy iteration.html",
    "title": "Policy Iteration Algorithm",
    "section": "",
    "text": "In reinforcement learning, the policy iteration algorithm is used to find the optimal policy for an agent to maximize its expected return in a Markov Decision Process (MDP). The algorithm consists of two main steps: policy evaluation and policy improvement.\n\nPolicy Evaluation: In this step, we evaluate the given policy by calculating the state-value function for each state. The state-value function, denoted as V(s), represents the expected return starting from state s and following the given policy. The Bellman equation for V(s) is given by:\n\n\\(V(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s', r|s, a) [r + \\gamma V(s')]\\)\nwhere:\n\n\\(\\pi(a|s)\\) is the probability of taking action a in state s according to the policy \\(\\pi\\).\n\\(p(s',r|s,a)\\) is the probability of transitioning to state s’ and receiving reward r, given that the agent took action a in state s.\n\\(\\gamma\\) is the discount factor, which determines the importance of future rewards compared to immediate rewards.\n\nThe policy evaluation step involves iteratively updating the state-value function until it converges to its true value under the given policy.\n\nPolicy Improvement: Once the state-value function is updated, we can improve the policy by selecting the action that maximizes the expected return in each state. This can be done by using the greedy policy improvement rule:\n\n\\(\\pi'(s) = \\arg\\max_{a} \\sum_{s', r} p(s', r|s, a) [r + \\gamma V(s')]\\)\nwhere \\(\\pi'(s)\\) is the new policy, and \\(V(s')\\) is the updated state-value function.\nThe policy improvement step involves updating the policy based on the updated state-value function from the previous step.\nThis process of policy evaluation and policy improvement is repeated iteratively until the policy converges to the optimal policy.\nNow let’s see an example of how to implement the policy iteration algorithm in Python:\n\n# Step 1: Policy Evaluation\ndef policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta):\n    # Initialize the state-value function with zeros\n    V = {s: 0 for s in states}\n\n    while True:\n        delta = 0\n        \n        # Iterate over all states\n        for s in states:\n            v = V[s]\n            new_v = 0\n            \n            # Iterate over all possible actions\n            for a in actions:\n                # Compute the expected return for each action\n                next_states = transitions[s][a]\n                expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n\n                # Compute the new state-value function\n                new_v += policy[s][a] * expected_return\n\n            # Update the state-value function for the current state\n            V[s] = new_v\n\n            # Calculate the maximum difference between the old and new state-value functions\n            delta = max(delta, abs(v - V[s]))\n        \n        # If the maximum difference is less than a threshold, we assume convergence\n        if delta &lt; theta:\n            break\n    \n    return V\n\n# Step 2: Policy Improvement\ndef policy_improvement(states, actions, rewards, transitions, discount_factor, V):\n    # Initialize the new policy with zeros\n    new_policy = {s: {a: 0 for a in actions} for s in states}\n    \n    for s in states:\n        action_values = []\n        \n        # Iterate over all possible actions\n        for a in actions:\n            # Compute the expected return for each action\n            next_states = transitions[s][a]\n            expected_return = sum(prob * (reward + discount_factor * V[next_state]) for next_state, reward, prob in next_states)\n            \n            # Add the expected return to the list of action values\n            action_values.append(expected_return)\n        \n        # Assign the action with the maximum expected return as the new policy for the current state\n        idx = np.argmax(action_values)\n        new_policy[s][actions[idx]] = 1\n    \n    return new_policy\n\n# Step 3: Policy Iteration\ndef policy_iteration(states, actions, rewards, transitions, discount_factor, theta):\n    # Initialize a random policy\n    policy = {s: {a: 1 / len(actions) for a in actions} for s in states}\n\n    while True:\n        # Policy Evaluation\n        V = policy_evaluation(policy, states, actions, rewards, transitions, discount_factor, theta)\n        \n        # Policy Improvement\n        new_policy = policy_improvement(states, actions, rewards, transitions, discount_factor, V)\n        \n        # If the new policy is the same as the old policy, we have converged to the optimal policy\n        if policy == new_policy:\n            break\n        \n        policy = new_policy\n    \n    return policy\n\nIn the code above, we define three functions to implement the policy iteration algorithm:\n\npolicy_evaluation: This function performs policy evaluation by iteratively updating the state-value function until it converges. It takes as input the policy, state and action spaces, rewards, transition probabilities, discount factor, and a convergence threshold.\npolicy_improvement: This function improves the policy by selecting the action that maximizes the expected return in each state. It takes as input the state and action spaces, rewards, transition probabilities, discount factor, and the updated state-value function.\npolicy_iteration: This function combines the policy evaluation and policy improvement steps to find the optimal policy. It iteratively updates the policy until it converges to the optimal policy.\n\nTo apply the policy iteration algorithm to a specific problem, you need to provide the state and action spaces, rewards, transition probabilities, discount factor, and convergence threshold.\nNow you can use the policy_iteration function to find the optimal policy for a given MDP."
  }
]