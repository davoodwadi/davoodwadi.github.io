[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "I’m a researcher and educator with a focus on Digital Marketing and the application of Artificial Intelligence in Marketing.\nI currently teach the following courses:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Courses",
    "section": "About this blog",
    "text": "About this blog\nI currently teach the following courses:"
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "Davood Wadi",
    "section": "Courses",
    "text": "Courses\n+ MIT: *Applied Data Science Program*\n+ HEC Montreal: *MATH60629A - Machine Learning I*"
  },
  {
    "objectID": "index.html#experiences",
    "href": "index.html#experiences",
    "title": "Davood Wadi",
    "section": "Experiences",
    "text": "Experiences\n\nPhD in Marketing - HEC Montreal (Best 2023 Thesis Award Nominee)\nPartner with intelChain.io as AI Scientist"
  },
  {
    "objectID": "courses.html#course-outline",
    "href": "courses.html#course-outline",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Davood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT\n\nFinal Exam\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated"
  },
  {
    "objectID": "courses.html#course-information",
    "href": "courses.html#course-information",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "courses.html#evaluations",
    "href": "courses.html#evaluations",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "Evaluations",
    "text": "Evaluations\n\n[Bonus] Class participation (5%)\nTo receive this 5% bonus point:\n\nPrepare for each session’s code before the class starts.\nLog in to deepnote with your Full Name.\nDuring the coding exercises in class, add your answer to the questions as a comment.\n\n\n\nHomework (20%)\n\nDue on 24 October 2023\nHomework assignments are counted for 20% of your final grade.\nThe homework should be done in jupyter notebook here or Google Colab here.\nInstructions:\n\nThe homework is due by 11:59PM EST on the due date.\nPlease upload a PDF version of your assignment on ZoneCours and insert the shareable link to your notebook in the same interface.\nHomeworks must be completed individually.\n\nDiscussion with others is okay, but you must write solutions yourself.\n\nAll code used to arrive at answers is submitted along with answers.\nNotes:\n\nPlease provide your code answers in the code block under each question and verbal answers in text boxes assigned in the notebook (where applicable).\nPlease run the notebook before the submission so that the outputs are displayed.\nPlease make sure that your results are reproducible. You may use random seeds from random and numpy packages. For scikit-learn modules, you may use the random_state argument.\n\n\n\n\n\nProject (30%)\nThe aim of this project is to allow you to learn about machine learning by trying to solve a task with it.\nFirst, select a question that can be answered using machine learning. I expect that your question will be about a model/algorithm or about an application. Then design a study that will try to answer your question. Your study must have an element of novelty. For example the novelty could be an extension or a variation of an existing algorithm or results of an existing method on a new dataset.\nYour study should involve reading and understanding some background material. Your study must involve running some experiments. You are free to use (or not) any of the tools or models we have seen in class.\nStudy plan: (1 upload per group) Please submit a one-page summary of your proposed research question and study to ZoneCours. I will meet with each group to discuss study plans during the lecture on Week 9. I will send you a schedule the day before. We will probably only have about 15 minutes so please make sure that your study plan is clear and precise. You may also include questions that you would like us to discuss at the end of the document.\nThe group report: (1 upload per group) Your report must contain a description of the question you are trying to answer, a clear description of the model/algorithm you are studying, a survey of related work which proper references, an empirical section that reports your results, and a conclusion that summarizes your findings and (if pertinent) highlights possible future directions of investigation. Your report should be no longer than 10 pages in length (plus references) for pairs or 13 pages (plus references) for teams of three.\nThe individual report: (1 upload per student) You will also submit a brief individual report (at most one page), which will: (1) Describe the parts of the project you worked on (which machine learning methods you applied, which preprocessing steps you performed on the data, which parts of the term paper you wrote, who you worked with on what parts, etc.) and what parts of the project your teammates worked on. (2) What you learned from the project. The purpose of the individual report is to facilitate fair grading and to allow the instructor to understand well what you learned from the project.\nProject Report (30%)\n\nClarity/Relevance of problem statement and description of approach: 10%\nDiscussion of relationship to previous work and references: 4%\nDesign and execution of experiments: 10%\nFigures/Tables/Writing: easily readable, properly labeled,\ninformative: 5%\nIndiviual report: 1%\n\n\nProject Topic\nConcretely, your project could take one of the following forms:\n\nA competition on kaggle.com that you try multiple ML methods on and can achieve a validation set performance in the top 100 on the leaderboard.\nA benchmark task on paperswithcode (e.g. Speech Recognition) where you try multiple methods and compare their performance.\nA task in your own field whereby you apply multiple ML methods to solve the task.\nAn ML method that you apply to a new task (e.g. Multi-head attention applied to time-series prediction)\n\nNote. The list above is not exhaustive. You are encouraged to be creative about the topic. Pick a topic that you find interesting and is relevant. The appropriateness of the topic will be discussed and resolved in the session on Study plan.\n\n\nGroup Report Structure\nYour group report should read like a published paper. Similarly, your poster presentation should have a similar structure to conference posters. You might want to check out similar papers to your topic on the structure of your report (e.g. A Comparison of Deep Learning Approach for Underwater Object Detection).\nRegardless of your topic, as a general rule of thumb, your Group Report should be structured as follows:\n\nIntroduction: Use this section to introduce the topic, its relevance, and a summary of the whole paper. This section should address the question, “Why the target audience of the paper should care about your work?”\nRelated Work: This section discusses what has been done, in prior research, on your topic. While most of the cited papers should be from academic publications, it is not uncommon to see non-academic references in papers (e.g. stackoverflow). You might want to use academic search engines (e.g. SemancticScholar) to find relavant papers.\nExperiments This section should cover the steps you take to answer the question you posed in the Introduction. This includes data preparation, model and hyper-parameter selection, and evaluation steps.\nResults In this section you present the resutls of your experiments with appropriate tables and figures.\nConclusion In your final section, you would summarize what you did in the paper, acknowledge the limitations of your work (All papers have this; no work is perfect.), suggest future research directions to address your limitations.\n\n\n\nTimeline\n\nTeam Registration, due: October 1. Fill this form.\nStudy plan, due: October 28 (by the end of the day EDT).\nHanding in: Through ZoneCours\nProject meeting, October 31\nProject Presentation, due: December 1. Upload the PDF of your poster/slides to ZoneCours.\nIn-class Presentation, on December 5.\nFinal individual report, due: December 15, 2023 (by the end of the day EDT).\n\nHanding in: Through ZoneCours (per each team member).\n\n\n\n\n\nProject Presentation (10%)\nMake a poster that describes your project [You do not need to print your poster]. You can think of a poster as supporting material for your oral presentation (in that way it is similar to slides). It could also follow a similar structure: begin by motivating your work, then (quickly) highlight related work, talk in depth about your solution, then go into results (pictures and tables are good tools for that), finally conclude and perhaps mention one or two ideas for future work.\n\nProject Presentation (10%)\n\nClarity of presentation: 3%\nSlide or Poster quality: 2%\nCorrectness: 2%\nAnswers to questions: 3%\n\n\n\n\nFinal Exam (30%)\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated\n\nPast exam - Fall 2018\n\n\nCapsule Quizzes (10%)\n\nIn-class quizzes of the capsules\nCapsule quizzes are counted for 10% of your final grade.\nTime to complete each quiz is 10 minutes.\n\nQuiz 0, 20 September 11:30 AM to 22 September 11:30 AM.\nQuiz 1, 27 September 11:30 AM to 29 September 11:30 AM.\nQuiz 2, TBA.\nQuiz 3, TBA.\nQuiz 4, TBA.\nQuiz 5, TBA.\nQuiz 6, TBA."
  },
  {
    "objectID": "docs/MATH60629A_Homework.html",
    "href": "docs/MATH60629A_Homework.html",
    "title": "Instructions:",
    "section": "",
    "text": "Due date: October 24, 2023\n# enter you full name and HEC ID\nfull_name = \"\"\nHEC_ID = \"\""
  },
  {
    "objectID": "docs/MATH60629A_Homework.html#svm-for-classfication",
    "href": "docs/MATH60629A_Homework.html#svm-for-classfication",
    "title": "Instructions:",
    "section": "SVM for classfication",
    "text": "SVM for classfication\n\n(1pt) Train a linear SVM on the training set for each one of these C hyperparameter values: {0:001; 0:01; 0:1; 1; 10}. Find the best hyperparameter on the validation set.\n\n\n[Your code here]\n\n\n(1pt) Using the best hyperparameter C, evaluate the accuracy, precision, recall, and F1-score on the test set.\n\n\n[Your code here]\n\n\n(0.5pt) Plot the confusion matrix on the test set and explain the reason for your false negatives/positives.\n\n\n[Your code here]\n\nObservations\n[_____]"
  },
  {
    "objectID": "MATH60629A_Homework.html",
    "href": "MATH60629A_Homework.html",
    "title": "Instructions:",
    "section": "",
    "text": "Due date: October 24, 2023\n# enter you full name and HEC ID\nfull_name = \"\"\nHEC_ID = \"\""
  },
  {
    "objectID": "MATH60629A_Homework.html#svm-for-classfication",
    "href": "MATH60629A_Homework.html#svm-for-classfication",
    "title": "Instructions:",
    "section": "SVM for classfication",
    "text": "SVM for classfication\n\n(1pt) Train a linear SVM on the training set for each one of these C hyperparameter values: {0:001; 0:01; 0:1; 1; 10}. Find the best hyperparameter on the validation set.\n\n\n[Your code here]\n\n\n(1pt) Using the best hyperparameter C, evaluate the accuracy, precision, recall, and F1-score on the test set.\n\n\n[Your code here]\n\n\n(0.5pt) Plot the confusion matrix on the test set and explain the reason for your false negatives/positives.\n\n\n[Your code here]\n\nObservations\n[_____]"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "sessions.html#session-materials",
    "href": "sessions.html#session-materials",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "courses.html#in-class-tools",
    "href": "courses.html#in-class-tools",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "In-class Tools",
    "text": "In-class Tools\n\nCups pacing webapp"
  },
  {
    "objectID": "code/Session1.html",
    "href": "code/Session1.html",
    "title": "Python Basics",
    "section": "",
    "text": "To start let’s load NumPy, a package for scientific computing with Python. We usually load it as the np shorthand.\n\nimport numpy as np\n\nLet’s initialize two arrays\n\na = np.random.randn(1,5)\nb = np.random.randn(1,5)"
  },
  {
    "objectID": "code/Session1.html#loading-necessary-packages",
    "href": "code/Session1.html#loading-necessary-packages",
    "title": "Python Basics",
    "section": "",
    "text": "To start let’s load NumPy, a package for scientific computing with Python. We usually load it as the np shorthand.\n\nimport numpy as np\n\nLet’s initialize two arrays\n\na = np.random.randn(1,5)\nb = np.random.randn(1,5)"
  },
  {
    "objectID": "code/Session1.html#dot-product",
    "href": "code/Session1.html#dot-product",
    "title": "Python Basics",
    "section": "Dot product",
    "text": "Dot product\nTo perform a dot product of two arrays, the shape of the arrays should match. Let’s get the shape of our arrays.\n\na.shape, b.shape\n\n((1, 5), (1, 5))\n\n\nboth arrays have a shape of \\((1, 5)\\).\nIn general, the shape of the two arrays should be \\((n, k) (k, m)\\), where \\(k\\) is the common dimension of the two arrays.\nFor our example, we can transpose the array b to make the shapes match. The NumPy command for transposition of arrays is .T.\n\na.shape, b.T.shape\n\n((1, 5), (5, 1))\n\n\nNow that the two arrays have the matching shapes, we can calculate their dot product using the @ operator.\n\na @ b.T\n\narray([[0.56077435]])\n\n\nThe result of the dot product is an array of shape \\((1,1)\\), or a scalar. In general, the result of a dot product has a shape of \\((n, m)\\)"
  },
  {
    "objectID": "code/Session1.html#best-fit-line",
    "href": "code/Session1.html#best-fit-line",
    "title": "Python Basics",
    "section": "Best fit line",
    "text": "Best fit line\nLet us now simulate a some data and find the best fit line, a line that minimizes the average distance of all data points to the line.\nHere our X is a vector of shape \\((100, 1)\\) samples from a standard normal distribution. Here we have \\(100\\) points with \\(1\\) feature for each point.\nThe y is 10 times x, with the shape \\((100, 1)\\).\n\\[y = X \\cdot coeff\\]\n\nX = np.random.randn(100, 1)\ncoeff = np.ones((1, 1)) * 10\ny = X @ coeff\ny.shape\n\n(100, 1)"
  },
  {
    "objectID": "code/Session1.html#fitting-the-data-using-sklearn",
    "href": "code/Session1.html#fitting-the-data-using-sklearn",
    "title": "Python Basics",
    "section": "Fitting the data using sklearn",
    "text": "Fitting the data using sklearn\nIn the next session, we will formulate a closed-form solution for finding the parameters of linear regression.\nFor now, let’s use the scikit-learn package to find the best line.\nWe’ll import the LinearRegression class from the linear_model submodule of sklearn. Each submodule of sklearn contains classes for that particular topic.\n\nfrom sklearn.linear_model import LinearRegression\n\nNow, let’s create an instance of the LinearRegression model and fit it to the data.\n\nmodel = LinearRegression()\n\n\nmodel.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nMost sklearn classes involve two steps: 1. Instantiating the class. This is where you provide the necessary hyper-parameters. - model = LinearRegression() 2. Fitting the data. - model.fit(X,y)\nNext, we can retrieve the coefficients of the fitted model by accessing the .coeff_ attribute of the model object.\n\na = model.coef_\na\n\narray([[10.]])\n\n\nIn our case, the coefficient is an array of shape \\((1,1)\\). This is in line with the shape of our X (\\(n,1\\)) and y (\\(n, 1\\)).\nWe can also access the intercept of the model using the .intercept_ attribute. Since we didn’t add an intercept in our data simulation, the value of the intercept should be \\(0\\).\nNote. In numerical computations, very small numbers (e.g. \\(2 \\times 10^{-16}\\)) are considered to be \\(0\\).\n\nb = model.intercept_\nb\n\narray([2.22044605e-16])"
  },
  {
    "objectID": "code/Session1.html#plotting-the-line-on-the-data",
    "href": "code/Session1.html#plotting-the-line-on-the-data",
    "title": "Python Basics",
    "section": "Plotting the line on the data",
    "text": "Plotting the line on the data\nWe can now plot the line and the data to see the fit.\nFirst, we get the predictions from the model using its fitted parameters a and b.\n\ny_pred = X @ a + b\n\nWe’ll use seaborn and Matplotlib to create two separate plots on the same axis, ax.\n\nax = sns.scatterplot(x=X[:,0], y=y[:,0], label=\"Actual y\")\nax = sns.lineplot(x=X[:,0], y=y_pred[:,0], ax=ax, color='red', label=\"Predicted y\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x120000280&gt;"
  },
  {
    "objectID": "code/Model.html",
    "href": "code/Model.html",
    "title": "Model capacity",
    "section": "",
    "text": "Model capacity refers to the ability of a machine learning model to capture and represent complex relationships between the input variables (features) and the target variable (labels). It determines the complexity and flexibility of the model in fitting the training data.\nIn other words, model capacity represents the amount of information or patterns that a model can learn from the data. A model with a high capacity can learn intricate relationships in the training data, which may result in overfitting. On the other hand, a model with low capacity may not be able to capture the underlying patterns in the data, leading to underfitting.\nThe capacity of a model can be controlled by adjusting its architectural complexity. For example, in neural networks, increasing the number of hidden layers and hidden units increases the capacity of the model.\nMathematically, we can define model capacity as the number of parameters that the model has to learn. For example, in linear regression, the model capacity is determined by the number of coefficients (slope and intercept) that the model needs to estimate. In neural networks, the model capacity is determined by the number of weights and biases associated with each neuron.\nNow, let’s understand the concept of model capacity using a simple example with polynomial regression. Polynomial regression is a form of linear regression where the relationship between the input feature (x) and the target variable (y) is modeled as an nth-degree polynomial.\nFirst, let’s generate some synthetic data that follows a quadratic relationship between x and y:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-5, 5, 100)\ny = 2 * x ** 2 + np.random.normal(0, 4, 100)\n\n# Plot the data\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Quadratic Relationship')\nplt.show()\n\n\n\n\nBy visualizing the data, we can observe that the relationship between x and y follows a quadratic curve. Now, let’s try fitting this data using different polynomial regression models with different degrees of complexity.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Fit linear regression model\nlinear_model = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())\nlinear_model.fit(x.reshape(-1, 1), y)\n\n# Fit quadratic regression model\nquadratic_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\nquadratic_model.fit(x.reshape(-1, 1), y)\n\n# Fit cubic regression model\ncubic_model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\ncubic_model.fit(x.reshape(-1, 1), y)\n\n# Predict on new data points\nx_test = np.linspace(-5, 5, 100)\ny_linear = linear_model.predict(x_test.reshape(-1, 1))\ny_quadratic = quadratic_model.predict(x_test.reshape(-1, 1))\ny_cubic = cubic_model.predict(x_test.reshape(-1, 1))\n\n# Plot the regression curves\nplt.scatter(x, y, label='Data')\nplt.plot(x_test, y_linear, label='Linear')\nplt.plot(x_test, y_quadratic, label='Quadratic')\nplt.plot(x_test, y_cubic, label='Cubic')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Polynomial Regression')\nplt.legend()\nplt.show()\n\n\n\n\nIn the above code, we fit three polynomial regression models: linear, quadratic, and cubic regression. The degree of the polynomial is indicated by the degree parameter in the PolynomialFeatures class. We can observe that as we increase the complexity (degree) of the polynomial, the models can better capture the underlying quadratic relationship.\nHowever, it’s important to note that high-capacity models can also be prone to overfitting, especially when the amount of training data is limited. Therefore, it’s crucial to balance the model’s capacity with the complexity of the problem at hand and the size of the training dataset.\nTo summarize, model capacity refers to the amount of information or patterns a machine learning model can learn from the data. It is determined by the number of parameters that the model needs to estimate. Increasing the model’s capacity can improve its ability to represent complex relationships in the data, but it can also increase the risk of overfitting."
  },
  {
    "objectID": "code/overfi.html",
    "href": "code/overfi.html",
    "title": "Overfitting",
    "section": "",
    "text": "Overfitting occurs when a machine learning model performs very well on the training data, but fails to generalize well on unseen data. It happens when the model captures noise and random fluctuations in the training data instead of the underlying pattern or relationship.\nOne way to understand overfitting is to consider fitting a polynomial to data points. The degree of the polynomial determines its complexity. A higher degree polynomial can fit the training data more closely, but it may also capture random noise, resulting in poor performance on new data.\nTo demonstrate overfitting using polynomials, we will generate a dataset with some noise and fit polynomials of different degrees to it. We will then visualize the models to see how they fit the data.\nLet’s start by importing the necessary libraries and generating the dataset. We will use the numpy library for array operations and random number generation, and the matplotlib library for data visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the dataset\nnp.random.seed(0)\nX = np.linspace(-1, 1, 20)\ny = 2 * X + np.random.normal(0, 0.5, 20)\n\nIn the above code, we first import the required libraries: numpy and matplotlib.pyplot. We then set a random seed to ensure reproducibility.\nNext, we create an array X with 20 equally spaced points between -1 and 1 using the linspace function. We add some noise to the array y using the numpy.random.normal function. Here, we use a linear relationship with some Gaussian noise to generate our dataset.\nNow, we will plot the generated dataset to visualize it.\n\nplt.scatter(X, y)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Dataset')\nplt.show()\n\n\n\n\nThe code above uses the scatter function from matplotlib.pyplot to create a scatter plot of the dataset. It also adds labels to the x and y axes and sets a title for the plot. Finally, the show function is called to display the plot.\nNow, let’s fit polynomials of different degrees to the dataset and see how they fit the data.\n\n# Polynomial fitting and visualization\ndegrees = [1, 3, 9, 12]\n\nplt.scatter(X, y)\n\nfor degree in degrees:\n    # Fit polynomial of given degree\n    coeffs = np.polyfit(X, y, degree)\n    poly = np.poly1d(coeffs)\n    \n    # Generate x values for plotting\n    x_plot = np.linspace(-1, 1, 100)\n    \n    # Compute predicted y values\n    y_plot = poly(x_plot)\n    \n    # Plot the polynomial\n    plt.plot(x_plot, y_plot, label=f'Degree {degree}')\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Fitting Polynomials')\nplt.legend()\nplt.show()\n\n\n\n\nIn the above code, we define a list degrees with the degrees of the polynomials that we want to fit to the dataset. We then iterate over each degree and perform the following steps:\n\nFit a polynomial of the given degree to the dataset using the polyfit function from numpy.\nCreate a polynomial object using the computed coefficients using the poly1d function from numpy.\nGenerate a set of x values for plotting using the linspace function from numpy.\nCompute the predicted y values for the generated x values using the polynomial.\nPlot the polynomial curve using the plot function from matplotlib.pyplot with a label indicating the degree of the polynomial.\n\nFinally, we add labels and a title to the plot, and display a legend to distinguish the different polynomial curves.\nWhen you run the code, you will see a plot showing the dataset points as scatter points, and different polynomial curves fitted to the data. From this visualization, you can observe the effect of overfitting as the degree of the polynomial increases. Higher degree polynomials tend to fit the training data more closely, but they also capture random noise and fluctuations, resulting in poor generalization to new data."
  },
  {
    "objectID": "code/multip1.html",
    "href": "code/multip1.html",
    "title": "Multiple Linear Regression - Part 1",
    "section": "",
    "text": "In machine learning and statistics, multiple linear regression is a commonly used technique to model the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the independent variables and the dependent variable.\nThe general form of multiple linear regression can be written as:\n\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\\]\nWhere:\nTo perform multiple linear regression, we can use the Ordinary Least Squares (OLS) method, which aims to find the coefficients that minimize the sum of the squared differences between the actual and predicted values."
  },
  {
    "objectID": "code/multip1.html#design-matrix",
    "href": "code/multip1.html#design-matrix",
    "title": "Multiple Linear Regression - Part 1",
    "section": "Design Matrix",
    "text": "Design Matrix\nIn multiple linear regression, we represent the independent variables as a matrix called the design matrix. The design matrix, denoted by \\(X\\), has one row for each observation and one column for each independent variable.\nFor example, if we have n observations and p independent variables, the design matrix \\(X\\) will be an n x p matrix."
  },
  {
    "objectID": "code/multip1.html#proof-of-ols",
    "href": "code/multip1.html#proof-of-ols",
    "title": "Multiple Linear Regression - Part 1",
    "section": "Proof of OLS",
    "text": "Proof of OLS\nThe OLS method minimizes the sum of squared errors (SSE) between the actual dependent variable values and the predicted values. The SSE can be written as:\n\\[SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nWhere:\n\n\\(y_i\\) is the actual dependent variable value for the i-th observation,\n\\(\\hat{y}_i\\) is the predicted dependent variable value for the i-th observation.\n\nTo find the coefficients that minimize SSE, we differentiate SSE with respect to each coefficient and set the derivatives to zero.\nLet’s derive the formula for OLS step by step.\n\nThe predicted value of the dependent variable can be written as:\n\n\\[\\hat{y} = X\\beta\\]\nWhere:\n\n\\(\\hat{y}\\) is an n x 1 vector of predicted values,\n\\(X\\) is the design matrix,\n\\(\\beta\\) is a vector of coefficients.\n\n\nThe SSE can be expressed in matrix form as:\n\n\\[SSE = (\\mathbf{y} - \\mathbf{\\hat{y}})^T (\\mathbf{y} - \\mathbf{\\hat{y}})\\]\nWhere:\n\n\\(\\mathbf{y}\\) is an n x 1 vector of actual dependent variable values,\n\\(\\mathbf{\\hat{y}}\\) is an n x 1 vector of predicted dependent variable values.\n\n\nExpanding the above equation, we get:\n\n\\[SSE = (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)\\]\n\nExpanding the squared term, we get:\n\n\\[SSE = \\mathbf{y}^T\\mathbf{y} - 2\\beta^TX^T\\mathbf{y} + \\beta^TX^TX\\beta\\]\n\nTo minimize SSE with respect to \\(\\beta\\), we differentiate SSE with respect to \\(\\beta\\) and set the derivative to zero:\n\n\\[\\frac{\\partial SSE}{\\partial \\beta} = -2X^T\\mathbf{y} + 2X^TX\\beta = 0\\]\n\nSolving for \\(\\beta\\), we get:\n\n\\[X^TX\\beta = X^T\\mathbf{y}\\]\n\\[\\beta = (X^TX)^{-1}X^T\\mathbf{y}\\]\nThe above formula gives us the optimal values for \\(\\beta\\) that minimize SSE."
  },
  {
    "objectID": "code/multip2.html",
    "href": "code/multip2.html",
    "title": "Multiple Linear Regression - Part 2",
    "section": "",
    "text": "Multiple linear regression is a powerful technique used for predicting a continuous outcome variable based on multiple predictor variables. In this tutorial, we will learn how to perform multiple linear regression using a design matrix in Python.\nFirst, let’s define the problem. In multiple linear regression, we have a dependent variable (also called the response or target variable) and several independent variables (also called features, input variables, or predictors). The goal is to find the best linear relationship between the predictors and the target variable.\nThe general equation for a multiple linear regression model with ‘p’ predictors is given by:\nY = β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ + ε\nWhere: - Y is the target variable - X₁, X₂, …, Xₚ are the predictor variables - β₀, β₁, β₂, …, βₚ are the coefficients (or weights) of the predictors - ε is the error term, representing the randomness or noise in the relationship\nTo estimate the coefficients (β₀, β₁, β₂, …, βₚ), we need to minimize the sum of squared residuals, which measures the differences between the actual values of the target variable (Y) and the predicted values from the regression model.\nIn multiple linear regression, the predictors are often organized into a design matrix (X), where each row represents an observation and each column represents a predictor variable.\nNow let’s see how to perform multiple linear regression using a design matrix in Python.\n\nimport numpy as np\n\n# Define the design matrix X\nX = np.array([[3,  3, -3],\n              [-4, 5,  6],\n              [7, -8,  9]])\n\n# Define the target variable Y\nY = np.array([10, 20, 30])\n\n# Calculate beta coefficients using the normal equation\nbeta = np.linalg.inv(X.T @ X) @ X.T @ Y\n\nprint('Beta coefficients:', beta)\n\nBeta coefficients: [3.56321839 2.98850575 3.2183908 ]\n\n\nIn the above code, we first import the necessary libraries. Then, we define the design matrix X as a 2-dimensional numpy array that contains the predictor variables. Each row represents an observation, and each column represents a predictor variable. We also define the target variable Y as a 1-dimensional numpy array.\nNext, we use the normal equation to calculate the beta coefficients. The normal equation is given by:\n\\[β = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y\\]\n\n\\(X^T\\) is the transpose of X\n\\((X^T \\cdot X)^{-1}\\) is the inverse of the matrix product of \\(X^T\\) and \\(X\\)\n\\(X^T \\cdot Y\\) is the matrix product of \\(X^T\\) and \\(Y\\)\n\nFinally, we print the beta coefficients, which represent the weights or coefficients of the predictors in the multiple linear regression model."
  },
  {
    "objectID": "code/L2 nor.html",
    "href": "code/L2 nor.html",
    "title": "L2 Normalization for Multiple Linear Regression with Design Matrix X",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 normalization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 normalization for multiple linear regression using Python:\n\nStep 1: Importing the Required Libraries\nWe will start by importing the necessary libraries for our implementation.\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\n\nStep 2: Generating Sample Data\nTo demonstrate the L2 normalization for multiple linear regression, we will generate some sample data.\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n\nStep 3: Fitting the Multiple Linear Regression Model\nWe will fit the multiple linear regression model using the OLS method.\n\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n\nStep 4: Fitting the Ridge Regression Model with L2 Normalization\nNow, let’s fit the Ridge regression model with L2 normalization to the data.\n\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 normalization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n\nStep 5: Comparing Results\nFinally, let’s compare the coefficients and MSE of the OLS and Ridge models.\n\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 normalization on the coefficients and model performance.\nBy introducing L2 normalization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\nNote that L2 normalization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 normalization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term."
  },
  {
    "objectID": "code/L2 reg.html",
    "href": "code/L2 reg.html",
    "title": "L2 Regularization for Multiple Linear Regression with Design Matrix X",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 regularization for multiple linear regression using Python:\n\nStep 1: Importing the Required Libraries\nWe will start by importing the necessary libraries for our implementation.\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\n\nStep 2: Generating Sample Data\nTo demonstrate the L2 regularization for multiple linear regression, we will generate some sample data.\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n\nStep 3: Fitting the Multiple Linear Regression Model\nWe will fit the multiple linear regression model using the OLS method.\n\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n\nStep 4: Fitting the Ridge Regression Model with L2 Regularization\nNow, let’s fit the Ridge regression model with L2 regularization to the data.\n\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 regularization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n\nStep 5: Comparing Results\nFinally, let’s compare the coefficients and MSE of the OLS and Ridge models.\n\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 regularization on the coefficients and model performance.\nBy introducing L2 regularization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\nNote that L2 regularization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 regularization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term."
  },
  {
    "objectID": "code/Genera.html",
    "href": "code/Genera.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To understand the generalization of L2 regularization on the test set for multiple linear regression with design matrix X, let’s first discuss what L2 regularization is and how it is applied in multiple linear regression.\nL2 Regularization in Multiple Linear Regression:\nMultiple Linear Regression is a technique used to model the relationship between a dependent variable and multiple independent variables. The goal is to find the best fitting line that minimizes the sum of squared errors.\nHowever, in some cases, the model can become overfit, meaning it fits the training data too closely and does not generalize well to unseen data. This can lead to poor performance on the test set.\nL2 regularization, also known as Ridge regression, is a technique used to prevent overfitting. It adds a penalty term to the least squares objective function, which reduces the magnitude of the coefficients and helps in controlling the complexity of the model.\nThe L2 regularization term is given by:\n\\(\\text{L2 regularization term} = \\lambda \\sum_{i=1}^{m} \\beta_i^2\\)\nWhere:\n\n() is the regularization parameter, which controls the amount of regularization applied. A higher value of () results in more regularization.\n(_i) is the coefficient associated with the (i)th independent variable.\n\nIncluding the L2 regularization term in the objective function, the cost function for multiple linear regression with L2 regularization becomes:\n\\(\\text{{Cost function}} = \\frac{1}{2m} \\left( \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right)\\)\nWhere:\n\n(m) is the number of training examples.\n(h_(x^{(i)})) is the predicted value for the (i)th example using the model’s parameters ().\n(y^{(i)}) is the actual value for the (i)th example.\n\nNow, let’s see how we can apply L2 regularization in multiple linear regression using Python.\nApplying L2 Regularization in Multiple Linear Regression:\nFirst, we need to import the required libraries for our example:\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nNext, we need to create the design matrix X and the target variable y. The design matrix X contains the values of the independent variables, and the target variable y contains the corresponding dependent variable values.\n\n# Create design matrix X\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Create target variable y\ny = np.array([10, 20, 30])\n\nWe split the data into training and test sets using the train_test_split function from scikit-learn. The training set will be used to train the model, and the test set will be used to evaluate its performance.\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nNext, we create an instance of the Ridge regression model and train it on the training set.\n\n# Create an instance of the Ridge regression model\nridge = Ridge(alpha=0.5)\n\n# Train the model on the training set\nridge.fit(X_train, y_train)\n\nRidge(alpha=0.5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=0.5)\n\n\nWe can now use the trained model to make predictions on the test set.\n\n# Make predictions on the test set\ny_pred = ridge.predict(X_test)\n\nFinally, we can evaluate the performance of the model using a performance metric such as mean squared error.\n\n# Calculate the mean squared error on the test set\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\n\nMean Squared Error: 0.28698979591836626\n\n\nThe mean squared error will give us an idea of how well the model is performing on the test set. A lower mean squared error indicates better performance.\nThis is how L2 regularization is applied in multiple linear regression to generalize the model on the test set. By adding a penalty term to the objective function, we can control the complexity of the model and prevent overfitting."
  },
  {
    "objectID": "code/Improv.html",
    "href": "code/Improv.html",
    "title": "L2 Regularization",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 regularization for multiple linear regression using Python:\nFirst, let’s import the necessary libraries:\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nNext, we’ll generate some random data for demonstration purposes:\n\n# Generate random data\nnp.random.seed(0)\nX = np.random.rand(100, 5)\nw = np.random.rand(5, 1)\ny = X**2 @ w + np.random.randn(100)*5\n\nWe split the data into training and test sets:\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nTo apply L2 regularization, we need to scale the input features using the StandardScaler:\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nLet’s use OLS without L2 regularization for a regression model:\n\n# Create a regression model without penalty\nno_ridge = Ridge(alpha=0.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nno_ridge.fit(X_train_scaled, y_train)\n\nRidge(alpha=0.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=0.0)\n\n\nNext, we can evaluate the trained model on the test set:\n\n# Evaluate the model on the test set\nscore = no_ridge.score(X_test_scaled, y_test)\nprint(f\"No-penalty Regression Score: {score}\")\n\nNo-penalty Regression Score: 0.9411011679689105\n\n\nThe score represents the coefficient of determination \\((R^2)\\) of the prediction. Higher values of \\(R^2\\) indicate better model performance.\nWe can now create and train the ridge regression model:\n\n# Create a ridge regression model\nridge = Ridge(alpha=10.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nridge.fit(X_train_scaled, y_train)\n\nRidge(alpha=10.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=10.0)\n\n\nFinally, we can evaluate the trained model on the test set:\n\n# Evaluate the model on the test set\nscore = ridge.score(X_test_scaled, y_test)\nprint(f\"Ridge Regression Score: {score}\")\n\nRidge Regression Score: 0.9439855609099314\n\n\nBy applying L2 regularization using ridge regression, we can improve the generalization of the multiple linear regression model by reducing overfitting and improving its performance on unseen data."
  },
  {
    "objectID": "code/bias v.html",
    "href": "code/bias v.html",
    "title": "Bias-Variance Trade-off",
    "section": "",
    "text": "The bias-variance trade-off is a fundamental concept in machine learning that helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. In supervised learning problems, we aim to find a model, usually a mathematical function, that can accurately predict the target variable based on the input features."
  },
  {
    "objectID": "code/bias v.html#bias-and-variance",
    "href": "code/bias v.html#bias-and-variance",
    "title": "Bias-Variance Trade-off",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nBefore diving into the bias-variance trade-off, let’s briefly explain two important concepts: bias and variance.\n\nBias measures how much our model’s predictions deviate from the true values. A model with high bias oversimplifies the underlying relationship between the features and the target variable. This can lead to underfitting, where the model fails to capture the patterns and relationships in the data.\nVariance measures the variability of model predictions for different training sets. A model with high variance is too sensitive to the specific training examples and does not generalize well to new, unseen data. This can lead to overfitting, where the model fits the training data too well but performs poorly on new data.\n\nThe aim is to find a good balance between bias and variance, where the model captures the underlying patterns in the training data without overfitting."
  },
  {
    "objectID": "code/bias v.html#bias-variance-trade-off",
    "href": "code/bias v.html#bias-variance-trade-off",
    "title": "Bias-Variance Trade-off",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\nThe bias-variance trade-off states that as we decrease the bias of a model (increasing complexity), we tend to increase its variance, and vice versa. This trade-off occurs because model complexity allows for a better fit to the training data, but at the risk of poor performance on new data.\nTo illustrate this concept, let’s consider a regression problem where we can adjust the complexity of a model by changing the degree of the polynomial used for fitting the data."
  },
  {
    "objectID": "code/bias v.html#example",
    "href": "code/bias v.html#example",
    "title": "Bias-Variance Trade-off",
    "section": "Example",
    "text": "Example\n\nImporting Required Libraries\nWe start by importing the necessary libraries: NumPy for numerical operations and matplotlib for visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# To plot the graphs inline in Jupyter Notebook\n%matplotlib inline\n\n\n\nGenerating Synthetic Data\nNext, we generate some synthetic data with a nonlinear relationship between the input features and the target variable using the numpy library.\n\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Generate input features\nX = np.linspace(-5, 5, 100).reshape(-1, 1)\n\n# Generate target variable with noise\nY_true = X**3 - X**2 + X + np.random.randn(100, 1)\n\nHere, we generate 100 samples of input features X ranging from -5 to 5. The target variable Y_true is generated using a cubic relationship with some random Gaussian noise.\n\n\nFitting Polynomial Models\nWe will now fit polynomial models with different degrees to the synthetic data and observe the effect of model complexity on bias and variance.\n\n# Create a function to fit polynomial models and visualize the results\ndef fit_polynomial(X, Y_true, degree):\n    # Fit polynomial regression model\n    poly_features = np.polynomial.Polynomial.fit(X.flatten(), Y_true.flatten(), degree)\n    Y_pred = poly_features(X.flatten())\n    \n    # Compute bias and variance\n    bias = np.mean(np.abs(Y_true - Y_pred))\n    variance = np.var(Y_pred)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X, Y_true, label='True Data', color='b')\n    plt.plot(X, Y_pred, label='Predicted', color='r')\n    plt.title(f'Polynomial Regression (Degree = {degree})\\nBias = {bias:.2f}, Variance = {variance:.2f}')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n# Fit polynomial models with degrees 1, 2, 3, 5, 10\nfit_polynomial(X, Y_true, degree=1)\nfit_polynomial(X, Y_true, degree=2)\nfit_polynomial(X, Y_true, degree=3)\nfit_polynomial(X, Y_true, degree=5)\nfit_polynomial(X, Y_true, degree=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this code block, we define a function fit_polynomial that takes the input features X, true target variable Y_true, and the degree of the polynomial model to be fitted as arguments. Inside the function, we use the numpy.polynomial.Polynomial.fit function to fit a polynomial regression model with the desired degree.\nFor each degree of the polynomial model, we compute the bias and variance using the mean absolute error and variance of the predicted values. Then, we plot the true data points, the predicted curve, and display the bias and variance in the title of the plot.\n\n\nAnalysis and Observations\nBy running the code above, we get a series of plots showing the true data points and the predicted curves for polynomial regression models with different degrees. Each plot also displays the corresponding bias and variance values.\n\nFor a linear model (degree=1), the model is too simple to capture the underlying cubic relationship in the data. Hence, it has a high bias and performs poorly in terms of fitting the data.\nAs the degree of the polynomial model increases, the model can fit the data more accurately, resulting in reduced bias. However, as the complexity increases (degree=5 and 10), we observe that the models start to capture the random fluctuations in the data, resulting in higher variance. These models may fit the training data very well but are likely to perform poorly on unseen data.\nThe model with a degree of 3 strikes a good balance between bias and variance, as it captures the underlying cubic relationship while avoiding overfitting."
  },
  {
    "objectID": "code/bias v.html#conclusion",
    "href": "code/bias v.html#conclusion",
    "title": "Bias-Variance Trade-off",
    "section": "Conclusion",
    "text": "Conclusion\nThe bias-variance trade-off is a fundamental concept in machine learning. It helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. By finding the right balance between bias and variance, we can develop models that accurately represent the patterns in the data without overfitting or oversimplifying the relationships."
  },
  {
    "objectID": "code/Linear_classification.html",
    "href": "code/Linear_classification.html",
    "title": "Linear least squares",
    "section": "",
    "text": "Linear least squares is a technique used for regression problems, where we aim to predict continuous numerical values. However, it can also be used for classification tasks by transforming the problem into a binary classification problem.\nIn linear least squares for classification, we use a linear model to classify data into two classes. We assign class labels of -1 and 1 to the two classes. The goal is to find a linear boundary that best separates the two classes, minimizing the sum of squared distances between the data points and the decision boundary.\nLet’s see how we can do this in Python:\nFirst, we need to import the required libraries: numpy and matplotlib.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNext, let’s generate some synthetic data with two classes. We will use the make_classification function from the sklearn.datasets module to create a random dataset.\n\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\nNow, let’s visualize the data using a scatter plot:\n\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n\n\n\n\nWe have plotted the data points for each class on a scatter plot.\nTo apply linear least squares for classification, we need to add a column of ones to our feature matrix X to incorporate the bias term in the linear equation.\n\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n\nNow, let’s define our linear model and solve for the optimal parameters using linear least squares.\n\ntheta = np.linalg.inv(X.T @ X) @ X.T @ y\n\nHere, theta is the vector of parameters that defines our linear model. The equation used to solve for theta is:\n\\(\\theta = (X^T X)^{-1} X^T y\\)\nFinally, let’s visualize the decision boundary of our linear model along with the data points.\n\nplt.scatter(X[y == 1][:,1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0][:,1], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\n\n# Plotting the decision boundary\nx_boundary = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n# x2_boundary = -(theta[0] + theta[1]*x1_boundary) / theta[2]\ny_boundary = x_boundary * theta[1] +  theta[0]\nplt.plot(x_boundary, y_boundary, color='black', linewidth=2)\n\nplt.show()\n\n\n\n\nWe have plotted the decision boundary determined by our linear model, which separates the two classes.\nLinear least squares for classification is a simple technique for linearly separable datasets. Note that this approach assumes the data points are linearly separable and does not work well for nonlinear classification problems."
  },
  {
    "objectID": "code/SVM.html",
    "href": "code/SVM.html",
    "title": "SVM",
    "section": "",
    "text": "Support Vector Machines (SVM) is a powerful machine learning algorithm used for both classification and regression tasks. In this tutorial, we will focus on how to use SVM for classification.\nBefore diving into the code, let’s understand the intuition behind SVM."
  },
  {
    "objectID": "code/SVM.html#intuition-behind-svm",
    "href": "code/SVM.html#intuition-behind-svm",
    "title": "SVM",
    "section": "Intuition behind SVM",
    "text": "Intuition behind SVM\nSVM is based on the concept of finding a hyperplane that best separates the data points belonging to different classes. The hyperplane is determined by support vectors, which are the data points closest to the decision boundary.\nIn a binary classification problem, SVM aims to find a hyperplane that maximizes the margin between the support vectors of the two classes. The margin is the distance between the hyperplane and the nearest data points from each class.\nThe optimal hyperplane can be described by the equation:\n\\(w^T x - b = 0\\)\nwhere \\(w\\) is the normal vector to the hyperplane and \\(b\\) is the bias.\nThe equation of the decision function is:\n\\(f(x) = sign(w^T x - b)\\)\nwhere \\(sign(\\cdot)\\) is the sign function.\nSVM can also handle non-linearly separable data by using a technique called the kernel trick. This technique transforms the original feature space into a higher-dimensional space, making the data linearly separable.\nNow, let’s implement SVM for a classification problem using the famous Iris dataset."
  },
  {
    "objectID": "code/SVM.html#importing-libraries-and-loading-the-dataset",
    "href": "code/SVM.html#importing-libraries-and-loading-the-dataset",
    "title": "SVM",
    "section": "Importing Libraries and Loading the Dataset",
    "text": "Importing Libraries and Loading the Dataset\nThe first step is to import the required libraries and load the dataset. We will use scikit-learn library, which provides a simple API for SVM implementation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nNext, let’s generate some synthetic data with two classes. We will use the make_classification function from the sklearn.datasets module to create a random dataset.\n\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\nNow, let’s visualize the data using a scatter plot:\n\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n\n\n\n\nWe have plotted the data points for each class on a scatter plot."
  },
  {
    "objectID": "code/SVM.html#training-the-support-vector-machine-classifier",
    "href": "code/SVM.html#training-the-support-vector-machine-classifier",
    "title": "SVM",
    "section": "Training the Support Vector Machine Classifier",
    "text": "Training the Support Vector Machine Classifier\nOnce we have loaded the dataset, we can proceed to train the SVM classifier.\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVM classifier object\nsvm = SVC(kernel='linear')\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear')\n\n\nIn this code block, we first split the data into training and testing sets using the train_test_split() function. We reserve 20% of the data for testing and set the random_state parameter for reproducibility purposes.\nNext, we create an SVM classifier object using the SVC class from scikit-learn. We specify the kernel parameter as ‘linear’ for a linear SVM.\nFinally, we train the SVM classifier using the fit() method by passing the training data (X_train) and the corresponding class labels (y_train)."
  },
  {
    "objectID": "code/SVM.html#making-predictions-and-evaluating-the-model",
    "href": "code/SVM.html#making-predictions-and-evaluating-the-model",
    "title": "SVM",
    "section": "Making Predictions and Evaluating the Model",
    "text": "Making Predictions and Evaluating the Model\nAfter training the SVM classifier, we can use it to make predictions on new, unseen data.\n\n# Make predictions on the testing set\ny_pred = svm.predict(X_test)\n\n# Evaluate the model\naccuracy = np.sum(y_pred == y_test) / len(y_test)\nprint(\"Accuracy: \", accuracy)\n\nAccuracy:  1.0\n\n\nIn this code block, we use the trained SVM classifier to make predictions on the testing set (X_test). The predicted class labels are stored in the y_pred variable.\nWe then evaluate the model by calculating the accuracy of the predictions. The accuracy is defined as the number of correctly classified data points divided by the total number of data points."
  },
  {
    "objectID": "code/SVM.html#visualizing-the-decision-boundary",
    "href": "code/SVM.html#visualizing-the-decision-boundary",
    "title": "SVM",
    "section": "Visualizing the Decision Boundary",
    "text": "Visualizing the Decision Boundary\nTo visualize the decision boundary and the support vectors, we can use the following code:\n\ndef plot_decision_boundary(classifier, X, y):\n    # Define the range of x values for the mesh grid\n    x_min, x_max = X.min() - 1, X.max() + 1\n\n    # Create a mesh grid\n    xx = np.linspace(x_min, x_max, 1000)[:, np.newaxis]\n\n    # Use the classifier to make predictions on the mesh grid\n    yy = classifier.decision_function(xx)\n\n    # Plot the decision boundary and support vectors\n    plt.scatter(X, y, c=y, cmap=plt.cm.Paired, edgecolors='k')\n    plt.plot(xx, yy, color='black', linewidth=3)\n    plt.xlim(x_min, x_max)\n    plt.ylim(-0.2, 1.2)\n    plt.xlabel('Feature')\n    plt.ylabel('label')\n    plt.show()\n\n# Visualize the decision boundary\nplot_decision_boundary(svm, X_train, y_train)\n\n\n\n\nIn this code block, we define a helper function plot_decision_boundary() that takes a trained classifier object (svm), the training data (X_train), and the corresponding class labels (y_train) as input.\nThe function calculates the minimum and maximum values of the two features to define the plotting range. It then generates a mesh grid with a step size h and predicts the class labels for each point in the grid using predict() method.\nFinally, it plots the decision boundary by contouring the predicted class labels and scatter plots the training data points.\nRunning this code will display the decision boundary and the support vectors.\nThis is how we can implement SVM for classification in Python. SVM is a versatile algorithm and can be further fine-tuned by selecting different kernels and hyperparameters for better performance."
  },
  {
    "objectID": "w3/week3_exercise.html",
    "href": "w3/week3_exercise.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "!pip install imbalanced-learn\n\nCollecting imbalanced-learn\n  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: numpy&gt;=1.17.3 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.11.2)\nRequirement already satisfied: scikit-learn&gt;=1.0.2 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.3.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (3.2.0)\nDownloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 235.6/235.6 kB 4.5 MB/s eta 0:00:00:00:01\nInstalling collected packages: imbalanced-learn\nSuccessfully installed imbalanced-learn-0.11.0\n\n\n\nfrom imblearn.datasets import fetch_datasets\n\n\n# Load the dataset\ndata = fetch_datasets()['mammography']\nX, y = data.data, data.target\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame(np.hstack([X,y.reshape(-1,1)]))\ndf.columns = [f\"feature_{i}\" for i in range(1,7)] + ['y']\ndf\n\n\n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\ny\n\n\n\n\n0\n0.230020\n5.072578\n-0.276061\n0.832444\n-0.377866\n0.480322\n-1.0\n\n\n1\n0.155491\n-0.169390\n0.670652\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n2\n-0.784415\n-0.443654\n5.674705\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n3\n0.546088\n0.131415\n-0.456387\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n4\n-0.102987\n-0.394994\n-0.140816\n0.979703\n-0.377866\n1.013566\n-1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11178\n-0.250012\n-0.377300\n-0.321142\n1.269157\n3.652984\n1.092791\n1.0\n\n\n11179\n0.281343\n-0.417112\n-0.366224\n0.851010\n2.789649\n1.345700\n1.0\n\n\n11180\n1.204988\n1.763724\n-0.501468\n1.562408\n6.489072\n0.931294\n1.0\n\n\n11181\n0.736644\n-0.222474\n-0.050653\n1.509665\n0.539269\n1.315229\n1.0\n\n\n11182\n0.177003\n-0.191508\n-0.501468\n1.578864\n7.750705\n1.555951\n1.0\n\n\n\n\n11183 rows × 7 columns\n\n\n\n\nX.shape, y.shape\n\n((11183, 6), (11183,))"
  },
  {
    "objectID": "code/naive.html",
    "href": "code/naive.html",
    "title": "Naive Bayes Classifier",
    "section": "",
    "text": "The Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification problems and is especially effective when dealing with high-dimensional data. Naive Bayes assumes that the presence of a particular feature in a class is independent of the presence of other features."
  },
  {
    "objectID": "code/naive.html#bayes-theorem",
    "href": "code/naive.html#bayes-theorem",
    "title": "Naive Bayes Classifier",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nBefore diving into Naive Bayes, let’s start with Bayes’ theorem. Bayes’ theorem allows us to calculate the probability of a hypothesis given some observed evidence. It is stated as:\n\\(P(h|e) = \\frac{{P(e|h) \\cdot P(h)}}{{P(e)}}\\)\nWhere:\n\n\\(P(h|e)\\) is the posterior probability of hypothesis \\(h\\) given evidence \\(e\\).\n\\(P(e|h)\\) is the probability of evidence \\(e\\) given hypothesis \\(h\\).\n\\(P(h)\\) is the prior probability of hypothesis \\(h\\).\n\\(P(e)\\) is the probability of evidence \\(e\\).\n\nIn the context of Naive Bayes, we can reframe this theorem as:\n\\(P(y|X) = \\frac{{P(X|y) \\cdot P(y)}}{{P(X)}}\\)\nWhere:\n\n\\(X\\) represents the input features.\n\\(y\\) represents the class or target variable.\n\\(P(y|X)\\) is the posterior probability of class \\(y\\) given features \\(X\\).\n\\(P(X|y)\\) is the probability of observing features \\(X\\) given class \\(y\\).\n\\(P(y)\\) is the prior probability of class \\(y\\).\n\\(P(X)\\) is the probability of observing features \\(X\\)."
  },
  {
    "objectID": "code/naive.html#naive-bayes-classifier",
    "href": "code/naive.html#naive-bayes-classifier",
    "title": "Naive Bayes Classifier",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\nThe Naive Bayes classifier assumes that the presence of each feature is independent of the presence of other features, given the class variable. This is where the “Naive” part comes in. Despite this simplifying assumption, Naive Bayes can still be very effective in practice, especially with text classification tasks.\nThere are three common types of Naive Bayes classifiers: 1. Gaussian Naive Bayes: It assumes that the features are normally distributed. 2. Multinomial Naive Bayes: It is suitable for discrete features (e.g., word counts). 3. Bernoulli Naive Bayes: It is suitable for binary features (e.g., true/false).\nIn this tutorial, we will focus on Gaussian Naive Bayes, which is commonly used for continuous features."
  },
  {
    "objectID": "code/naive.html#gaussian-naive-bayes",
    "href": "code/naive.html#gaussian-naive-bayes",
    "title": "Naive Bayes Classifier",
    "section": "Gaussian Naive Bayes",
    "text": "Gaussian Naive Bayes\nGaussian Naive Bayes assumes that the continuous features in each class are normally distributed. It calculates the mean and standard deviation for each feature in each class and uses a Gaussian probability density function to estimate the likelihood of observing a particular feature value given a class. The formula for the Gaussian probability density function is:\n\\(P(x|y) = \\frac{1}{{\\sqrt{{2\\pi\\sigma_y^2}}}} \\cdot e^{-\\frac{{(x - \\mu_y)^2}}{{2\\sigma_y^2}}}\\)\nWhere:\n\n\\(x\\) is the feature value.\n\\(y\\) is the class label.\n\\(\\mu_y\\) is the mean of the feature values in class \\(y\\).\n\\(\\sigma_y\\) is the standard deviation of the feature values in class \\(y\\).\n\\(\\pi\\) is the mathematical constant pi."
  },
  {
    "objectID": "code/naive.html#implementation",
    "href": "code/naive.html#implementation",
    "title": "Naive Bayes Classifier",
    "section": "Implementation",
    "text": "Implementation\nNow let’s see the implementation of Gaussian Naive Bayes in Python using the sklearn library.\nStep 1: Import the required libraries.\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nStep 2: Load the dataset.\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nStep 3: Split the dataset into training and testing sets.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 4: Create a Gaussian Naive Bayes classifier and fit it to the training data.\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nStep 5: Make predictions on the test data.\n\ny_pred = clf.predict(X_test)\n\nStep 6: Evaluate the accuracy of the classifier.\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 1.0\n\n\nThat’s it! You have successfully implemented Gaussian Naive Bayes for classification in Python using sklearn.\nNote: This tutorial only covers the basic usage of Naive Bayes. There are many other aspects and variations of Naive Bayes that you can explore, such as Laplace smoothing, handling missing values, and dealing with categorical features."
  },
  {
    "objectID": "code/multin.html",
    "href": "code/multin.html",
    "title": "Multinomial Naive Bayes",
    "section": "",
    "text": "In machine learning, Naive Bayes is a probabilistic algorithm that is based on Bayes’ theorem. It is commonly used for classification problems and is known for its simplicity and efficiency. In particular, Multinomial Naive Bayes is a variation of the Naive Bayes algorithm that is specifically designed for discrete features, such as word counts in text documents."
  },
  {
    "objectID": "code/multin.html#bayes-theorem",
    "href": "code/multin.html#bayes-theorem",
    "title": "Multinomial Naive Bayes",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nTo understand how Multinomial Naive Bayes works, let’s first review Bayes’ theorem. Bayes’ theorem provides a way to calculate conditional probabilities. It can be formulated as follows:\n\\(P(A|B) = \\frac{{P(B|A) P(A)}}{{P(B)}}\\)\nWhere:\n\n\\(P(A|B)\\) is the probability of event A occurring given that event B has occurred.\n\\(P(B|A)\\) is the probability of event B occurring given that event A has occurred.\n\\(P(A)\\) is the probability of event A occurring.\n\\(P(B)\\) is the probability of event B occurring."
  },
  {
    "objectID": "code/multin.html#multinomial-naive-bayes",
    "href": "code/multin.html#multinomial-naive-bayes",
    "title": "Multinomial Naive Bayes",
    "section": "Multinomial Naive Bayes",
    "text": "Multinomial Naive Bayes\nMultinomial Naive Bayes is specifically designed for problems with discrete features. It assumes that the features are generated from a multinomial distribution and that the features are conditionally independent given the class label. This assumption simplifies the conditional probability calculation and makes the algorithm computationally efficient.\nHere’s a step-by-step overview of how Multinomial Naive Bayes works:\n\nPreparing the Dataset: First, we need a dataset consisting of samples with features and corresponding class labels. The features should be discrete, such as word counts in text documents.\nFeature Extraction: Next, we need to extract features from the dataset. This can involve techniques like tokenization, stemming, and vectorization.\nTraining: We then split the dataset into a training set and a test set. The training set is used to calculate the probabilities required for classification.\nCalculating Class Prior Probabilities: We calculate the prior probability of each class by counting the frequency of each class label in the training set.\nCalculating Conditional Probabilities: We calculate the conditional probability of each feature given the class label by counting the frequency of each feature in each class.\nClassifying New Instances: Finally, we use the calculated probabilities to classify new instances. For each new instance, we calculate the posterior probability of each class given the features and select the class with the highest probability as the predicted class.\n\nLet’s now implement Multinomial Naive Bayes in Python using the scikit-learn library."
  },
  {
    "objectID": "code/multin.html#implementation",
    "href": "code/multin.html#implementation",
    "title": "Multinomial Naive Bayes",
    "section": "Implementation",
    "text": "Implementation\nFirst, we need to import the necessary libraries:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nNext, let’s create an example dataset consisting of text documents and corresponding class labels:\n\ndocuments = ['The sun is shining',\n             'The weather is beautiful',\n             'I enjoy going for walks',\n             'I hate rainy days']\n\nlabels = ['positive', 'positive', 'negative', 'negative']\n\nNow, let’s create a CountVectorizer object to extract features from the text documents:\n\nvectorizer = CountVectorizer()\n\nWe can then use the fit_transform() method of the vectorizer to transform the documents into a feature matrix:\n\nX = vectorizer.fit_transform(documents)\n\nNext, we need to split the dataset into a training set and a test set:\n\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n\nNow, let’s create a MultinomialNB object and train it on the training set:\n\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nWe can then use the trained model to classify the instances in the test set:\n\ny_pred = model.predict(X_test)\n\nFinally, let’s calculate the accuracy of the model:\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n\nAccuracy: 1.0\n\n\nWe have successfully trained a Multinomial Naive Bayes classifier and used it to classify new instances."
  },
  {
    "objectID": "code/precis.html",
    "href": "code/precis.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "The precision-recall curve is a graphical representation of the trade-off between precision and recall for different threshold values. It is commonly used in binary classification problems where the goal is to classify data into one of two classes.\nLet’s start by understanding precision and recall:\nPrecision is defined as the number of true positives (TP) divided by the sum of true positives and false positives (FP): [ Precision = ]\nRecall is defined as the number of true positives (TP) divided by the sum of true positives and false negatives (FN): [ Recall = ]\nIn a classification problem, a high precision means that the classifier is making fewer false positive predictions, while a high recall means that it is making fewer false negative predictions.\nTo create a precision-recall curve, we need a classifier that can provide prediction probabilities or scores for each instance. Then, by varying the threshold on these scores, we can generate different points on the precision-recall curve.\nWe will demonstrate this process using the scikit-learn library and the Breast Cancer Wisconsin (Diagnostic) dataset. Let’s get started:\nStep 1: Import the necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\n\nStep 2: Load and prepare the dataset\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 3: Train a classifier and obtain prediction probabilities\n\n# Train a logistic regression classifier\nclassifier = LogisticRegression(max_iter=10_000)\nclassifier.fit(X_train, y_train)\n\n# Obtain prediction probabilities for the test set\ny_prob = classifier.predict_proba(X_test)[:, 1]\n\nStep 4: Calculate precision and recall for different threshold values\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n\nStep 5: Plot the precision-recall curve\n\nplt.plot(thresholds, precision[:-1], label='Precision')\nplt.plot(thresholds, recall[:-1], label='Recall')\nplt.xlabel('Threshold')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.show()\n\n\n\n\nIn this code, we first import the necessary libraries such as numpy, matplotlib, scikit-learn modules, and data from scikit-learn’s built-in Breast Cancer Wisconsin dataset.\nNext, we load and prepare the dataset. We split it into training and testing sets using the train_test_split function.\nThen, we train a logistic regression classifier on the training set and obtain prediction probabilities for the test set using the predict_proba method.\nFinally, we calculate precision and recall values for different threshold values using the precision_recall_curve function. We plot these values to visualize the precision-recall curve using the plt.plot function.\nThe resulting precision-recall curve shows how the precision and recall values change for different threshold values. A higher precision and recall value indicates a better classifier performance.\nThis curve can be useful in identifying an appropriate threshold value that balances precision and recall according to the specific problem requirements."
  },
  {
    "objectID": "code/confus.html",
    "href": "code/confus.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "A confusion matrix is a useful tool for evaluating the performance of a classification model. It provides a tabular representation of the predicted and actual classes of a binary classification problem. The matrix helps us understand how well the model is performing by showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\nLet’s define the confusion matrix for a binary classification problem using the following notations:\n\nTP: Number of true positive predictions\nTN: Number of true negative predictions\nFP: Number of false positive predictions\nFN: Number of false negative predictions\n\nTo illustrate this, let’s consider a dataset of 100 samples. Our binary classifier predicts whether a sample is positive or negative. After running the prediction, we obtain the following results:\n\nThere are 60 true positive predictions (TP = 60).\nThere are 30 true negative predictions (TN = 30).\nThere are 5 false positive predictions (FP = 5).\nThere are 5 false negative predictions (FN = 5).\n\nNow, let’s plot these values in a confusion matrix:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP = 60\nFN = 5\n\n\nActual Negative\nFP = 5\nTN = 30\n\n\n\nIn Python, we can use the scikit-learn library to calculate the confusion matrix. Here’s an example of how to compute the confusion matrix for binary classification:\nStep 1: Import the necessary libraries\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nStep 2: Define the actual and predicted classes\n\nactual = np.array([1, 1, 0, 0, 1, 0, 0, 1, 0, 1])\npredicted = np.array([1, 0, 0, 1, 1, 0, 1, 1, 0, 0])\n\nStep 3: Calculate the confusion matrix using the confusion_matrix function\n\ncm = confusion_matrix(actual, predicted)\nprint(cm)\n\n[[3 2]\n [2 3]]\n\n\nOutput:\n[[3 2]\n [2 3]]\nIn this example, we have 3 true positive predictions, 3 true negative predictions, 2 false positive predictions, and 2 false negative predictions. Hence, the confusion matrix is:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP = 3\nFN = 2\n\n\nActual Negative\nFP = 2\nTN = 3\n\n\n\nThe confusion matrix provides essential information for evaluating the performance of a binary classification model, such as accuracy, precision, recall, and F1 score. It helps us understand the model’s strengths and weaknesses, identify any imbalances in the predictions, and make informed decisions about improving the model."
  },
  {
    "objectID": "code/prf1.html",
    "href": "code/prf1.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "In the field of machine learning and classification, evaluating the model’s performance is crucial. One common way to evaluate classification models is by using a confusion matrix. The confusion matrix provides a detailed breakdown of the model’s predictions and their corresponding actual labels. From the confusion matrix, we can calculate several performance metrics, including precision, recall, and F1-score.\nLet’s start by understanding what a confusion matrix is.\nA confusion matrix is a table that visualizes the performance of a classification model. It consists of four different values:\n\nTrue Positive (TP): The number of positive instances that the model correctly predicted as positive.\nFalse Positive (FP): The number of negative instances that the model incorrectly predicted as positive.\nTrue Negative (TN): The number of negative instances that the model correctly predicted as negative.\nFalse Negative (FN): The number of positive instances that the model incorrectly predicted as negative.\n\nThe confusion matrix is typically presented in the following format:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\nNow, let’s calculate precision, recall, and F1-score using the confusion matrix.\nPrecision measures the accuracy of positive predictions. It is calculated using the formula:\n\\(\\text{{Precision}} = \\frac{TP}{{TP + FP}}\\)\nRecall, also known as the sensitivity or true positive rate, measures the proportion of actual positive instances that were correctly classified. It is calculated using the formula:\n\\(\\text{{Recall}} = \\frac{TP}{{TP + FN}}\\)\nF1-score is the harmonic mean of precision and recall. It provides a balanced measure between the two. F1-score is calculated using the formula:\n\\(F1 = \\frac{2 \\times \\text{{Precision}} \\times \\text{{Recall}}}{{\\text{{Precision}} + \\text{{Recall}}}}\\)"
  }
]