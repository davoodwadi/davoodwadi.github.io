[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "I’m a researcher and educator with a focus on Digital Marketing and the application of Artificial Intelligence in Marketing.\nI currently teach the following courses:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Courses",
    "section": "About this blog",
    "text": "About this blog\nI currently teach the following courses:"
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "Davood Wadi",
    "section": "Courses",
    "text": "Courses\n+ MIT: *Applied Data Science Program*\n+ HEC Montreal: *MATH60629A - Machine Learning I*"
  },
  {
    "objectID": "index.html#experiences",
    "href": "index.html#experiences",
    "title": "Davood Wadi",
    "section": "Experiences",
    "text": "Experiences\n\nPhD in Marketing - HEC Montreal (Best 2023 Thesis Award Nominee)\nPartner with intelChain.io as AI Scientist"
  },
  {
    "objectID": "courses.html#course-outline",
    "href": "courses.html#course-outline",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Davood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT\n\nFinal Exam\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated"
  },
  {
    "objectID": "courses.html#course-information",
    "href": "courses.html#course-information",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "courses.html#evaluations",
    "href": "courses.html#evaluations",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "Evaluations",
    "text": "Evaluations\n\n[Bonus] Class participation (5%)\nTo receive this 5% bonus point:\n\nPrepare for each session’s code before the class starts.\nLog in to deepnote with your Full Name.\nDuring the coding exercises in class, add your answer to the questions as a comment.\n\n\n\nHomework (20%)\n\nDue on 24 October 2023\nHomework assignments are counted for 20% of your final grade.\nThe homework should be done in jupyter notebook here or Google Colab here.\nInstructions:\n\nThe homework is due by 11:59PM EST on the due date.\nPlease upload a PDF version of your assignment on ZoneCours and insert the shareable link to your notebook in the same interface.\nHomeworks must be completed individually.\n\nDiscussion with others is okay, but you must write solutions yourself.\n\nAll code used to arrive at answers is submitted along with answers.\nNotes:\n\nPlease provide your code answers in the code block under each question and verbal answers in text boxes assigned in the notebook (where applicable).\nPlease run the notebook before the submission so that the outputs are displayed.\nPlease make sure that your results are reproducible. You may use random seeds from random and numpy packages. For scikit-learn modules, you may use the random_state argument.\n\n\n\n\n\nProject (30%)\nThe aim of this project is to allow you to learn about machine learning by trying to solve a task with it.\nFirst, select a question that can be answered using machine learning. I expect that your question will be about a model/algorithm or about an application. Then design a study that will try to answer your question. Your study must have an element of novelty. For example the novelty could be an extension or a variation of an existing algorithm or results of an existing method on a new dataset.\nYour study should involve reading and understanding some background material. Your study must involve running some experiments. You are free to use (or not) any of the tools or models we have seen in class.\nStudy plan: (1 upload per group) Please submit a one-page summary of your proposed research question and study to ZoneCours. I will meet with each group to discuss study plans during the lecture on Week 9. I will send you a schedule the day before. We will probably only have about 15 minutes so please make sure that your study plan is clear and precise. You may also include questions that you would like us to discuss at the end of the document.\nThe group report: (1 upload per group) Your report must contain a description of the question you are trying to answer, a clear description of the model/algorithm you are studying, a survey of related work which proper references, an empirical section that reports your results, and a conclusion that summarizes your findings and (if pertinent) highlights possible future directions of investigation. Your report should be no longer than 10 pages in length (plus references) for pairs or 13 pages (plus references) for teams of three.\nThe individual report: (1 upload per student) You will also submit a brief individual report (at most one page), which will: (1) Describe the parts of the project you worked on (which machine learning methods you applied, which preprocessing steps you performed on the data, which parts of the term paper you wrote, who you worked with on what parts, etc.) and what parts of the project your teammates worked on. (2) What you learned from the project. The purpose of the individual report is to facilitate fair grading and to allow the instructor to understand well what you learned from the project.\nProject Report (30%)\n\nClarity/Relevance of problem statement and description of approach: 10%\nDiscussion of relationship to previous work and references: 4%\nDesign and execution of experiments: 10%\nFigures/Tables/Writing: easily readable, properly labeled,\ninformative: 5%\nIndiviual report: 1%\n\n\nProject Topic\nConcretely, your project could take one of the following forms:\n\nA competition on kaggle.com that you try multiple ML methods on and can achieve a validation set performance in the top 100 on the leaderboard.\nA benchmark task on paperswithcode (e.g. Speech Recognition) where you try multiple methods and compare their performance.\nA task in your own field whereby you apply multiple ML methods to solve the task.\nAn ML method that you apply to a new task (e.g. Multi-head attention applied to time-series prediction)\n\nNote. The list above is not exhaustive. You are encouraged to be creative about the topic. Pick a topic that you find interesting and is relevant. The appropriateness of the topic will be discussed and resolved in the session on Study plan.\n\n\nGroup Report Structure\nYour group report should read like a published paper. Similarly, your poster presentation should have a similar structure to conference posters. You might want to check out similar papers to your topic on the structure of your report (e.g. A Comparison of Deep Learning Approach for Underwater Object Detection).\nRegardless of your topic, as a general rule of thumb, your Group Report should be structured as follows:\n\nIntroduction: Use this section to introduce the topic, its relevance, and a summary of the whole paper. This section should address the question, “Why the target audience of the paper should care about your work?”\nRelated Work: This section discusses what has been done, in prior research, on your topic. While most of the cited papers should be from academic publications, it is not uncommon to see non-academic references in papers (e.g. stackoverflow). You might want to use academic search engines (e.g. SemancticScholar) to find relavant papers.\nExperiments This section should cover the steps you take to answer the question you posed in the Introduction. This includes data preparation, model and hyper-parameter selection, and evaluation steps.\nResults In this section you present the resutls of your experiments with appropriate tables and figures.\nConclusion In your final section, you would summarize what you did in the paper, acknowledge the limitations of your work (All papers have this; no work is perfect.), suggest future research directions to address your limitations.\n\n\n\nTimeline\n\nTeam Registration, due: October 1. Fill this form.\nStudy plan, due: October 28 (by the end of the day EDT).\nHanding in: Through ZoneCours\nProject meeting, October 31\nProject Presentation, due: December 1. Upload the PDF of your poster/slides to ZoneCours.\nIn-class Presentation, on December 5.\nFinal individual report, due: December 15, 2023 (by the end of the day EDT).\n\nHanding in: Through ZoneCours (per each team member).\n\n\n\n\n\nProject Presentation (10%)\nMake a poster that describes your project [You do not need to print your poster]. You can think of a poster as supporting material for your oral presentation (in that way it is similar to slides). It could also follow a similar structure: begin by motivating your work, then (quickly) highlight related work, talk in depth about your solution, then go into results (pictures and tables are good tools for that), finally conclude and perhaps mention one or two ideas for future work.\n\nProject Presentation (10%)\n\nClarity of presentation: 3%\nSlide or Poster quality: 2%\nCorrectness: 2%\nAnswers to questions: 3%\n\n\n\n\nFinal Exam (30%)\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated\n\nPast exam - Fall 2018\n\n\nCapsule Quizzes (10%)\n\nIn-class quizzes of the capsules\nCapsule quizzes are counted for 10% of your final grade.\nTime to complete each quiz is 10 minutes.\n\nQuiz 0, 20 September 11:30 AM to 22 September 11:30 AM.\nQuiz 1, 27 September 11:30 AM to 29 September 11:30 AM.\nQuiz 2, TBA.\nQuiz 3, TBA.\nQuiz 4, TBA.\nQuiz 5, TBA.\nQuiz 6, TBA."
  },
  {
    "objectID": "docs/MATH60629A_Homework.html",
    "href": "docs/MATH60629A_Homework.html",
    "title": "Instructions:",
    "section": "",
    "text": "Due date: October 24, 2023\n# enter you full name and HEC ID\nfull_name = \"\"\nHEC_ID = \"\""
  },
  {
    "objectID": "docs/MATH60629A_Homework.html#svm-for-classfication",
    "href": "docs/MATH60629A_Homework.html#svm-for-classfication",
    "title": "Instructions:",
    "section": "SVM for classfication",
    "text": "SVM for classfication\n\n(1pt) Train a linear SVM on the training set for each one of these C hyperparameter values: {0:001; 0:01; 0:1; 1; 10}. Find the best hyperparameter on the validation set.\n\n\n[Your code here]\n\n\n(1pt) Using the best hyperparameter C, evaluate the accuracy, precision, recall, and F1-score on the test set.\n\n\n[Your code here]\n\n\n(0.5pt) Plot the confusion matrix on the test set and explain the reason for your false negatives/positives.\n\n\n[Your code here]\n\nObservations\n[_____]"
  },
  {
    "objectID": "MATH60629A_Homework.html",
    "href": "MATH60629A_Homework.html",
    "title": "Instructions:",
    "section": "",
    "text": "Due date: October 24, 2023\n# enter you full name and HEC ID\nfull_name = \"\"\nHEC_ID = \"\""
  },
  {
    "objectID": "MATH60629A_Homework.html#svm-for-classfication",
    "href": "MATH60629A_Homework.html#svm-for-classfication",
    "title": "Instructions:",
    "section": "SVM for classfication",
    "text": "SVM for classfication\n\n(1pt) Train a linear SVM on the training set for each one of these C hyperparameter values: {0:001; 0:01; 0:1; 1; 10}. Find the best hyperparameter on the validation set.\n\n\n[Your code here]\n\n\n(1pt) Using the best hyperparameter C, evaluate the accuracy, precision, recall, and F1-score on the test set.\n\n\n[Your code here]\n\n\n(0.5pt) Plot the confusion matrix on the test set and explain the reason for your false negatives/positives.\n\n\n[Your code here]\n\nObservations\n[_____]"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "sessions.html#session-materials",
    "href": "sessions.html#session-materials",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "courses.html#in-class-tools",
    "href": "courses.html#in-class-tools",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "In-class Tools",
    "text": "In-class Tools\n\nCups pacing webapp"
  },
  {
    "objectID": "code/Session1.html",
    "href": "code/Session1.html",
    "title": "Python Basics",
    "section": "",
    "text": "To start let’s load NumPy, a package for scientific computing with Python. We usually load it as the np shorthand.\n\nimport numpy as np\n\nLet’s initialize two arrays\n\na = np.random.randn(1,5)\nb = np.random.randn(1,5)"
  },
  {
    "objectID": "code/Session1.html#loading-necessary-packages",
    "href": "code/Session1.html#loading-necessary-packages",
    "title": "Python Basics",
    "section": "",
    "text": "To start let’s load NumPy, a package for scientific computing with Python. We usually load it as the np shorthand.\n\nimport numpy as np\n\nLet’s initialize two arrays\n\na = np.random.randn(1,5)\nb = np.random.randn(1,5)"
  },
  {
    "objectID": "code/Session1.html#dot-product",
    "href": "code/Session1.html#dot-product",
    "title": "Python Basics",
    "section": "Dot product",
    "text": "Dot product\nTo perform a dot product of two arrays, the shape of the arrays should match. Let’s get the shape of our arrays.\n\na.shape, b.shape\n\n((1, 5), (1, 5))\n\n\nboth arrays have a shape of \\((1, 5)\\).\nIn general, the shape of the two arrays should be \\((n, k) (k, m)\\), where \\(k\\) is the common dimension of the two arrays.\nFor our example, we can transpose the array b to make the shapes match. The NumPy command for transposition of arrays is .T.\n\na.shape, b.T.shape\n\n((1, 5), (5, 1))\n\n\nNow that the two arrays have the matching shapes, we can calculate their dot product using the @ operator.\n\na @ b.T\n\narray([[0.56077435]])\n\n\nThe result of the dot product is an array of shape \\((1,1)\\), or a scalar. In general, the result of a dot product has a shape of \\((n, m)\\)"
  },
  {
    "objectID": "code/Session1.html#best-fit-line",
    "href": "code/Session1.html#best-fit-line",
    "title": "Python Basics",
    "section": "Best fit line",
    "text": "Best fit line\nLet us now simulate a some data and find the best fit line, a line that minimizes the average distance of all data points to the line.\nHere our X is a vector of shape \\((100, 1)\\) samples from a standard normal distribution. Here we have \\(100\\) points with \\(1\\) feature for each point.\nThe y is 10 times x, with the shape \\((100, 1)\\).\n\\[y = X \\cdot coeff\\]\n\nX = np.random.randn(100, 1)\ncoeff = np.ones((1, 1)) * 10\ny = X @ coeff\ny.shape\n\n(100, 1)"
  },
  {
    "objectID": "code/Session1.html#fitting-the-data-using-sklearn",
    "href": "code/Session1.html#fitting-the-data-using-sklearn",
    "title": "Python Basics",
    "section": "Fitting the data using sklearn",
    "text": "Fitting the data using sklearn\nIn the next session, we will formulate a closed-form solution for finding the parameters of linear regression.\nFor now, let’s use the scikit-learn package to find the best line.\nWe’ll import the LinearRegression class from the linear_model submodule of sklearn. Each submodule of sklearn contains classes for that particular topic.\n\nfrom sklearn.linear_model import LinearRegression\n\nNow, let’s create an instance of the LinearRegression model and fit it to the data.\n\nmodel = LinearRegression()\n\n\nmodel.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nMost sklearn classes involve two steps: 1. Instantiating the class. This is where you provide the necessary hyper-parameters. - model = LinearRegression() 2. Fitting the data. - model.fit(X,y)\nNext, we can retrieve the coefficients of the fitted model by accessing the .coeff_ attribute of the model object.\n\na = model.coef_\na\n\narray([[10.]])\n\n\nIn our case, the coefficient is an array of shape \\((1,1)\\). This is in line with the shape of our X (\\(n,1\\)) and y (\\(n, 1\\)).\nWe can also access the intercept of the model using the .intercept_ attribute. Since we didn’t add an intercept in our data simulation, the value of the intercept should be \\(0\\).\nNote. In numerical computations, very small numbers (e.g. \\(2 \\times 10^{-16}\\)) are considered to be \\(0\\).\n\nb = model.intercept_\nb\n\narray([2.22044605e-16])"
  },
  {
    "objectID": "code/Session1.html#plotting-the-line-on-the-data",
    "href": "code/Session1.html#plotting-the-line-on-the-data",
    "title": "Python Basics",
    "section": "Plotting the line on the data",
    "text": "Plotting the line on the data\nWe can now plot the line and the data to see the fit.\nFirst, we get the predictions from the model using its fitted parameters a and b.\n\ny_pred = X @ a + b\n\nWe’ll use seaborn and Matplotlib to create two separate plots on the same axis, ax.\n\nax = sns.scatterplot(x=X[:,0], y=y[:,0], label=\"Actual y\")\nax = sns.lineplot(x=X[:,0], y=y_pred[:,0], ax=ax, color='red', label=\"Predicted y\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x120000280&gt;"
  },
  {
    "objectID": "code/Model.html",
    "href": "code/Model.html",
    "title": "Model capacity",
    "section": "",
    "text": "Model capacity refers to the ability of a machine learning model to capture and represent complex relationships between the input variables (features) and the target variable (labels). It determines the complexity and flexibility of the model in fitting the training data.\nIn other words, model capacity represents the amount of information or patterns that a model can learn from the data. A model with a high capacity can learn intricate relationships in the training data, which may result in overfitting. On the other hand, a model with low capacity may not be able to capture the underlying patterns in the data, leading to underfitting.\nThe capacity of a model can be controlled by adjusting its architectural complexity. For example, in neural networks, increasing the number of hidden layers and hidden units increases the capacity of the model.\nMathematically, we can define model capacity as the number of parameters that the model has to learn. For example, in linear regression, the model capacity is determined by the number of coefficients (slope and intercept) that the model needs to estimate. In neural networks, the model capacity is determined by the number of weights and biases associated with each neuron.\nNow, let’s understand the concept of model capacity using a simple example with polynomial regression. Polynomial regression is a form of linear regression where the relationship between the input feature (x) and the target variable (y) is modeled as an nth-degree polynomial.\nFirst, let’s generate some synthetic data that follows a quadratic relationship between x and y:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-5, 5, 100)\ny = 2 * x ** 2 + np.random.normal(0, 4, 100)\n\n# Plot the data\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Quadratic Relationship')\nplt.show()\n\n\n\n\nBy visualizing the data, we can observe that the relationship between x and y follows a quadratic curve. Now, let’s try fitting this data using different polynomial regression models with different degrees of complexity.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Fit linear regression model\nlinear_model = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())\nlinear_model.fit(x.reshape(-1, 1), y)\n\n# Fit quadratic regression model\nquadratic_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\nquadratic_model.fit(x.reshape(-1, 1), y)\n\n# Fit cubic regression model\ncubic_model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\ncubic_model.fit(x.reshape(-1, 1), y)\n\n# Predict on new data points\nx_test = np.linspace(-5, 5, 100)\ny_linear = linear_model.predict(x_test.reshape(-1, 1))\ny_quadratic = quadratic_model.predict(x_test.reshape(-1, 1))\ny_cubic = cubic_model.predict(x_test.reshape(-1, 1))\n\n# Plot the regression curves\nplt.scatter(x, y, label='Data')\nplt.plot(x_test, y_linear, label='Linear')\nplt.plot(x_test, y_quadratic, label='Quadratic')\nplt.plot(x_test, y_cubic, label='Cubic')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Polynomial Regression')\nplt.legend()\nplt.show()\n\n\n\n\nIn the above code, we fit three polynomial regression models: linear, quadratic, and cubic regression. The degree of the polynomial is indicated by the degree parameter in the PolynomialFeatures class. We can observe that as we increase the complexity (degree) of the polynomial, the models can better capture the underlying quadratic relationship.\nHowever, it’s important to note that high-capacity models can also be prone to overfitting, especially when the amount of training data is limited. Therefore, it’s crucial to balance the model’s capacity with the complexity of the problem at hand and the size of the training dataset.\nTo summarize, model capacity refers to the amount of information or patterns a machine learning model can learn from the data. It is determined by the number of parameters that the model needs to estimate. Increasing the model’s capacity can improve its ability to represent complex relationships in the data, but it can also increase the risk of overfitting."
  },
  {
    "objectID": "code/overfi.html",
    "href": "code/overfi.html",
    "title": "Overfitting",
    "section": "",
    "text": "Overfitting occurs when a machine learning model performs very well on the training data, but fails to generalize well on unseen data. It happens when the model captures noise and random fluctuations in the training data instead of the underlying pattern or relationship.\nOne way to understand overfitting is to consider fitting a polynomial to data points. The degree of the polynomial determines its complexity. A higher degree polynomial can fit the training data more closely, but it may also capture random noise, resulting in poor performance on new data.\nTo demonstrate overfitting using polynomials, we will generate a dataset with some noise and fit polynomials of different degrees to it. We will then visualize the models to see how they fit the data.\nLet’s start by importing the necessary libraries and generating the dataset. We will use the numpy library for array operations and random number generation, and the matplotlib library for data visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the dataset\nnp.random.seed(0)\nX = np.linspace(-1, 1, 20)\ny = 2 * X + np.random.normal(0, 0.5, 20)\n\nIn the above code, we first import the required libraries: numpy and matplotlib.pyplot. We then set a random seed to ensure reproducibility.\nNext, we create an array X with 20 equally spaced points between -1 and 1 using the linspace function. We add some noise to the array y using the numpy.random.normal function. Here, we use a linear relationship with some Gaussian noise to generate our dataset.\nNow, we will plot the generated dataset to visualize it.\n\nplt.scatter(X, y)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Dataset')\nplt.show()\n\n\n\n\nThe code above uses the scatter function from matplotlib.pyplot to create a scatter plot of the dataset. It also adds labels to the x and y axes and sets a title for the plot. Finally, the show function is called to display the plot.\nNow, let’s fit polynomials of different degrees to the dataset and see how they fit the data.\n\n# Polynomial fitting and visualization\ndegrees = [1, 3, 9, 12]\n\nplt.scatter(X, y)\n\nfor degree in degrees:\n    # Fit polynomial of given degree\n    coeffs = np.polyfit(X, y, degree)\n    poly = np.poly1d(coeffs)\n    \n    # Generate x values for plotting\n    x_plot = np.linspace(-1, 1, 100)\n    \n    # Compute predicted y values\n    y_plot = poly(x_plot)\n    \n    # Plot the polynomial\n    plt.plot(x_plot, y_plot, label=f'Degree {degree}')\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Fitting Polynomials')\nplt.legend()\nplt.show()\n\n\n\n\nIn the above code, we define a list degrees with the degrees of the polynomials that we want to fit to the dataset. We then iterate over each degree and perform the following steps:\n\nFit a polynomial of the given degree to the dataset using the polyfit function from numpy.\nCreate a polynomial object using the computed coefficients using the poly1d function from numpy.\nGenerate a set of x values for plotting using the linspace function from numpy.\nCompute the predicted y values for the generated x values using the polynomial.\nPlot the polynomial curve using the plot function from matplotlib.pyplot with a label indicating the degree of the polynomial.\n\nFinally, we add labels and a title to the plot, and display a legend to distinguish the different polynomial curves.\nWhen you run the code, you will see a plot showing the dataset points as scatter points, and different polynomial curves fitted to the data. From this visualization, you can observe the effect of overfitting as the degree of the polynomial increases. Higher degree polynomials tend to fit the training data more closely, but they also capture random noise and fluctuations, resulting in poor generalization to new data."
  },
  {
    "objectID": "code/multip1.html",
    "href": "code/multip1.html",
    "title": "Multiple Linear Regression - Part 1",
    "section": "",
    "text": "In machine learning and statistics, multiple linear regression is a commonly used technique to model the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the independent variables and the dependent variable.\nThe general form of multiple linear regression can be written as:\n\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\\]\nWhere:\nTo perform multiple linear regression, we can use the Ordinary Least Squares (OLS) method, which aims to find the coefficients that minimize the sum of the squared differences between the actual and predicted values."
  },
  {
    "objectID": "code/multip1.html#design-matrix",
    "href": "code/multip1.html#design-matrix",
    "title": "Multiple Linear Regression - Part 1",
    "section": "Design Matrix",
    "text": "Design Matrix\nIn multiple linear regression, we represent the independent variables as a matrix called the design matrix. The design matrix, denoted by \\(X\\), has one row for each observation and one column for each independent variable.\nFor example, if we have n observations and p independent variables, the design matrix \\(X\\) will be an n x p matrix."
  },
  {
    "objectID": "code/multip1.html#proof-of-ols",
    "href": "code/multip1.html#proof-of-ols",
    "title": "Multiple Linear Regression - Part 1",
    "section": "Proof of OLS",
    "text": "Proof of OLS\nThe OLS method minimizes the sum of squared errors (SSE) between the actual dependent variable values and the predicted values. The SSE can be written as:\n\\[SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nWhere:\n\n\\(y_i\\) is the actual dependent variable value for the i-th observation,\n\\(\\hat{y}_i\\) is the predicted dependent variable value for the i-th observation.\n\nTo find the coefficients that minimize SSE, we differentiate SSE with respect to each coefficient and set the derivatives to zero.\nLet’s derive the formula for OLS step by step.\n\nThe predicted value of the dependent variable can be written as:\n\n\\[\\hat{y} = X\\beta\\]\nWhere:\n\n\\(\\hat{y}\\) is an n x 1 vector of predicted values,\n\\(X\\) is the design matrix,\n\\(\\beta\\) is a vector of coefficients.\n\n\nThe SSE can be expressed in matrix form as:\n\n\\[SSE = (\\mathbf{y} - \\mathbf{\\hat{y}})^T (\\mathbf{y} - \\mathbf{\\hat{y}})\\]\nWhere:\n\n\\(\\mathbf{y}\\) is an n x 1 vector of actual dependent variable values,\n\\(\\mathbf{\\hat{y}}\\) is an n x 1 vector of predicted dependent variable values.\n\n\nExpanding the above equation, we get:\n\n\\[SSE = (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)\\]\n\nExpanding the squared term, we get:\n\n\\[SSE = \\mathbf{y}^T\\mathbf{y} - 2\\beta^TX^T\\mathbf{y} + \\beta^TX^TX\\beta\\]\n\nTo minimize SSE with respect to \\(\\beta\\), we differentiate SSE with respect to \\(\\beta\\) and set the derivative to zero:\n\n\\[\\frac{\\partial SSE}{\\partial \\beta} = -2X^T\\mathbf{y} + 2X^TX\\beta = 0\\]\n\nSolving for \\(\\beta\\), we get:\n\n\\[X^TX\\beta = X^T\\mathbf{y}\\]\n\\[\\beta = (X^TX)^{-1}X^T\\mathbf{y}\\]\nThe above formula gives us the optimal values for \\(\\beta\\) that minimize SSE."
  },
  {
    "objectID": "code/multip2.html",
    "href": "code/multip2.html",
    "title": "Multiple Linear Regression - Part 2",
    "section": "",
    "text": "Multiple linear regression is a powerful technique used for predicting a continuous outcome variable based on multiple predictor variables. In this tutorial, we will learn how to perform multiple linear regression using a design matrix in Python.\nFirst, let’s define the problem. In multiple linear regression, we have a dependent variable (also called the response or target variable) and several independent variables (also called features, input variables, or predictors). The goal is to find the best linear relationship between the predictors and the target variable.\nThe general equation for a multiple linear regression model with ‘p’ predictors is given by:\nY = β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ + ε\nWhere: - Y is the target variable - X₁, X₂, …, Xₚ are the predictor variables - β₀, β₁, β₂, …, βₚ are the coefficients (or weights) of the predictors - ε is the error term, representing the randomness or noise in the relationship\nTo estimate the coefficients (β₀, β₁, β₂, …, βₚ), we need to minimize the sum of squared residuals, which measures the differences between the actual values of the target variable (Y) and the predicted values from the regression model.\nIn multiple linear regression, the predictors are often organized into a design matrix (X), where each row represents an observation and each column represents a predictor variable.\nNow let’s see how to perform multiple linear regression using a design matrix in Python.\n\nimport numpy as np\n\n# Define the design matrix X\nX = np.array([[3,  3, -3],\n              [-4, 5,  6],\n              [7, -8,  9]])\n\n# Define the target variable Y\nY = np.array([10, 20, 30])\n\n# Calculate beta coefficients using the normal equation\nbeta = np.linalg.inv(X.T @ X) @ X.T @ Y\n\nprint('Beta coefficients:', beta)\n\nBeta coefficients: [3.56321839 2.98850575 3.2183908 ]\n\n\nIn the above code, we first import the necessary libraries. Then, we define the design matrix X as a 2-dimensional numpy array that contains the predictor variables. Each row represents an observation, and each column represents a predictor variable. We also define the target variable Y as a 1-dimensional numpy array.\nNext, we use the normal equation to calculate the beta coefficients. The normal equation is given by:\n\\[β = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y\\]\n\n\\(X^T\\) is the transpose of X\n\\((X^T \\cdot X)^{-1}\\) is the inverse of the matrix product of \\(X^T\\) and \\(X\\)\n\\(X^T \\cdot Y\\) is the matrix product of \\(X^T\\) and \\(Y\\)\n\nFinally, we print the beta coefficients, which represent the weights or coefficients of the predictors in the multiple linear regression model."
  },
  {
    "objectID": "code/L2 nor.html",
    "href": "code/L2 nor.html",
    "title": "L2 Normalization for Multiple Linear Regression with Design Matrix X",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 normalization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 normalization for multiple linear regression using Python:\n\nStep 1: Importing the Required Libraries\nWe will start by importing the necessary libraries for our implementation.\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\n\nStep 2: Generating Sample Data\nTo demonstrate the L2 normalization for multiple linear regression, we will generate some sample data.\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n\nStep 3: Fitting the Multiple Linear Regression Model\nWe will fit the multiple linear regression model using the OLS method.\n\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n\nStep 4: Fitting the Ridge Regression Model with L2 Normalization\nNow, let’s fit the Ridge regression model with L2 normalization to the data.\n\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 normalization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n\nStep 5: Comparing Results\nFinally, let’s compare the coefficients and MSE of the OLS and Ridge models.\n\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 normalization on the coefficients and model performance.\nBy introducing L2 normalization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\nNote that L2 normalization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 normalization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term."
  },
  {
    "objectID": "code/L2 reg.html",
    "href": "code/L2 reg.html",
    "title": "L2 Regularization for Multiple Linear Regression with Design Matrix X",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 regularization for multiple linear regression using Python:\n\nStep 1: Importing the Required Libraries\nWe will start by importing the necessary libraries for our implementation.\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\n\nStep 2: Generating Sample Data\nTo demonstrate the L2 regularization for multiple linear regression, we will generate some sample data.\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nN = 100\n\n# Number of features\nD = 3\n\n# Generate random X matrix\nX = np.random.randn(N, D)\n\n# Generate random true coefficients\ntrue_coeffs = np.random.randn(D)\n\n# Generate random noise term\nnoise = 0.5 * np.random.randn(N)\n\n# Generate target variable y using true coefficients, X, and noise\ny = np.dot(X, true_coeffs) + noise\n\nIn this step, we generate a random X matrix with dimensions N x D, where N represents the number of samples and D represents the number of features. We also generate random true coefficients and noise to generate the target variable y.\n\n\nStep 3: Fitting the Multiple Linear Regression Model\nWe will fit the multiple linear regression model using the OLS method.\n\n# Fit the OLS model\nols = Ridge(alpha=0)  # Set the regularization parameter alpha to 0 for OLS\nols.fit(X, y)\n\n# Predict the target variable\ny_pred_ols = ols.predict(X)\n\n# Calculate the MSE for OLS model\nmse_ols = mean_squared_error(y, y_pred_ols)\n\n# Print the coefficients and MSE of the OLS model\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"OLS MSE:\", mse_ols)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nOLS MSE: 0.2041660939560613\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 0, which effectively removes the regularization term and gives us the OLS solution. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the OLS model.\n\n\nStep 4: Fitting the Ridge Regression Model with L2 Regularization\nNow, let’s fit the Ridge regression model with L2 regularization to the data.\n\n# Fit the Ridge regression model\nridge = Ridge(alpha=1)  # Set the regularization parameter alpha to 1 for L2 regularization\nridge.fit(X, y)\n\n# Predict the target variable\ny_pred_ridge = ridge.predict(X)\n\n# Calculate the MSE for Ridge model\nmse_ridge = mean_squared_error(y, y_pred_ridge)\n\n# Print the coefficients and MSE of the Ridge model\nprint(\"Ridge Coefficients:\", ridge.coef_)\nprint(\"Ridge MSE:\", mse_ridge)\n\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we create an instance of the Ridge regression model from scikit-learn. We set the regularization parameter alpha to 1 to control the amount of regularization. The higher the value of alpha, the stronger the regularization effect. We fit the model on the data, predict the target variable, and calculate the mean squared error (MSE) to evaluate the performance of the Ridge model.\n\n\nStep 5: Comparing Results\nFinally, let’s compare the coefficients and MSE of the OLS and Ridge models.\n\n# Compare coefficients\nprint(\"OLS Coefficients:\", ols.coef_)\nprint(\"Ridge Coefficients:\", ridge.coef_)\n\n# Compare MSE\nprint(\"OLS MSE:\", mse_ols)\nprint(\"Ridge MSE:\", mse_ridge)\n\nOLS Coefficients: [-1.25960771  1.6134695  -0.14386058]\nRidge Coefficients: [-1.24885203  1.59773433 -0.1439717 ]\nOLS MSE: 0.2041660939560613\nRidge MSE: 0.2045516626562138\n\n\nIn this step, we compare the coefficients and MSE of the OLS and Ridge models to observe the effect of L2 regularization on the coefficients and model performance.\nBy introducing L2 regularization, Ridge regression helps to address multicollinearity issues, which can arise when independent variables are highly correlated with each other. The regularization term tends to shrink the coefficients towards zero, reducing their magnitude and reducing overfitting.\nThe choice of the regularization parameter alpha is crucial, as it balances the trade-off between model simplicity and performance. It is typically determined using techniques like cross-validation or grid search.\nNote that L2 regularization is just one of the regularization techniques used in multiple linear regression. There are other methods like L1 regularization (Lasso regression), which uses the sum of absolute magnitudes of the coefficients as the penalty term."
  },
  {
    "objectID": "code/Genera.html",
    "href": "code/Genera.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "To understand the generalization of L2 regularization on the test set for multiple linear regression with design matrix X, let’s first discuss what L2 regularization is and how it is applied in multiple linear regression.\nL2 Regularization in Multiple Linear Regression:\nMultiple Linear Regression is a technique used to model the relationship between a dependent variable and multiple independent variables. The goal is to find the best fitting line that minimizes the sum of squared errors.\nHowever, in some cases, the model can become overfit, meaning it fits the training data too closely and does not generalize well to unseen data. This can lead to poor performance on the test set.\nL2 regularization, also known as Ridge regression, is a technique used to prevent overfitting. It adds a penalty term to the least squares objective function, which reduces the magnitude of the coefficients and helps in controlling the complexity of the model.\nThe L2 regularization term is given by:\n\\(\\text{L2 regularization term} = \\lambda \\sum_{i=1}^{m} \\beta_i^2\\)\nWhere:\n\n() is the regularization parameter, which controls the amount of regularization applied. A higher value of () results in more regularization.\n(_i) is the coefficient associated with the (i)th independent variable.\n\nIncluding the L2 regularization term in the objective function, the cost function for multiple linear regression with L2 regularization becomes:\n\\(\\text{{Cost function}} = \\frac{1}{2m} \\left( \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right)\\)\nWhere:\n\n(m) is the number of training examples.\n(h_(x^{(i)})) is the predicted value for the (i)th example using the model’s parameters ().\n(y^{(i)}) is the actual value for the (i)th example.\n\nNow, let’s see how we can apply L2 regularization in multiple linear regression using Python.\nApplying L2 Regularization in Multiple Linear Regression:\nFirst, we need to import the required libraries for our example:\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nNext, we need to create the design matrix X and the target variable y. The design matrix X contains the values of the independent variables, and the target variable y contains the corresponding dependent variable values.\n\n# Create design matrix X\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Create target variable y\ny = np.array([10, 20, 30])\n\nWe split the data into training and test sets using the train_test_split function from scikit-learn. The training set will be used to train the model, and the test set will be used to evaluate its performance.\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nNext, we create an instance of the Ridge regression model and train it on the training set.\n\n# Create an instance of the Ridge regression model\nridge = Ridge(alpha=0.5)\n\n# Train the model on the training set\nridge.fit(X_train, y_train)\n\nRidge(alpha=0.5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=0.5)\n\n\nWe can now use the trained model to make predictions on the test set.\n\n# Make predictions on the test set\ny_pred = ridge.predict(X_test)\n\nFinally, we can evaluate the performance of the model using a performance metric such as mean squared error.\n\n# Calculate the mean squared error on the test set\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\n\nMean Squared Error: 0.28698979591836626\n\n\nThe mean squared error will give us an idea of how well the model is performing on the test set. A lower mean squared error indicates better performance.\nThis is how L2 regularization is applied in multiple linear regression to generalize the model on the test set. By adding a penalty term to the objective function, we can control the complexity of the model and prevent overfitting."
  },
  {
    "objectID": "code/Improv.html",
    "href": "code/Improv.html",
    "title": "L2 Regularization",
    "section": "",
    "text": "In multiple linear regression, we aim to find the relationship between the independent variables X and the dependent variable y. This relationship is represented by a linear equation of the form:\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\nwhere:\n\n\\(y\\) is the dependent variable (target variable)\n\\(X\\) is the design matrix that consists of independent variables (features)\n\\(\\beta\\) is the vector of coefficients (slopes) for each feature in X\n\\(\\epsilon\\) is the error term\n\nThe Ordinary Least Squares (OLS) method is a common approach used to estimate the values of the coefficients \\(\\beta\\) that minimize the sum of squared residuals.\nL2 regularization, also known as Ridge regression, is a method used to regularize the OLS model to improve its performance and reduce the effect of multicollinearity among the independent variables.\nL2 regularization involves adding the squared magnitudes of the coefficients as a penalty term to the loss function of the OLS model. This penalty term is controlled by a hyperparameter called regularization parameter \\(\\lambda\\).\nThe loss function for multiple linear regression with L2 regularization is given by:\n\\[\\begin{equation}\nL(\\beta) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|^2\\right]\n\\end{equation}\\]\nwhere:\n\n\\(N\\) is the number of samples in the dataset\n\\(y_i\\) is the target value for the i-th sample\n\\(X_i\\) is the feature vector for the i-th sample\n\\(\\|\\beta\\|^2\\) represents the squared L2 norm of the coefficient vector \\(\\beta\\)\n\nThe goal is to find the value of \\(\\beta\\) that minimizes this loss function.\nLet’s now implement L2 regularization for multiple linear regression using Python:\nFirst, let’s import the necessary libraries:\n\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nNext, we’ll generate some random data for demonstration purposes:\n\n# Generate random data\nnp.random.seed(0)\nX = np.random.rand(100, 5)\nw = np.random.rand(5, 1)\ny = X**2 @ w + np.random.randn(100)*5\n\nWe split the data into training and test sets:\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nTo apply L2 regularization, we need to scale the input features using the StandardScaler:\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nLet’s use OLS without L2 regularization for a regression model:\n\n# Create a regression model without penalty\nno_ridge = Ridge(alpha=0.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nno_ridge.fit(X_train_scaled, y_train)\n\nRidge(alpha=0.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=0.0)\n\n\nNext, we can evaluate the trained model on the test set:\n\n# Evaluate the model on the test set\nscore = no_ridge.score(X_test_scaled, y_test)\nprint(f\"No-penalty Regression Score: {score}\")\n\nNo-penalty Regression Score: 0.9411011679689105\n\n\nThe score represents the coefficient of determination \\((R^2)\\) of the prediction. Higher values of \\(R^2\\) indicate better model performance.\nWe can now create and train the ridge regression model:\n\n# Create a ridge regression model\nridge = Ridge(alpha=10.0)  # alpha is the regularization parameter (lambda)\n\n# Train the model\nridge.fit(X_train_scaled, y_train)\n\nRidge(alpha=10.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=10.0)\n\n\nFinally, we can evaluate the trained model on the test set:\n\n# Evaluate the model on the test set\nscore = ridge.score(X_test_scaled, y_test)\nprint(f\"Ridge Regression Score: {score}\")\n\nRidge Regression Score: 0.9439855609099314\n\n\nBy applying L2 regularization using ridge regression, we can improve the generalization of the multiple linear regression model by reducing overfitting and improving its performance on unseen data."
  },
  {
    "objectID": "code/bias v.html",
    "href": "code/bias v.html",
    "title": "Bias-Variance Trade-off",
    "section": "",
    "text": "The bias-variance trade-off is a fundamental concept in machine learning that helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. In supervised learning problems, we aim to find a model, usually a mathematical function, that can accurately predict the target variable based on the input features."
  },
  {
    "objectID": "code/bias v.html#bias-and-variance",
    "href": "code/bias v.html#bias-and-variance",
    "title": "Bias-Variance Trade-off",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nBefore diving into the bias-variance trade-off, let’s briefly explain two important concepts: bias and variance.\n\nBias measures how much our model’s predictions deviate from the true values. A model with high bias oversimplifies the underlying relationship between the features and the target variable. This can lead to underfitting, where the model fails to capture the patterns and relationships in the data.\nVariance measures the variability of model predictions for different training sets. A model with high variance is too sensitive to the specific training examples and does not generalize well to new, unseen data. This can lead to overfitting, where the model fits the training data too well but performs poorly on new data.\n\nThe aim is to find a good balance between bias and variance, where the model captures the underlying patterns in the training data without overfitting."
  },
  {
    "objectID": "code/bias v.html#bias-variance-trade-off",
    "href": "code/bias v.html#bias-variance-trade-off",
    "title": "Bias-Variance Trade-off",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\nThe bias-variance trade-off states that as we decrease the bias of a model (increasing complexity), we tend to increase its variance, and vice versa. This trade-off occurs because model complexity allows for a better fit to the training data, but at the risk of poor performance on new data.\nTo illustrate this concept, let’s consider a regression problem where we can adjust the complexity of a model by changing the degree of the polynomial used for fitting the data."
  },
  {
    "objectID": "code/bias v.html#example",
    "href": "code/bias v.html#example",
    "title": "Bias-Variance Trade-off",
    "section": "Example",
    "text": "Example\n\nImporting Required Libraries\nWe start by importing the necessary libraries: NumPy for numerical operations and matplotlib for visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# To plot the graphs inline in Jupyter Notebook\n%matplotlib inline\n\n\n\nGenerating Synthetic Data\nNext, we generate some synthetic data with a nonlinear relationship between the input features and the target variable using the numpy library.\n\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Generate input features\nX = np.linspace(-5, 5, 100).reshape(-1, 1)\n\n# Generate target variable with noise\nY_true = X**3 - X**2 + X + np.random.randn(100, 1)\n\nHere, we generate 100 samples of input features X ranging from -5 to 5. The target variable Y_true is generated using a cubic relationship with some random Gaussian noise.\n\n\nFitting Polynomial Models\nWe will now fit polynomial models with different degrees to the synthetic data and observe the effect of model complexity on bias and variance.\n\n# Create a function to fit polynomial models and visualize the results\ndef fit_polynomial(X, Y_true, degree):\n    # Fit polynomial regression model\n    poly_features = np.polynomial.Polynomial.fit(X.flatten(), Y_true.flatten(), degree)\n    Y_pred = poly_features(X.flatten())\n    \n    # Compute bias and variance\n    bias = np.mean(np.abs(Y_true - Y_pred))\n    variance = np.var(Y_pred)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X, Y_true, label='True Data', color='b')\n    plt.plot(X, Y_pred, label='Predicted', color='r')\n    plt.title(f'Polynomial Regression (Degree = {degree})\\nBias = {bias:.2f}, Variance = {variance:.2f}')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n# Fit polynomial models with degrees 1, 2, 3, 5, 10\nfit_polynomial(X, Y_true, degree=1)\nfit_polynomial(X, Y_true, degree=2)\nfit_polynomial(X, Y_true, degree=3)\nfit_polynomial(X, Y_true, degree=5)\nfit_polynomial(X, Y_true, degree=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this code block, we define a function fit_polynomial that takes the input features X, true target variable Y_true, and the degree of the polynomial model to be fitted as arguments. Inside the function, we use the numpy.polynomial.Polynomial.fit function to fit a polynomial regression model with the desired degree.\nFor each degree of the polynomial model, we compute the bias and variance using the mean absolute error and variance of the predicted values. Then, we plot the true data points, the predicted curve, and display the bias and variance in the title of the plot.\n\n\nAnalysis and Observations\nBy running the code above, we get a series of plots showing the true data points and the predicted curves for polynomial regression models with different degrees. Each plot also displays the corresponding bias and variance values.\n\nFor a linear model (degree=1), the model is too simple to capture the underlying cubic relationship in the data. Hence, it has a high bias and performs poorly in terms of fitting the data.\nAs the degree of the polynomial model increases, the model can fit the data more accurately, resulting in reduced bias. However, as the complexity increases (degree=5 and 10), we observe that the models start to capture the random fluctuations in the data, resulting in higher variance. These models may fit the training data very well but are likely to perform poorly on unseen data.\nThe model with a degree of 3 strikes a good balance between bias and variance, as it captures the underlying cubic relationship while avoiding overfitting."
  },
  {
    "objectID": "code/bias v.html#conclusion",
    "href": "code/bias v.html#conclusion",
    "title": "Bias-Variance Trade-off",
    "section": "Conclusion",
    "text": "Conclusion\nThe bias-variance trade-off is a fundamental concept in machine learning. It helps us understand the relationship between the complexity of a model and its ability to generalize to unseen data. By finding the right balance between bias and variance, we can develop models that accurately represent the patterns in the data without overfitting or oversimplifying the relationships."
  },
  {
    "objectID": "code/Linear_classification.html",
    "href": "code/Linear_classification.html",
    "title": "Linear least squares",
    "section": "",
    "text": "Linear least squares is a technique used for regression problems, where we aim to predict continuous numerical values. However, it can also be used for classification tasks by transforming the problem into a binary classification problem.\nIn linear least squares for classification, we use a linear model to classify data into two classes. We assign class labels of -1 and 1 to the two classes. The goal is to find a linear boundary that best separates the two classes, minimizing the sum of squared distances between the data points and the decision boundary.\nLet’s see how we can do this in Python:\nFirst, we need to import the required libraries: numpy and matplotlib.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNext, let’s generate some synthetic data with two classes. We will use the make_classification function from the sklearn.datasets module to create a random dataset.\n\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\nNow, let’s visualize the data using a scatter plot:\n\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n\n\n\n\nWe have plotted the data points for each class on a scatter plot.\nTo apply linear least squares for classification, we need to add a column of ones to our feature matrix X to incorporate the bias term in the linear equation.\n\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n\nNow, let’s define our linear model and solve for the optimal parameters using linear least squares.\n\ntheta = np.linalg.inv(X.T @ X) @ X.T @ y\n\nHere, theta is the vector of parameters that defines our linear model. The equation used to solve for theta is:\n\\(\\theta = (X^T X)^{-1} X^T y\\)\nFinally, let’s visualize the decision boundary of our linear model along with the data points.\n\nplt.scatter(X[y == 1][:,1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == 0][:,1], y[y == 0], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\n\n# Plotting the decision boundary\nx_boundary = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)\n# x2_boundary = -(theta[0] + theta[1]*x1_boundary) / theta[2]\ny_boundary = x_boundary * theta[1] +  theta[0]\nplt.plot(x_boundary, y_boundary, color='black', linewidth=2)\n\nplt.show()\n\n\n\n\nWe have plotted the decision boundary determined by our linear model, which separates the two classes.\nLinear least squares for classification is a simple technique for linearly separable datasets. Note that this approach assumes the data points are linearly separable and does not work well for nonlinear classification problems."
  },
  {
    "objectID": "code/SVM.html",
    "href": "code/SVM.html",
    "title": "SVM",
    "section": "",
    "text": "Support Vector Machines (SVM) is a powerful machine learning algorithm used for both classification and regression tasks. In this tutorial, we will focus on how to use SVM for classification.\nBefore diving into the code, let’s understand the intuition behind SVM."
  },
  {
    "objectID": "code/SVM.html#intuition-behind-svm",
    "href": "code/SVM.html#intuition-behind-svm",
    "title": "SVM",
    "section": "Intuition behind SVM",
    "text": "Intuition behind SVM\nSVM is based on the concept of finding a hyperplane that best separates the data points belonging to different classes. The hyperplane is determined by support vectors, which are the data points closest to the decision boundary.\nIn a binary classification problem, SVM aims to find a hyperplane that maximizes the margin between the support vectors of the two classes. The margin is the distance between the hyperplane and the nearest data points from each class.\nThe optimal hyperplane can be described by the equation:\n\\(w^T x - b = 0\\)\nwhere \\(w\\) is the normal vector to the hyperplane and \\(b\\) is the bias.\nThe equation of the decision function is:\n\\(f(x) = sign(w^T x - b)\\)\nwhere \\(sign(\\cdot)\\) is the sign function.\nSVM can also handle non-linearly separable data by using a technique called the kernel trick. This technique transforms the original feature space into a higher-dimensional space, making the data linearly separable.\nNow, let’s implement SVM for a classification problem using the famous Iris dataset."
  },
  {
    "objectID": "code/SVM.html#importing-libraries-and-loading-the-dataset",
    "href": "code/SVM.html#importing-libraries-and-loading-the-dataset",
    "title": "SVM",
    "section": "Importing Libraries and Loading the Dataset",
    "text": "Importing Libraries and Loading the Dataset\nThe first step is to import the required libraries and load the dataset. We will use scikit-learn library, which provides a simple API for SVM implementation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nNext, let’s generate some synthetic data with two classes. We will use the make_classification function from the sklearn.datasets module to create a random dataset.\n\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(0)\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\ny = y*2 - 1\n\nWe have created a dataset with 100 samples, 1 feature, and 1 informative features.\nNow, let’s visualize the data using a scatter plot:\n\nplt.scatter(X[y == 1], y[y == 1], color='b', label='Class 1')\nplt.scatter(X[y == -1], y[y == -1], color='r', label='Class 0')\nplt.xlabel('Feature')\nplt.ylabel('Label')\nplt.legend()\nplt.show()\n\n\n\n\nWe have plotted the data points for each class on a scatter plot."
  },
  {
    "objectID": "code/SVM.html#training-the-support-vector-machine-classifier",
    "href": "code/SVM.html#training-the-support-vector-machine-classifier",
    "title": "SVM",
    "section": "Training the Support Vector Machine Classifier",
    "text": "Training the Support Vector Machine Classifier\nOnce we have loaded the dataset, we can proceed to train the SVM classifier.\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVM classifier object\nsvm = SVC(kernel='linear', C=1e10)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\nSVC(C=10000000000.0, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=10000000000.0, kernel='linear')\n\n\nIn this code block, we first split the data into training and testing sets using the train_test_split() function. We reserve 20% of the data for testing and set the random_state parameter for reproducibility purposes.\nNext, we create an SVM classifier object using the SVC class from scikit-learn. We specify the kernel parameter as ‘linear’ for a linear SVM.\nFinally, we train the SVM classifier using the fit() method by passing the training data (X_train) and the corresponding class labels (y_train)."
  },
  {
    "objectID": "code/SVM.html#making-predictions-and-evaluating-the-model",
    "href": "code/SVM.html#making-predictions-and-evaluating-the-model",
    "title": "SVM",
    "section": "Making Predictions and Evaluating the Model",
    "text": "Making Predictions and Evaluating the Model\nAfter training the SVM classifier, we can use it to make predictions on new, unseen data.\n\n# Make predictions on the testing set\ny_pred = svm.predict(X_test)\n\n# Evaluate the model\naccuracy = np.sum(y_pred == y_test) / len(y_test)\nprint(\"Accuracy: \", accuracy)\n\nAccuracy:  1.0\n\n\nIn this code block, we use the trained SVM classifier to make predictions on the testing set (X_test). The predicted class labels are stored in the y_pred variable.\nWe then evaluate the model by calculating the accuracy of the predictions. The accuracy is defined as the number of correctly classified data points divided by the total number of data points."
  },
  {
    "objectID": "code/SVM.html#visualizing-the-decision-boundary",
    "href": "code/SVM.html#visualizing-the-decision-boundary",
    "title": "SVM",
    "section": "Visualizing the Decision Boundary",
    "text": "Visualizing the Decision Boundary\nTo visualize the decision boundary and the support vectors, we can use the following code:\n\ndef plot_decision_boundary(classifier, X, y):\n    # Define the range of x values for the mesh grid\n    x_min, x_max = X.min() - 1, X.max() + 1\n\n    # Create a mesh grid\n    xx = np.linspace(x_min, x_max, 1000)[:, np.newaxis]\n\n    # Use the classifier to make predictions on the mesh grid\n    yy = classifier.decision_function(xx)\n\n    # get support vectors\n    i_sv = classifier.support_\n    sv_x = X[i_sv]\n    sv_y = y[i_sv]\n    print(f'Support vectors are: \\nX={sv_x}\\ny={sv_y}')\n\n    # get w and b\n    w = svm.coef_\n    b = svm.intercept_\n    print(f'W={w}\\nb={b}')\n    # print(f\"wX-b:\\n{w*X-b}\")\n\n    # where the decision function is zero\n    ind_0 = np.where((yy&lt;=5e-2) & (yy&gt;=-5e-2))[0]\n    # print(ind_0)\n    distance1 = xx[ind_0].mean() - sv_x[0]\n    distance2 = xx[ind_0].mean() - sv_x[1]\n    print(f'\\ndistance to \\nsupport 1: {distance1} \\nsupport 2:{distance2}')\n\n    # Plot the decision boundary and support vectors\n    plt.hlines(0,-2, 3)\n    plt.scatter(X, y, c=y, cmap=plt.cm.Paired, edgecolors='k')\n    plt.scatter(xx, yy, color='black', linewidth=3)\n    plt.scatter(sv_x,sv_y, color='red')\n    plt.scatter(X,w*X-b,color='yellow', alpha=0.5)\n    plt.scatter(xx[ind_0], np.zeros(len(ind_0),), marker='x', color='red')\n    plt.xlim(x_min, x_max)\n    # plt.ylim(-0.2, 1.2)\n    plt.xlabel('Feature')\n    plt.ylabel('label')\n    plt.show()\n\n# Visualize the decision boundary\nplot_decision_boundary(svm, X_train, y_train)\n\nSupport vectors are: \nX=[[-0.61120543]\n [ 0.65311592]]\ny=[-1  1]\nW=[[1.58187634]]\nb=[-0.0331486]\n\ndistance to \nsupport 1: [0.63295407] \nsupport 2:[-0.63136728]\n\n\n\n\n\nIn this code block, we define a helper function plot_decision_boundary() that takes a trained classifier object (svm), the training data (X_train), and the corresponding class labels (y_train) as input.\nThe function calculates the minimum and maximum values of the two features to define the plotting range. It then generates a mesh grid with a step size h and predicts the class labels for each point in the grid using predict() method.\nFinally, it plots the decision boundary by contouring the predicted class labels and scatter plots the training data points.\nRunning this code will display the decision boundary and the support vectors.\nThis is how we can implement SVM for classification in Python. SVM is a versatile algorithm and can be further fine-tuned by selecting different kernels and hyperparameters for better performance."
  },
  {
    "objectID": "w3/week3_exercise.html",
    "href": "w3/week3_exercise.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "!pip install imbalanced-learn\n\nCollecting imbalanced-learn\n  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: numpy&gt;=1.17.3 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.11.2)\nRequirement already satisfied: scikit-learn&gt;=1.0.2 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.3.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/davoodwadi/mambaforge/lib/python3.10/site-packages (from imbalanced-learn) (3.2.0)\nDownloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 235.6/235.6 kB 4.5 MB/s eta 0:00:00:00:01\nInstalling collected packages: imbalanced-learn\nSuccessfully installed imbalanced-learn-0.11.0\n\n\n\nfrom imblearn.datasets import fetch_datasets\n\n\n# Load the dataset\ndata = fetch_datasets()['mammography']\nX, y = data.data, data.target\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame(np.hstack([X,y.reshape(-1,1)]))\ndf.columns = [f\"feature_{i}\" for i in range(1,7)] + ['y']\ndf\n\n\n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\ny\n\n\n\n\n0\n0.230020\n5.072578\n-0.276061\n0.832444\n-0.377866\n0.480322\n-1.0\n\n\n1\n0.155491\n-0.169390\n0.670652\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n2\n-0.784415\n-0.443654\n5.674705\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n3\n0.546088\n0.131415\n-0.456387\n-0.859553\n-0.377866\n-0.945723\n-1.0\n\n\n4\n-0.102987\n-0.394994\n-0.140816\n0.979703\n-0.377866\n1.013566\n-1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11178\n-0.250012\n-0.377300\n-0.321142\n1.269157\n3.652984\n1.092791\n1.0\n\n\n11179\n0.281343\n-0.417112\n-0.366224\n0.851010\n2.789649\n1.345700\n1.0\n\n\n11180\n1.204988\n1.763724\n-0.501468\n1.562408\n6.489072\n0.931294\n1.0\n\n\n11181\n0.736644\n-0.222474\n-0.050653\n1.509665\n0.539269\n1.315229\n1.0\n\n\n11182\n0.177003\n-0.191508\n-0.501468\n1.578864\n7.750705\n1.555951\n1.0\n\n\n\n\n11183 rows × 7 columns\n\n\n\n\nX.shape, y.shape\n\n((11183, 6), (11183,))"
  },
  {
    "objectID": "code/naive.html",
    "href": "code/naive.html",
    "title": "Naive Bayes Classifier",
    "section": "",
    "text": "The Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification problems and is especially effective when dealing with high-dimensional data. Naive Bayes assumes that the presence of a particular feature in a class is independent of the presence of other features."
  },
  {
    "objectID": "code/naive.html#bayes-theorem",
    "href": "code/naive.html#bayes-theorem",
    "title": "Naive Bayes Classifier",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nBefore diving into Naive Bayes, let’s start with Bayes’ theorem. Bayes’ theorem allows us to calculate the probability of a hypothesis given some observed evidence. It is stated as:\n\\(P(h|e) = \\frac{{P(e|h) \\cdot P(h)}}{{P(e)}}\\)\nWhere:\n\n\\(P(h|e)\\) is the posterior probability of hypothesis \\(h\\) given evidence \\(e\\).\n\\(P(e|h)\\) is the probability of evidence \\(e\\) given hypothesis \\(h\\).\n\\(P(h)\\) is the prior probability of hypothesis \\(h\\).\n\\(P(e)\\) is the probability of evidence \\(e\\).\n\nIn the context of Naive Bayes, we can reframe this theorem as:\n\\(P(y|X) = \\frac{{P(X|y) \\cdot P(y)}}{{P(X)}}\\)\nWhere:\n\n\\(X\\) represents the input features.\n\\(y\\) represents the class or target variable.\n\\(P(y|X)\\) is the posterior probability of class \\(y\\) given features \\(X\\).\n\\(P(X|y)\\) is the probability of observing features \\(X\\) given class \\(y\\).\n\\(P(y)\\) is the prior probability of class \\(y\\).\n\\(P(X)\\) is the probability of observing features \\(X\\)."
  },
  {
    "objectID": "code/naive.html#naive-bayes-classifier",
    "href": "code/naive.html#naive-bayes-classifier",
    "title": "Naive Bayes Classifier",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\nThe Naive Bayes classifier assumes that the presence of each feature is independent of the presence of other features, given the class variable. This is where the “Naive” part comes in. Despite this simplifying assumption, Naive Bayes can still be very effective in practice, especially with text classification tasks.\nThere are three common types of Naive Bayes classifiers: 1. Gaussian Naive Bayes: It assumes that the features are normally distributed. 2. Multinomial Naive Bayes: It is suitable for discrete features (e.g., word counts). 3. Bernoulli Naive Bayes: It is suitable for binary features (e.g., true/false).\nIn this tutorial, we will focus on Gaussian Naive Bayes, which is commonly used for continuous features."
  },
  {
    "objectID": "code/naive.html#gaussian-naive-bayes",
    "href": "code/naive.html#gaussian-naive-bayes",
    "title": "Naive Bayes Classifier",
    "section": "Gaussian Naive Bayes",
    "text": "Gaussian Naive Bayes\nGaussian Naive Bayes assumes that the continuous features in each class are normally distributed. It calculates the mean and standard deviation for each feature in each class and uses a Gaussian probability density function to estimate the likelihood of observing a particular feature value given a class. The formula for the Gaussian probability density function is:\n\\(P(x|y) = \\frac{1}{{\\sqrt{{2\\pi\\sigma_y^2}}}} \\cdot e^{-\\frac{{(x - \\mu_y)^2}}{{2\\sigma_y^2}}}\\)\nWhere:\n\n\\(x\\) is the feature value.\n\\(y\\) is the class label.\n\\(\\mu_y\\) is the mean of the feature values in class \\(y\\).\n\\(\\sigma_y\\) is the standard deviation of the feature values in class \\(y\\).\n\\(\\pi\\) is the mathematical constant pi."
  },
  {
    "objectID": "code/naive.html#implementation",
    "href": "code/naive.html#implementation",
    "title": "Naive Bayes Classifier",
    "section": "Implementation",
    "text": "Implementation\nNow let’s see the implementation of Gaussian Naive Bayes in Python using the sklearn library.\nStep 1: Import the required libraries.\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nStep 2: Load the dataset.\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nStep 3: Split the dataset into training and testing sets.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 4: Create a Gaussian Naive Bayes classifier and fit it to the training data.\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nStep 5: Make predictions on the test data.\n\ny_pred = clf.predict(X_test)\n\nStep 6: Evaluate the accuracy of the classifier.\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 1.0\n\n\nThat’s it! You have successfully implemented Gaussian Naive Bayes for classification in Python using sklearn.\nNote: This tutorial only covers the basic usage of Naive Bayes. There are many other aspects and variations of Naive Bayes that you can explore, such as Laplace smoothing, handling missing values, and dealing with categorical features."
  },
  {
    "objectID": "code/multin.html",
    "href": "code/multin.html",
    "title": "Multinomial Naive Bayes",
    "section": "",
    "text": "In machine learning, Naive Bayes is a probabilistic algorithm that is based on Bayes’ theorem. It is commonly used for classification problems and is known for its simplicity and efficiency. In particular, Multinomial Naive Bayes is a variation of the Naive Bayes algorithm that is specifically designed for discrete features, such as word counts in text documents."
  },
  {
    "objectID": "code/multin.html#bayes-theorem",
    "href": "code/multin.html#bayes-theorem",
    "title": "Multinomial Naive Bayes",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nTo understand how Multinomial Naive Bayes works, let’s first review Bayes’ theorem. Bayes’ theorem provides a way to calculate conditional probabilities. It can be formulated as follows:\n\\(P(A|B) = \\frac{{P(B|A) P(A)}}{{P(B)}}\\)\nWhere:\n\n\\(P(A|B)\\) is the probability of event A occurring given that event B has occurred.\n\\(P(B|A)\\) is the probability of event B occurring given that event A has occurred.\n\\(P(A)\\) is the probability of event A occurring.\n\\(P(B)\\) is the probability of event B occurring."
  },
  {
    "objectID": "code/multin.html#multinomial-naive-bayes",
    "href": "code/multin.html#multinomial-naive-bayes",
    "title": "Multinomial Naive Bayes",
    "section": "Multinomial Naive Bayes",
    "text": "Multinomial Naive Bayes\nMultinomial Naive Bayes is specifically designed for problems with discrete features. It assumes that the features are generated from a multinomial distribution and that the features are conditionally independent given the class label. This assumption simplifies the conditional probability calculation and makes the algorithm computationally efficient.\nThe formula for the multinomial distribution:\n\\(p(x_i | y) = \\frac{n_{yi} + \\alpha}{n_y + \\alpha n}\\)\nWhere\n\n\\(n_{yi}\\) is the number of times feature i appears in class y\n\\(n_y\\) is the number of time class y appears\n\\(\\alpha&gt;0\\) is the smoothing prior, which accounts for features not present in the learning samples and prevents zero probabilities in further computations.\n\nSetting \\(\\alpha=1\\) is called Laplace smoothing, while \\(\\alpha&lt;1\\) is called Lidstone smoothing.\n\n\nFor a comrehensive analysis of Naive Bayes algorithms, visit this link\nHere’s a step-by-step overview of how Multinomial Naive Bayes works:\n\nPreparing the Dataset: First, we need a dataset consisting of samples with features and corresponding class labels. The features should be discrete, such as word counts in text documents.\nFeature Extraction: Next, we need to extract features from the dataset. This can involve techniques like tokenization, stemming, and vectorization.\nTraining: We then split the dataset into a training set and a test set. The training set is used to calculate the probabilities required for classification.\nCalculating Class Prior Probabilities: We calculate the prior probability of each class by counting the frequency of each class label in the training set.\nCalculating Conditional Probabilities: We calculate the conditional probability of each feature given the class label by counting the frequency of each feature in each class.\nClassifying New Instances: Finally, we use the calculated probabilities to classify new instances. For each new instance, we calculate the posterior probability of each class given the features and select the class with the highest probability as the predicted class.\n\nLet’s now implement Multinomial Naive Bayes in Python using the scikit-learn library."
  },
  {
    "objectID": "code/multin.html#implementation",
    "href": "code/multin.html#implementation",
    "title": "Multinomial Naive Bayes",
    "section": "Implementation",
    "text": "Implementation\nFirst, we need to import the necessary libraries:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nNext, let’s create an example dataset consisting of text documents and corresponding class labels:\n\ndocuments = ['The sun is shining',\n             'The weather is beautiful',\n             'I enjoy going for walks',\n             'I hate rainy days']\n\nlabels = ['positive', 'positive', 'negative', 'negative']\n\nNow, let’s create a CountVectorizer object to extract features from the text documents:\n\nvectorizer = CountVectorizer()\n\nWe can then use the fit_transform() method of the vectorizer to transform the documents into a feature matrix:\n\nX = vectorizer.fit_transform(documents)\n\nNext, we need to split the dataset into a training set and a test set:\n\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n\nNow, let’s create a MultinomialNB object and train it on the training set:\n\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nWe can then use the trained model to classify the instances in the test set:\n\ny_pred = model.predict(X_test)\n\nFinally, let’s calculate the accuracy of the model:\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n\nAccuracy: 1.0\n\n\nWe have successfully trained a Multinomial Naive Bayes classifier and used it to classify new instances."
  },
  {
    "objectID": "code/precis.html",
    "href": "code/precis.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "The precision-recall curve is a graphical representation of the trade-off between precision and recall for different threshold values. It is commonly used in binary classification problems where the goal is to classify data into one of two classes.\nLet’s start by understanding precision and recall:\nPrecision is defined as the number of true positives (TP) divided by the sum of true positives and false positives (FP): [ Precision = ]\nRecall is defined as the number of true positives (TP) divided by the sum of true positives and false negatives (FN): [ Recall = ]\nIn a classification problem, a high precision means that the classifier is making fewer false positive predictions, while a high recall means that it is making fewer false negative predictions.\nTo create a precision-recall curve, we need a classifier that can provide prediction probabilities or scores for each instance. Then, by varying the threshold on these scores, we can generate different points on the precision-recall curve.\nWe will demonstrate this process using the scikit-learn library and the Breast Cancer Wisconsin (Diagnostic) dataset. Let’s get started:\nStep 1: Import the necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\n\nStep 2: Load and prepare the dataset\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 3: Train a classifier and obtain prediction probabilities\n\n# Train a logistic regression classifier\nclassifier = LogisticRegression(max_iter=10_000)\nclassifier.fit(X_train, y_train)\n\n# Obtain prediction probabilities for the test set\ny_prob = classifier.predict_proba(X_test)[:, 1]\n\nStep 4: Calculate precision and recall for different threshold values\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n\nStep 5: Plot the precision-recall curve\n\nplt.plot(thresholds, precision[:-1], label='Precision')\nplt.plot(thresholds, recall[:-1], label='Recall')\nplt.xlabel('Threshold')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.show()\n\n\n\n\nIn this code, we first import the necessary libraries such as numpy, matplotlib, scikit-learn modules, and data from scikit-learn’s built-in Breast Cancer Wisconsin dataset.\nNext, we load and prepare the dataset. We split it into training and testing sets using the train_test_split function.\nThen, we train a logistic regression classifier on the training set and obtain prediction probabilities for the test set using the predict_proba method.\nFinally, we calculate precision and recall values for different threshold values using the precision_recall_curve function. We plot these values to visualize the precision-recall curve using the plt.plot function.\nThe resulting precision-recall curve shows how the precision and recall values change for different threshold values. A higher precision and recall value indicates a better classifier performance.\nThis curve can be useful in identifying an appropriate threshold value that balances precision and recall according to the specific problem requirements."
  },
  {
    "objectID": "code/confus.html",
    "href": "code/confus.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "A confusion matrix is a useful tool for evaluating the performance of a classification model. It provides a tabular representation of the predicted and actual classes of a binary classification problem. The matrix helps us understand how well the model is performing by showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\nLet’s define the confusion matrix for a binary classification problem using the following notations:\n\nTP: Number of true positive predictions\nTN: Number of true negative predictions\nFP: Number of false positive predictions\nFN: Number of false negative predictions\n\nTo illustrate this, let’s consider a dataset of 100 samples. Our binary classifier predicts whether a sample is positive or negative. After running the prediction, we obtain the following results:\n\nThere are 60 true positive predictions (TP = 60).\nThere are 30 true negative predictions (TN = 30).\nThere are 5 false positive predictions (FP = 5).\nThere are 5 false negative predictions (FN = 5).\n\nNow, let’s plot these values in a confusion matrix:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP = 60\nFN = 5\n\n\nActual Negative\nFP = 5\nTN = 30\n\n\n\nIn Python, we can use the scikit-learn library to calculate the confusion matrix. Here’s an example of how to compute the confusion matrix for binary classification:\nStep 1: Import the necessary libraries\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nStep 2: Define the actual and predicted classes\n\nactual = np.array([1, 1, 0, 0, 1, 0, 0, 1, 0, 1])\npredicted = np.array([1, 0, 0, 1, 1, 0, 1, 1, 0, 0])\n\nStep 3: Calculate the confusion matrix using the confusion_matrix function\n\ncm = confusion_matrix(actual, predicted)\nprint(cm)\n\n[[3 2]\n [2 3]]\n\n\nOutput:\n[[3 2]\n [2 3]]\nIn this example, we have 3 true positive predictions, 3 true negative predictions, 2 false positive predictions, and 2 false negative predictions. Hence, the confusion matrix is:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP = 3\nFN = 2\n\n\nActual Negative\nFP = 2\nTN = 3\n\n\n\nThe confusion matrix provides essential information for evaluating the performance of a binary classification model, such as accuracy, precision, recall, and F1 score. It helps us understand the model’s strengths and weaknesses, identify any imbalances in the predictions, and make informed decisions about improving the model."
  },
  {
    "objectID": "code/prf1.html",
    "href": "code/prf1.html",
    "title": "Davood Wadi",
    "section": "",
    "text": "In the field of machine learning and classification, evaluating the model’s performance is crucial. One common way to evaluate classification models is by using a confusion matrix. The confusion matrix provides a detailed breakdown of the model’s predictions and their corresponding actual labels. From the confusion matrix, we can calculate several performance metrics, including precision, recall, and F1-score.\nLet’s start by understanding what a confusion matrix is.\nA confusion matrix is a table that visualizes the performance of a classification model. It consists of four different values:\n\nTrue Positive (TP): The number of positive instances that the model correctly predicted as positive.\nFalse Positive (FP): The number of negative instances that the model incorrectly predicted as positive.\nTrue Negative (TN): The number of negative instances that the model correctly predicted as negative.\nFalse Negative (FN): The number of positive instances that the model incorrectly predicted as negative.\n\nThe confusion matrix is typically presented in the following format:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\nNow, let’s calculate precision, recall, and F1-score using the confusion matrix.\nPrecision measures the accuracy of positive predictions. It is calculated using the formula:\n\\(\\text{{Precision}} = \\frac{TP}{{TP + FP}}\\)\nRecall, also known as the sensitivity or true positive rate, measures the proportion of actual positive instances that were correctly classified. It is calculated using the formula:\n\\(\\text{{Recall}} = \\frac{TP}{{TP + FN}}\\)\nF1-score is the harmonic mean of precision and recall. It provides a balanced measure between the two. F1-score is calculated using the formula:\n\\(F1 = \\frac{2 \\times \\text{{Precision}} \\times \\text{{Recall}}}{{\\text{{Precision}} + \\text{{Recall}}}}\\)"
  },
  {
    "objectID": "Introduction_to_ML.html",
    "href": "Introduction_to_ML.html",
    "title": "MATH60629A Fall 2022",
    "section": "",
    "text": "This tutorial provides a short introduction to the practice of machine learning.\nWe assume that the user already has an understanding of the basic concepts that underlie the field. We review both methodological concepts like supervised learning and also use software libraries such as scikit-learn, pandas, and numpy.\nIn particular, we will: 1. load some data, 2. fit different supervised models on variations of the data, 3. and compare results.\nThis tutorial is not meant to be exhaustive (references are provided throughout, and links to extra material are provided at the end).\n\n\n\nLaurent Charlin lcharlin@gmail.com\n\n\n\n\n\nSection 0. Introduction\nSection 1. Data Pre-Processing\nSection 2. Modelling\nSection 3. Concluding Remarks\n\n ### Section 0. Introduction We will use the example of a recommender system, i.e., a system which must recommend movies of interests to its users (e.g., Netflix). We will model user-movie preferences from a popular publicly available dataset (Movielens 1M). We will learn, from past user-movie ratings, to predict (missing/future) user-movie ratings from user socio-demographics and movie-tags data.\nMathematically, we are interested in learning the (parameters of the) following function:\n\\[ r_{um} = f_\\theta(x_u, x_m)\\] where - \\(u\\) indexes users - \\(m\\) indexes items - \\(r_{um}\\) is u’s rating for m (that user’s preference) – the dependent variable - \\(f_\\theta\\) is some model parametrized by \\(\\theta\\). For example, a linear regression with coefficients \\(\\theta\\) - \\(x_u\\) are user u’s covariates (e.g., age and occupation of this user) - \\(x_m\\) are movie m’s covariates (e.g., tags associated with the movie)\nThe function \\(f\\) can take several forms (in other words, we can use a variety of models for this task). In today’s tutorial we will assume that the problem is a regression one and we will experiment with several models ranging from a simple linear regression model to a more complicated two-hidden layer neural network.\n\n\n\nIt can be useful to think of machine learning as comprising three elements: 1. Task (T) 2. Experience (E) 3. Performance measure (P).\n(a good description of these concepts is provided in Ch. 5 of the Deep Learning Book)\nThe intuition is that the task (T) is “the type of problem you are trying the solve” (e.g., classification, regression, anomaly detection), the experience (E) is “how your data comes about” (e.g., does it come with labels or not, do you observe it all at once or as a stream), and the performance (P) is “how well your model does”. Standard performance measures include accuracy and mean-squared error.\nNote that the above terminology does not define the model used to learn (fit) the data nor does it define the fitting procedure (e.g., gradient descent).\nRelationship to the problem of rating prediction: - Task: Our task is to predict user-movie ratings. It can be modelled in different ways (more on this during week 11), but here we will model it as a regression problem. - Experience: The experience is a supervised learning one because we are predicting some dependent variable (rating) from a set of independent variables - Performance measure: We will be using the mean-squared error (MSE).\n\n\n\nFor supervised learning, it is customary, to construct two data matrix \\(X\\) and \\(Y\\). The former, \\(X\\), contains the covariates (features). It is a matrix of size \\(n \\times p\\) with \\(n\\) the number of examples and \\(p\\) the dimensionality of each example (in other words the number of covariates associated with each example). They are the input to the function.\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\n\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\\nx_{n1} & x_{12} & \\ldots & x_{np} \\\\\n\\end{bmatrix}\n\\]\nThe latter, \\(Y\\), is a (column) vector of length \\(n\\) which contains the labels (here ratings). \\(Y_1\\) corresponds to the rating of \\(X_1\\) (row contains the labels (here ratings).\n\\[\nY = \\begin{bmatrix}\nr_1 \\\\\nr_2 \\\\\n\\vdots \\\\\nr_n\n\\end{bmatrix}\\]\nOf course, in a real problem we will differentiate the train and test sets, e.g., with \\(X_\\text{train}\\) and \\(X_\\text{test}\\). Same for the labels using, e.g., \\(Y_\\text{train}\\) and \\(Y_\\text{test}\\).\n\n\n\nFollowing this brief introduction, we now dive into the problem.\n\n# We first download the repo to get access to data and some utility code (This is specifically for colab.)\n!rm -rf 80-629/\n!git clone https://github.com/lcharlin/80-629/\n\nCloning into '80-629'...\nremote: Enumerating objects: 25, done.\nremote: Counting objects: 100% (25/25), done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 77 (delta 16), reused 19 (delta 10), pack-reused 52\nUnpacking objects: 100% (77/77), done.\n\n\nWe begin by importing the packages that we will need: - reduce function will come in handy to iteratively process data - os standard packages for performing system operations (e.g., opening files) - re package for regex - sys package to deal with system-level operations (here used to change the search path) - time package we will use to measure the duration of certain operations\n\nmatplotlib for plotting\nnumpy for linear-algebra computations\npandas for data wrangling\nsklearn (scikit-learn) for machine learning models and useful machine learning related routines\n\n\nfrom functools import reduce\nimport os\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neural_network\n\nimport sys\nsys.path += ['80-629/week4-PracticalSession/']\nfrom local_utils import DrawNN\n\n # Section 1: Data Pre-Processing\nIn the following we load data from several csv files and pre-process it.\nWhile this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\nI suggest that you read this section, but that you spend most of your time on the other sections. If you have time at the end, you can come back and do this more thoroughly.\n\n\nWe will use the publically available movielens dataset. The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the ML-1M data (it contains 1M ratings) but we will also use movie tags from the ML-20M dataset (20M ratings).\nExcept for downloading the dataset (to save you some time), I have not processed nor modified the data in any way.\n\nROOT_DIR='80-629/week4-PracticalSession'\nDATA_DIR=os.path.join(ROOT_DIR, 'dat/ml-1m/') # this is where most of our data lives\nDATA_DIR_20ML=os.path.join(ROOT_DIR, 'dat/ml-20m/') # for the tags data\n\n\n\n\nWe begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format MovieID::Name::Genres.\nAfter loading into a pandas dataFrame structure, we will have movie names (mName), IDs (mid), and movie genres (mGenres)\n\nmovies_pd = pd.read_csv(os.path.join(DATA_DIR, 'movies.dat'),\n                        sep='::',\n                        names=['mid', 'mName', 'mGenres'], engine='python',\n                        encoding='latin-1')\n\n\nprint(f'The dataset contains {movies_pd.shape[0]} movies')\n\nThe dataset contains 3883 movies\n\n\n\ndisplay(movies_pd.head())\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n0\n1\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children's|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\nUsing pandas we can also search for movies by mid or by their name:\n\nmid = 10\ndisplay(movies_pd[movies_pd.mid==mid])\n\nname = 'Machine'\ndisplay(movies_pd[movies_pd.mName.str.contains(name,\n                                               regex=False, case=False)])\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n9\n10\nGoldenEye (1995)\nAction|Adventure|Thriller\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n1409\n1433\nMachine, The (1994)\nComedy|Horror\n\n\n\n\n\n\n\n\n\n\nUsing a similar routine as above, we load the ratings data which is in this format UserID::MovieID::Rating::Timestamp, and we will name the column UserID with uid, the column MovieID with mid, the rating with rating, and the time of the rating with timestamp.\n\nratings_pd = pd.read_csv(os.path.join(DATA_DIR, 'ratings.dat'),\n                         sep='::',\n                         names=['uid', 'mid', 'rating', 'timestamp'],\n                         parse_dates=['timestamp'],\n                         infer_datetime_format=True,\n                         engine='python')\n\ndisplay(ratings_pd.head())\n\n\n\n\n\n\n\n\nuid\nmid\nrating\ntimestamp\n\n\n\n\n0\n1\n1193\n5\n978300760\n\n\n1\n1\n661\n3\n978302109\n\n\n2\n1\n914\n3\n978301968\n\n\n3\n1\n3408\n4\n978300275\n\n\n4\n1\n2355\n5\n978824291\n\n\n\n\n\n\n\n\nprint(f\"\"\"The dataset contains {ratings_pd.shape[0]} ratings,\n      from {ratings_pd.uid.nunique()} users,\n      and {ratings_pd.mid.nunique()} items.\"\"\")\n\nThe dataset contains 1000209 ratings, \n      from 6040 users, \n      and 3706 items.\n\n\n\n\n\nThe file is in this format UserID::Gender::Age::Occupation::Zip-code, which we will load in a dataFrame with the following column names uid,gender,age,occupation,zip.\n\nusers_pd = pd.read_csv(os.path.join(DATA_DIR, 'users.dat'),\n                       sep='::',\n                       names=['uid', 'gender', 'age', 'occupation', 'zip'],\n                       engine=\"python\")\n\ndisplay(users_pd.head())\n\n\n\n\n\n\n\n\nuid\ngender\nage\noccupation\nzip\n\n\n\n\n0\n1\nF\n1\n10\n48067\n\n\n1\n2\nM\n56\n16\n70072\n\n\n2\n3\nM\n25\n15\n55117\n\n\n3\n4\nM\n45\n7\n02460\n\n\n4\n5\nM\n25\n20\n55455\n\n\n\n\n\n\n\n\nprint(f'This table contains {users_pd.shape[0]} users')\n\nThis table contains 6040 users\n\n\nFurther we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and &gt;3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature.\n\nprint(f'We originally have {users_pd.zip.nunique()} different zip codes')\nusers_pd['zip'] = users_pd['zip'].apply(lambda x: x[:2])\ndisplay(users_pd['zip'].head())\nprint(f'By only keep the first two digits of each zip code, \\\nwe reduced the unique number of zip codes to {users_pd.zip.nunique()}.')\n\nWe originally have 3439 different zip codes\nBy only keep the first two digits of each zip code, we reduced the unique number of zip codes to 100.\n\n\n0    48\n1    70\n2    55\n3    02\n4    55\nName: zip, dtype: object\n\n\n\n\n\nThe remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\nWe will load the csv data movieId,tagId,relevance into a dataFrame with the columns mid,tid,relevance.\n\n# load ml-20m tags\ntags_scores = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-scores.csv.gz'),\n                          skiprows=1,\n                          names=['mid', 'tid', 'relevance'])\ndisplay(tags_scores.head(10))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\n\n\n\n\n0\n1\n1\n0.02500\n\n\n1\n1\n2\n0.02500\n\n\n2\n1\n3\n0.05775\n\n\n3\n1\n4\n0.09675\n\n\n4\n1\n5\n0.14675\n\n\n5\n1\n6\n0.21700\n\n\n6\n1\n7\n0.06700\n\n\n7\n1\n8\n0.26275\n\n\n8\n1\n9\n0.26200\n\n\n9\n1\n10\n0.03200\n\n\n\n\n\n\n\n\nprint(f'The data contains {tags_scores.tid.nunique()} unique tags.')\nprint(f'Affinities (relevances) are contained in the {tags_scores.relevance.min()}--{tags_scores.relevance.max()} range.')\ndisplay(tags_scores.relevance.describe())\n\nThe data contains 1128 unique tags.\nAffinities (relevances) are contained in the 0.00024999999999997247--1.0 range.\n\n\ncount    1.170977e+07\nmean     1.164833e-01\nstd      1.542463e-01\nmin      2.500000e-04\n25%      2.425000e-02\n50%      5.650000e-02\n75%      1.415000e-01\nmax      1.000000e+00\nName: relevance, dtype: float64\n\n\nFrom above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12.\nWe also load the tag names. This will be useful for exploration purposes.\n\ntags_names = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-tags.csv'), skiprows=1, names=['tid', 'tName'])\ndisplay(tags_names.head(10))\n\n\n\n\n\n\n\n\ntid\ntName\n\n\n\n\n0\n1\n007\n\n\n1\n2\n007 (series)\n\n\n2\n3\n18th century\n\n\n3\n4\n1920s\n\n\n4\n5\n1930s\n\n\n5\n6\n1950s\n\n\n6\n7\n1960s\n\n\n7\n8\n1970s\n\n\n8\n9\n1980s\n\n\n9\n10\n19th century\n\n\n\n\n\n\n\nSince we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (mid) are the same across these two datasets (i.e., not need for messy string match).\n(Note: Pandas’ functionalities allow you to do operations similar to what you would do in SQL for relational databases.)\n\ntags_scores = tags_scores.loc[tags_scores['mid'].isin(ratings_pd['mid'].unique())]\nprint(tags_scores.mid.nunique())\n\n3470\n\n\nWe lost a few movies compared to the original count of 3706 but we can live with that.\nNext, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie.\n\n# Keep only high-relevance tags (here this is defined as having a relevance above 0.9)\nprint('unique tags:', tags_scores['tid'].nunique())\ntags_scores_high = tags_scores.loc[tags_scores['relevance'] &gt; 0.9]\nprint('unique tags w. high relevance:', tags_scores_high['tid'].nunique())\n\nunique tags: 1128\nunique tags w. high relevance: 968\n\n\n\n\n\nLet’s get some understanding of how these tags are used. To help, we first build a dataFrame that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names).\nPandas’ merge function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)\n\ntags_high_names_movies = pd.merge(tags_scores_high, tags_names, how='inner', on='tid')\ntags_high_names_movies = pd.merge(tags_high_names_movies, movies_pd, how='inner', on='mid')\ndisplay(tags_high_names_movies.head())\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n0\n1\n63\n0.93325\nanimated\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n1\n64\n0.98575\nanimation\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n2\n1\n186\n0.95650\ncartoon\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n3\n1\n203\n0.92625\nchildhood\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n4\n1\n204\n0.96425\nchildren\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n\n\n\n\n\nSimilarly as above we can search for top movies according to a particular tag:\n\ntag = 'scary' # This is the (sub) tag we search for\ndisplay(tags_high_names_movies[\n    tags_high_names_movies.tName.str.contains(tag,\n                                               regex=False, case=False)].sort_values(by=['relevance'],\n                                                                                    ascending=False))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n13151\n2710\n882\n0.96700\nscary\nBlair Witch Project, The (1999)\nHorror\n\n\n14005\n1342\n882\n0.96625\nscary\nCandyman (1992)\nHorror\n\n\n9205\n1347\n882\n0.96550\nscary\nNightmare on Elm Street, A (1984)\nHorror\n\n\n14658\n3892\n882\n0.96475\nscary\nAnatomy (Anatomie) (2000)\nHorror\n\n\n3187\n1997\n882\n0.96200\nscary\nExorcist, The (1973)\nHorror\n\n\n14021\n2550\n882\n0.95625\nscary\nHaunting, The (1963)\nHorror|Thriller\n\n\n11492\n1350\n882\n0.94675\nscary\nOmen, The (1976)\nHorror\n\n\n11546\n2841\n882\n0.94650\nscary\nStir of Echoes (1999)\nThriller\n\n\n9340\n1974\n882\n0.94625\nscary\nFriday the 13th (1980)\nHorror\n\n\n2976\n1387\n882\n0.94250\nscary\nJaws (1975)\nAction|Horror\n\n\n5121\n2460\n882\n0.93925\nscary\nTexas Chainsaw Massacre 2, The (1986)\nHorror\n\n\n2488\n1214\n882\n0.93875\nscary\nAlien (1979)\nAction|Horror|Sci-Fi|Thriller\n\n\n9478\n3273\n882\n0.93825\nscary\nScream 3 (2000)\nHorror|Mystery|Thriller\n\n\n13208\n1590\n882\n0.93525\nscary\nEvent Horizon (1997)\nAction|Mystery|Sci-Fi|Thriller\n\n\n13916\n1321\n882\n0.93475\nscary\nAmerican Werewolf in London, An (1981)\nHorror\n\n\n11518\n2160\n882\n0.93325\nscary\nRosemary's Baby (1968)\nHorror|Thriller\n\n\n4825\n2719\n882\n0.92450\nscary\nHaunting, The (1999)\nHorror|Thriller\n\n\n7636\n3499\n882\n0.92350\nscary\nMisery (1990)\nHorror\n\n\n3452\n2762\n882\n0.92075\nscary\nSixth Sense, The (1999)\nThriller\n\n\n2781\n1258\n882\n0.91675\nscary\nShining, The (1980)\nHorror\n\n\n12355\n3081\n882\n0.91425\nscary\nSleepy Hollow (1999)\nHorror|Romance\n\n\n2343\n1200\n882\n0.90475\nscary\nAliens (1986)\nAction|Sci-Fi|Thriller|War\n\n\n\n\n\n\n\nThis next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data.\nIn the current dataset, every movie-tag id pair is a separate entry (row of the dataFrame). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example.\nTo do so, we re-encode tids using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., “cats” and “dogs”) which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag scary is id 882) but tags cannot be compared using their numbers (e.g., tag 882 is not “bigger” than tag 880 or smaller than tag 900). 1-of-K encoding deals with this by encoding each tag as a binary vector of length \\(K\\) with a single non-zero value which corresponds to the tag. In the present case, \\(K=968\\) tags, and tag scary would have a 1 at position 882.\nBelow we see that our data now has 971 columns: 968 for tag ids, 1 for mid, and 1 for relevance, and 1 for the pandas index.\n\n#print(tags_scores_high.shape)\ntags_scores_high_dum = pd.get_dummies(tags_scores_high, columns=['tid'])\ntags_scores_high_dum = tags_scores_high_dum.reset_index()\n#print(tags_scores_high_dum.shape)\ndisplay(tags_scores_high_dum.head())\ntags_per_movie = tags_scores_high_dum.groupby(\"mid\").sum()\n\n\n\n\n\n\n\n\nindex\nmid\nrelevance\ntid_1\ntid_2\ntid_3\ntid_5\ntid_6\ntid_7\ntid_9\n...\ntid_1119\ntid_1120\ntid_1121\ntid_1122\ntid_1123\ntid_1124\ntid_1125\ntid_1126\ntid_1127\ntid_1128\n\n\n\n\n0\n62\n1\n0.93325\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n63\n1\n0.98575\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n185\n1\n0.95650\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n202\n1\n0.92625\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n203\n1\n0.96425\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 971 columns\n\n\n\nWith this data we can then explore the distribution of movies per tag (and tags per movie below).\n\nth = tags_scores_high.groupby(\"tid\").count()\nhists = th.hist(bins=50, column=\"mid\", xlabelsize=15, ylabelsize=15)[0][0]\nhists.set_ylabel(\"Tags\", size=15)\nhists.set_xlabel(\"Movies per tag\", size=15)\nhists.set_title(\"Histogram of Movies per tag\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (&lt;5). On the other hand, the most popular tag was used to tag 210 movies.\nWe note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies.\n\ntname = tags_names.at[tags_names.tid.eq(th.mid.idxmax()).idxmax(), 'tName']\nprint(f'The most popular tag \"{tname}\" has been used for {th.mid.max()} movies')\n\nThe most popular tag \"comedy\" has been used for 210 movies\n\n\nUsing the same recipe, we can do something similar for movies instead of tags\n\nhists = tags_scores_high.groupby(\"mid\").count().hist(bins=40, column=\"tid\", xlabelsize=15, ylabelsize=15)\nhists[0][0].set_ylabel(\"Movies\", size=15)\nhists[0][0].set_xlabel(\"Tags per movie\", size=15)\nhists[0][0].set_title(\"Histogram of Tags per movie\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags.\n\n\n\nWhat is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags?\n\nmh = ...\nmname = ...\nprint(f'The most popular movie \"{mname}\" has {mh.tid.max()} tags')\n\nAttributeError: 'ellipsis' object has no attribute 'tid'\n\n\nIn the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features.\n\n# Join users, ratings, and tags\ndata_pd = pd.merge(users_pd, ratings_pd, how='inner', on='uid')\ndata_pd = pd.merge(data_pd, tags_per_movie, how='inner', on='mid')\n\nFor the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes.\n\n# shuffle data and keep 2% of the ratings.\n# (this small percentage ensures that all computations in this tutorial are fast)\ndata_pd = data_pd.sample(frac=0.02, random_state=1234)\nprint(data_pd.shape)\n\nWe can have a look at our current dataset.\n\nprint('Final descriptive stats of our dataset.')\nprint('\\t- %d items'   % data_pd['mid'].nunique())\nprint('\\t- %d users'   % data_pd['uid'].nunique())\nprint('\\t- %d ratings' % data_pd.shape[0])\n\nNotice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags).\n\nprint(data_pd.shape)\ndisplay(data_pd[:10])\n\n\ncols = ['gender','occupation','zip','mid','uid']\ndata_pd_dum = pd.get_dummies(data_pd, columns=cols)\nprint(data_pd_dum.shape)\ndisplay(data_pd_dum.head(10))\n\nWhile we used pandas to create dummies, scikit-learn has similar capacities. The preprocessing module is detailed here. You can also checkout the section on Categorical features.\nWe are ready to construct our first dataset. We will first use a subset of the columns (not including tags).\nBelow you will also note that we split our data into train and test using train_test_split from scikit-learn.\n\nattributes = \"mid_*|uid_*|gender_*|age|zip_*|occupation_*\"\nX = data_pd_dum.filter(regex=('('+attributes+')'))\nprint(X.shape)\n\nrating = data_pd_dum['rating']\nprint(rating.shape)\n\n# Split Train/Test\n# Keep 20% of the data for testing.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, rating, test_size=0.2, random_state=1234, shuffle=False)\n\nRecommender Systems note: We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items).\nRecommender Systems note #2: Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings).\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n # Section 2: Modelling\nRatings (likert scale) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n\\[ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2\\]\nThe MSE can be understood as the average square distance between the predictor \\(f(x_i)\\) and the target \\(y_i\\). MSE returns a non-negative quantity and the perfect predictor has an MSE of \\(0\\). If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\nTrain/Test: Recall that while we estimate the parameters of the model using the train data, we evaluate the quality of the model (its performance) using test data.\n\n\n\nIt is often helpful to use a very simple benchmark to compare against the performance of our models.\nOur initial benchmark is a model which simply predicts the mean (train) rating.\nRecommender Systems note: We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean.\n\n# Check accuracy of constant predictor\n\nprint(\"Constant predictor\")\n\nprint(\"\\tTrain mean-squared error: %.3f\"\n      % mean_squared_error(y_train,\n                           np.full_like(y_train, y_train.mean())))\nprint(\"\\tTest mean-squared error: %.3f\"\n      % mean_squared_error(y_test,\n                           np.full_like(y_test, y_train.mean())))\n\nThe train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance).\nIn terms of absolute values these indicate that, on average, our predictions are 1.3 units (\\(\\sqrt{1.6}\\)) away from the true rating. This indicates that you shouldn’t be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods.\n\n\n\nFor our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users’ gender, age, zip, and occupation. We fit this model \\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\\(\\theta_{1:6}\\) are the parameters, \\(\\text{gender}_u\\) stands for the gender of user \\(u\\) and similarly for other covariates. Also, \\(x_{\\text{uid}_u}\\) represents the identity of the user and similarly for \\(x_{\\text{mid}_i}\\) and movies.\nNote that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so \\(\\theta_{\\text{zip}}\\) has 100 dimensions.\nTraining this model involves minimizing the train MSE, this is exactly what the LinearRegression class does. (This is a least-squares problem and it can be solved in closed form.)\n\n# Create linear regression object\nreg = linear_model.LinearRegression()\n\n# Train the model using the training sets\nreg.fit(X_train, y_train)\n\nprint(\"Number of parameters: \", reg.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = reg.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = reg.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nWe note that train error \\(&lt;&lt;\\) test error (\\(&lt;&lt;\\) stands for “much smaller”). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it’s a low bias and high variance model).\nDifferent methods can help prevent the model from overfitting, this is often referred to as regularizing the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: \\[  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 \\]\nInstead of the previous $ := $.\nNote: - \\(||\\cdot||_2\\) stands for the 2-Norm. That is, the square root of the sum of the operand’s squared elements. - \\(\\alpha\\) is a hyper-parameter which denotes the strength of the regularizer (if \\(\\alpha=0\\) the regularizer vanishes and if \\(\\alpha=\\infty\\) all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning \\(\\alpha\\) along with the \\(\\theta\\)s would lead to a \\(\\alpha=0\\)).\nDuring learning the model must then tradeoff performance (MSE) and complexity (high \\(\\theta\\)s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the Ridge class from the linear_model package to fit regularized linear regression models.\n\n# Create linear regression object\nregr = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr.fit(X_train, y_train)\nfit_time = time.time() - start\n\nprint(\"Fitting time: %.2f seconds\" % fit_time)\n\nprint(\"Number of parameters:\", regr.coef_.shape[0]+1)\n\nQuestion 3 Explain why there are 7,184 parameters.\nHint: Don’t forget the intercept/bias term\n\n# Make train predictions\ny_train_pred = regr.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nCompared to above, we see that with \\(\\alpha=10\\) the train and test errors are much closer (i.e., there’s less overfitting). Presumably different values of \\(\\alpha\\) would yield different generalizations.\nQuestion 4 How do you find the “best” value of \\(\\alpha\\) for a given model and dataset?\nHint: Have a look at the RidgeCV, a cross-validation enabled version of Ridge.\nAnswer: See below.\n\n# Create linear regression object\nregRCV = ...\n\n# Train the model using the training sets\n\n\nprint(\"Number of parameters: %d, estimated alpha: %d\" % (regRCV.coef_.shape[0], regRCV.alpha_))\n\nTechnical remark: since the optimization is often done in log space, it’s typical for the set of \\(\\alpha\\)’s to be powers of 10.\n\n# Make train predictions\ny_train_pred = ...\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = ...\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nThe advantage of doing cross validation (for example using RidgeCV) is clear. It automatically searches for the best values of hyperparameters (here \\(\\alpha\\)) from a given set (here \\(\\{ 1, 10, 100 \\}\\)).\nA validation set (or cross validation) should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement validation. In such cases you will likely need to split a separate validation set from your training data–in sklearn you can use the train_test_split function. It is typical for the validation set to be the same size as the test set.\nYou should also remember to never select model hyperparameters based on performance on test set, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models.\n\nArmed with a good model we can explore the learned model including some of its predictions\n\n# helper function to return non-zero columns\ndef non_zero(row, columns):\n    col_name = list(columns[~(row == 0)])[0]\n    #r = re.sub('mid_','',l)\n    return col_name\n\n\n# get number of ratings per movie (popularity)\nmids = X_test.filter(regex=('mid_*'))\ny_mid_cols = mids.apply(lambda x: non_zero(x, mids.columns), axis=1)\nmovie_popularity = X_train.filter(regex=('mid_*')).sum(axis=0)[ y_mid_cols ]\n\n# get number of ratings per user (activity)\nuids = X_test.filter(regex=('uid_*'))\ny_uid_cols = uids.apply(lambda x: non_zero(x, uids.columns), axis=1)\nuser_activity = X_train.filter(regex=('uid_*')).sum(axis=0)[ y_uid_cols ]\n\nerr = (y_test_pred-y_test)\n\n\n# only plot a subsample for higher readability\nsubn = 500\nfig, (ax0, ax1) = plt.subplots(ncols=2)\nfig.set_figwidth(15)\nax0.scatter(movie_popularity[:subn], err[:subn])\nax0.set_ylabel('Prediction error')\nax0.set_xlabel('Movie Popularity')\n\nax1.scatter(user_activity[:subn], err[:subn])\nax1.set_ylabel('Prediction error')\nax1.set_xlabel('User Activity');\n\nAbove we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n\nThis empirical distribution looks symmetrical so there doesn’t seem to be a bias toward lower or higher predictions\nThe prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a “triangle” pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters (\\(\\theta_{\\text{mid}}\\)). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process.\n\n\n\n\nWe use a linear regression model as above but also model movie tags:\n\\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n\\]\nThe last term on the right-hand side (bolded) is the only difference wrt to our previous model.\nQuestion 5: How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance?\nHint: One model is a special case of another.\n\n# This is very similar to how we constructed our dataset above except that we add the tags columns\nX_tags = data_pd_dum.filter(regex=('('+attributes+\"|tid_*\"+')'))\nprint(X_tags.shape)\n\n# Split Train/Test. Notice that we use the same seed as above to replicate that split.\nX_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(\n    X_tags, rating, test_size=0.2, random_state=1234, shuffle=False)\nprint(X_train_tags.shape)\n\n\n# Create linear regression object\nregr_tags = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr_tags.fit(X_train_tags, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", regr_tags.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = regr_tags.predict(X_train_tags)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_tags.predict(X_test_tags)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n\nRemarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n\n\n\n\nSo far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant.\nHere we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\nNeural network basics: (We will discuss these models in some depth over the next two weeks.) - A neural network is made up of interconnected neurons. Each neuron computes a few simple operations. - In a feed-forward network, neurons are organized into sets called layers. - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer. - The first layer is called the input layer it provides data to the next layer. The last layer is the output layer it provides a prediction \\(\\hat{y}\\). - Layers in between the input layer and the output layer are called hidden layers. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (activation function): \\(f(x) = \\sum_i x_i \\theta_i\\). - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions). - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers.\nMathematically for a regression task (with a single output), a one-hidden layer neural net is: \\[\nf(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) )\n\\] where - \\(\\theta_{ij}\\) are the parameters of input \\(i\\) and neuron \\(j\\) in the hidden layer. - \\(f_h\\) is the activation function of the hidden layer - \\(\\theta'_{j}\\) are the parameters that connect the neuron \\(j\\) in the hidden layer to the output layer. - \\(f_o\\) is the activation function of the output layer\nAn intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:\n\n# a 1 hidden layer neural net, where the input has 10 dimensions (p=10) and the output 1\ninput_dims = 10 # p\nhidden_layers_size = [4] # number of hidden neurons for each hidden layer (adding a dimension adds a layer)\noutput_dims = 1 # number of outputs\n\nnetwork = DrawNN( [input_dims] + hidden_layers_size + [output_dims] )\nnetwork.draw()\n\n\n# Fit a neural network on this data.\nregr_nn = neural_network.MLPRegressor(alpha=0.1, # l2-regularization (weight decay)\n                                      hidden_layer_sizes=tuple(hidden_layers_size),\n                                      early_stopping=True, # stop if validation performance decreases\n                                      verbose=True,\n                                      random_state=1234)\nstart = time.time()\nregr_nn.fit(X_train_tags.values, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", reduce(lambda x,y: x+y,\n                                       list(map(lambda x: x.size, regr_nn.coefs_+regr_nn.intercepts_)) ))\n\nMuch like previous models we can regularize a neural net to combat overfitting: - Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by \\(\\alpha\\). - In addition, we use a second regularizer called early-stopping. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In scikit-learn, the MLPRegressor class with early_stopping=True automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters.\nQuestion 6: Why does this model come with the possibility to set the random seed (i.e., random_state) while linear regression did not?\n\n# Make train predictions\ny_train_pred = regr_nn.predict(X_train_tags.values)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_nn.predict(X_test_tags.values)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n#Train Mean squared error: 0.6623\n#Test Mean squared error: 1.0465\n\nHere is our updated table of results\n\n\n\nModel\nTest MSE\n\n\n\n\n2.2 (Linear Reg. w. basic features)\n1.031\n\n\n2.3 (2.2 + movie tags)\n0.991\n\n\n2.4 (Neural Network w. features from 2.3)\n1.029\n\n\n\nAlthough neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better.\n ### Section 3. Concluding Remarks\nThe goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it’s coming from and how to model it.\nHere are a few more parting thoughts:\n\n\nAs you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n\n\n\nscikit-learn is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\nModel Selection, i.e., which model should I use for a particular dataset/task can be daunting. This page provides some tips particular to scikit-learn. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\nNote that scikit-learn does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n\n\n\nSoftware is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. scikit-learn is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more.\n\n\n\n\nIn our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don’t have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be:\n\n\\[\nf(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\nQuestion 7: What’s wrong with the above model? Try to think about it for a minute or two before looking at the answer.\nAs we will see during week 11 (on recommender systems), many models take ratings as output and as input. For example, one could take a user’s previous ratings and try to predict one’s future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist).\n\n\n\n\n\n\nScikit-learn - Documentation - Tutorials - Help for model selection"
  },
  {
    "objectID": "Introduction_to_ML.html#start",
    "href": "Introduction_to_ML.html#start",
    "title": "MATH60629A Fall 2022",
    "section": "",
    "text": "Following this brief introduction, we now dive into the problem.\n\n# We first download the repo to get access to data and some utility code (This is specifically for colab.)\n!rm -rf 80-629/\n!git clone https://github.com/lcharlin/80-629/\n\nCloning into '80-629'...\nremote: Enumerating objects: 25, done.\nremote: Counting objects: 100% (25/25), done.\nremote: Compressing objects: 100% (15/15), done.\nremote: Total 77 (delta 16), reused 19 (delta 10), pack-reused 52\nUnpacking objects: 100% (77/77), done.\n\n\nWe begin by importing the packages that we will need: - reduce function will come in handy to iteratively process data - os standard packages for performing system operations (e.g., opening files) - re package for regex - sys package to deal with system-level operations (here used to change the search path) - time package we will use to measure the duration of certain operations\n\nmatplotlib for plotting\nnumpy for linear-algebra computations\npandas for data wrangling\nsklearn (scikit-learn) for machine learning models and useful machine learning related routines\n\n\nfrom functools import reduce\nimport os\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neural_network\n\nimport sys\nsys.path += ['80-629/week4-PracticalSession/']\nfrom local_utils import DrawNN\n\n # Section 1: Data Pre-Processing\nIn the following we load data from several csv files and pre-process it.\nWhile this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\nI suggest that you read this section, but that you spend most of your time on the other sections. If you have time at the end, you can come back and do this more thoroughly.\n\n\nWe will use the publically available movielens dataset. The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the ML-1M data (it contains 1M ratings) but we will also use movie tags from the ML-20M dataset (20M ratings).\nExcept for downloading the dataset (to save you some time), I have not processed nor modified the data in any way.\n\nROOT_DIR='80-629/week4-PracticalSession'\nDATA_DIR=os.path.join(ROOT_DIR, 'dat/ml-1m/') # this is where most of our data lives\nDATA_DIR_20ML=os.path.join(ROOT_DIR, 'dat/ml-20m/') # for the tags data\n\n\n\n\nWe begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format MovieID::Name::Genres.\nAfter loading into a pandas dataFrame structure, we will have movie names (mName), IDs (mid), and movie genres (mGenres)\n\nmovies_pd = pd.read_csv(os.path.join(DATA_DIR, 'movies.dat'),\n                        sep='::',\n                        names=['mid', 'mName', 'mGenres'], engine='python',\n                        encoding='latin-1')\n\n\nprint(f'The dataset contains {movies_pd.shape[0]} movies')\n\nThe dataset contains 3883 movies\n\n\n\ndisplay(movies_pd.head())\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n0\n1\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children's|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\nUsing pandas we can also search for movies by mid or by their name:\n\nmid = 10\ndisplay(movies_pd[movies_pd.mid==mid])\n\nname = 'Machine'\ndisplay(movies_pd[movies_pd.mName.str.contains(name,\n                                               regex=False, case=False)])\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n9\n10\nGoldenEye (1995)\nAction|Adventure|Thriller\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmid\nmName\nmGenres\n\n\n\n\n1409\n1433\nMachine, The (1994)\nComedy|Horror\n\n\n\n\n\n\n\n\n\n\nUsing a similar routine as above, we load the ratings data which is in this format UserID::MovieID::Rating::Timestamp, and we will name the column UserID with uid, the column MovieID with mid, the rating with rating, and the time of the rating with timestamp.\n\nratings_pd = pd.read_csv(os.path.join(DATA_DIR, 'ratings.dat'),\n                         sep='::',\n                         names=['uid', 'mid', 'rating', 'timestamp'],\n                         parse_dates=['timestamp'],\n                         infer_datetime_format=True,\n                         engine='python')\n\ndisplay(ratings_pd.head())\n\n\n\n\n\n\n\n\nuid\nmid\nrating\ntimestamp\n\n\n\n\n0\n1\n1193\n5\n978300760\n\n\n1\n1\n661\n3\n978302109\n\n\n2\n1\n914\n3\n978301968\n\n\n3\n1\n3408\n4\n978300275\n\n\n4\n1\n2355\n5\n978824291\n\n\n\n\n\n\n\n\nprint(f\"\"\"The dataset contains {ratings_pd.shape[0]} ratings,\n      from {ratings_pd.uid.nunique()} users,\n      and {ratings_pd.mid.nunique()} items.\"\"\")\n\nThe dataset contains 1000209 ratings, \n      from 6040 users, \n      and 3706 items.\n\n\n\n\n\nThe file is in this format UserID::Gender::Age::Occupation::Zip-code, which we will load in a dataFrame with the following column names uid,gender,age,occupation,zip.\n\nusers_pd = pd.read_csv(os.path.join(DATA_DIR, 'users.dat'),\n                       sep='::',\n                       names=['uid', 'gender', 'age', 'occupation', 'zip'],\n                       engine=\"python\")\n\ndisplay(users_pd.head())\n\n\n\n\n\n\n\n\nuid\ngender\nage\noccupation\nzip\n\n\n\n\n0\n1\nF\n1\n10\n48067\n\n\n1\n2\nM\n56\n16\n70072\n\n\n2\n3\nM\n25\n15\n55117\n\n\n3\n4\nM\n45\n7\n02460\n\n\n4\n5\nM\n25\n20\n55455\n\n\n\n\n\n\n\n\nprint(f'This table contains {users_pd.shape[0]} users')\n\nThis table contains 6040 users\n\n\nFurther we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and &gt;3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature.\n\nprint(f'We originally have {users_pd.zip.nunique()} different zip codes')\nusers_pd['zip'] = users_pd['zip'].apply(lambda x: x[:2])\ndisplay(users_pd['zip'].head())\nprint(f'By only keep the first two digits of each zip code, \\\nwe reduced the unique number of zip codes to {users_pd.zip.nunique()}.')\n\nWe originally have 3439 different zip codes\nBy only keep the first two digits of each zip code, we reduced the unique number of zip codes to 100.\n\n\n0    48\n1    70\n2    55\n3    02\n4    55\nName: zip, dtype: object\n\n\n\n\n\nThe remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\nWe will load the csv data movieId,tagId,relevance into a dataFrame with the columns mid,tid,relevance.\n\n# load ml-20m tags\ntags_scores = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-scores.csv.gz'),\n                          skiprows=1,\n                          names=['mid', 'tid', 'relevance'])\ndisplay(tags_scores.head(10))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\n\n\n\n\n0\n1\n1\n0.02500\n\n\n1\n1\n2\n0.02500\n\n\n2\n1\n3\n0.05775\n\n\n3\n1\n4\n0.09675\n\n\n4\n1\n5\n0.14675\n\n\n5\n1\n6\n0.21700\n\n\n6\n1\n7\n0.06700\n\n\n7\n1\n8\n0.26275\n\n\n8\n1\n9\n0.26200\n\n\n9\n1\n10\n0.03200\n\n\n\n\n\n\n\n\nprint(f'The data contains {tags_scores.tid.nunique()} unique tags.')\nprint(f'Affinities (relevances) are contained in the {tags_scores.relevance.min()}--{tags_scores.relevance.max()} range.')\ndisplay(tags_scores.relevance.describe())\n\nThe data contains 1128 unique tags.\nAffinities (relevances) are contained in the 0.00024999999999997247--1.0 range.\n\n\ncount    1.170977e+07\nmean     1.164833e-01\nstd      1.542463e-01\nmin      2.500000e-04\n25%      2.425000e-02\n50%      5.650000e-02\n75%      1.415000e-01\nmax      1.000000e+00\nName: relevance, dtype: float64\n\n\nFrom above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12.\nWe also load the tag names. This will be useful for exploration purposes.\n\ntags_names = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-tags.csv'), skiprows=1, names=['tid', 'tName'])\ndisplay(tags_names.head(10))\n\n\n\n\n\n\n\n\ntid\ntName\n\n\n\n\n0\n1\n007\n\n\n1\n2\n007 (series)\n\n\n2\n3\n18th century\n\n\n3\n4\n1920s\n\n\n4\n5\n1930s\n\n\n5\n6\n1950s\n\n\n6\n7\n1960s\n\n\n7\n8\n1970s\n\n\n8\n9\n1980s\n\n\n9\n10\n19th century\n\n\n\n\n\n\n\nSince we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (mid) are the same across these two datasets (i.e., not need for messy string match).\n(Note: Pandas’ functionalities allow you to do operations similar to what you would do in SQL for relational databases.)\n\ntags_scores = tags_scores.loc[tags_scores['mid'].isin(ratings_pd['mid'].unique())]\nprint(tags_scores.mid.nunique())\n\n3470\n\n\nWe lost a few movies compared to the original count of 3706 but we can live with that.\nNext, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie.\n\n# Keep only high-relevance tags (here this is defined as having a relevance above 0.9)\nprint('unique tags:', tags_scores['tid'].nunique())\ntags_scores_high = tags_scores.loc[tags_scores['relevance'] &gt; 0.9]\nprint('unique tags w. high relevance:', tags_scores_high['tid'].nunique())\n\nunique tags: 1128\nunique tags w. high relevance: 968\n\n\n\n\n\nLet’s get some understanding of how these tags are used. To help, we first build a dataFrame that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names).\nPandas’ merge function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)\n\ntags_high_names_movies = pd.merge(tags_scores_high, tags_names, how='inner', on='tid')\ntags_high_names_movies = pd.merge(tags_high_names_movies, movies_pd, how='inner', on='mid')\ndisplay(tags_high_names_movies.head())\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n0\n1\n63\n0.93325\nanimated\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n1\n1\n64\n0.98575\nanimation\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n2\n1\n186\n0.95650\ncartoon\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n3\n1\n203\n0.92625\nchildhood\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n4\n1\n204\n0.96425\nchildren\nToy Story (1995)\nAnimation|Children's|Comedy\n\n\n\n\n\n\n\nSimilarly as above we can search for top movies according to a particular tag:\n\ntag = 'scary' # This is the (sub) tag we search for\ndisplay(tags_high_names_movies[\n    tags_high_names_movies.tName.str.contains(tag,\n                                               regex=False, case=False)].sort_values(by=['relevance'],\n                                                                                    ascending=False))\n\n\n\n\n\n\n\n\nmid\ntid\nrelevance\ntName\nmName\nmGenres\n\n\n\n\n13151\n2710\n882\n0.96700\nscary\nBlair Witch Project, The (1999)\nHorror\n\n\n14005\n1342\n882\n0.96625\nscary\nCandyman (1992)\nHorror\n\n\n9205\n1347\n882\n0.96550\nscary\nNightmare on Elm Street, A (1984)\nHorror\n\n\n14658\n3892\n882\n0.96475\nscary\nAnatomy (Anatomie) (2000)\nHorror\n\n\n3187\n1997\n882\n0.96200\nscary\nExorcist, The (1973)\nHorror\n\n\n14021\n2550\n882\n0.95625\nscary\nHaunting, The (1963)\nHorror|Thriller\n\n\n11492\n1350\n882\n0.94675\nscary\nOmen, The (1976)\nHorror\n\n\n11546\n2841\n882\n0.94650\nscary\nStir of Echoes (1999)\nThriller\n\n\n9340\n1974\n882\n0.94625\nscary\nFriday the 13th (1980)\nHorror\n\n\n2976\n1387\n882\n0.94250\nscary\nJaws (1975)\nAction|Horror\n\n\n5121\n2460\n882\n0.93925\nscary\nTexas Chainsaw Massacre 2, The (1986)\nHorror\n\n\n2488\n1214\n882\n0.93875\nscary\nAlien (1979)\nAction|Horror|Sci-Fi|Thriller\n\n\n9478\n3273\n882\n0.93825\nscary\nScream 3 (2000)\nHorror|Mystery|Thriller\n\n\n13208\n1590\n882\n0.93525\nscary\nEvent Horizon (1997)\nAction|Mystery|Sci-Fi|Thriller\n\n\n13916\n1321\n882\n0.93475\nscary\nAmerican Werewolf in London, An (1981)\nHorror\n\n\n11518\n2160\n882\n0.93325\nscary\nRosemary's Baby (1968)\nHorror|Thriller\n\n\n4825\n2719\n882\n0.92450\nscary\nHaunting, The (1999)\nHorror|Thriller\n\n\n7636\n3499\n882\n0.92350\nscary\nMisery (1990)\nHorror\n\n\n3452\n2762\n882\n0.92075\nscary\nSixth Sense, The (1999)\nThriller\n\n\n2781\n1258\n882\n0.91675\nscary\nShining, The (1980)\nHorror\n\n\n12355\n3081\n882\n0.91425\nscary\nSleepy Hollow (1999)\nHorror|Romance\n\n\n2343\n1200\n882\n0.90475\nscary\nAliens (1986)\nAction|Sci-Fi|Thriller|War\n\n\n\n\n\n\n\nThis next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data.\nIn the current dataset, every movie-tag id pair is a separate entry (row of the dataFrame). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example.\nTo do so, we re-encode tids using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., “cats” and “dogs”) which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag scary is id 882) but tags cannot be compared using their numbers (e.g., tag 882 is not “bigger” than tag 880 or smaller than tag 900). 1-of-K encoding deals with this by encoding each tag as a binary vector of length \\(K\\) with a single non-zero value which corresponds to the tag. In the present case, \\(K=968\\) tags, and tag scary would have a 1 at position 882.\nBelow we see that our data now has 971 columns: 968 for tag ids, 1 for mid, and 1 for relevance, and 1 for the pandas index.\n\n#print(tags_scores_high.shape)\ntags_scores_high_dum = pd.get_dummies(tags_scores_high, columns=['tid'])\ntags_scores_high_dum = tags_scores_high_dum.reset_index()\n#print(tags_scores_high_dum.shape)\ndisplay(tags_scores_high_dum.head())\ntags_per_movie = tags_scores_high_dum.groupby(\"mid\").sum()\n\n\n\n\n\n\n\n\nindex\nmid\nrelevance\ntid_1\ntid_2\ntid_3\ntid_5\ntid_6\ntid_7\ntid_9\n...\ntid_1119\ntid_1120\ntid_1121\ntid_1122\ntid_1123\ntid_1124\ntid_1125\ntid_1126\ntid_1127\ntid_1128\n\n\n\n\n0\n62\n1\n0.93325\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n63\n1\n0.98575\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n185\n1\n0.95650\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n202\n1\n0.92625\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n203\n1\n0.96425\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 971 columns\n\n\n\nWith this data we can then explore the distribution of movies per tag (and tags per movie below).\n\nth = tags_scores_high.groupby(\"tid\").count()\nhists = th.hist(bins=50, column=\"mid\", xlabelsize=15, ylabelsize=15)[0][0]\nhists.set_ylabel(\"Tags\", size=15)\nhists.set_xlabel(\"Movies per tag\", size=15)\nhists.set_title(\"Histogram of Movies per tag\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (&lt;5). On the other hand, the most popular tag was used to tag 210 movies.\nWe note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies.\n\ntname = tags_names.at[tags_names.tid.eq(th.mid.idxmax()).idxmax(), 'tName']\nprint(f'The most popular tag \"{tname}\" has been used for {th.mid.max()} movies')\n\nThe most popular tag \"comedy\" has been used for 210 movies\n\n\nUsing the same recipe, we can do something similar for movies instead of tags\n\nhists = tags_scores_high.groupby(\"mid\").count().hist(bins=40, column=\"tid\", xlabelsize=15, ylabelsize=15)\nhists[0][0].set_ylabel(\"Movies\", size=15)\nhists[0][0].set_xlabel(\"Tags per movie\", size=15)\nhists[0][0].set_title(\"Histogram of Tags per movie\", size=15);\n\n\n\n\nIn this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags.\n\n\n\nWhat is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags?\n\nmh = ...\nmname = ...\nprint(f'The most popular movie \"{mname}\" has {mh.tid.max()} tags')\n\nAttributeError: 'ellipsis' object has no attribute 'tid'\n\n\nIn the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features.\n\n# Join users, ratings, and tags\ndata_pd = pd.merge(users_pd, ratings_pd, how='inner', on='uid')\ndata_pd = pd.merge(data_pd, tags_per_movie, how='inner', on='mid')\n\nFor the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes.\n\n# shuffle data and keep 2% of the ratings.\n# (this small percentage ensures that all computations in this tutorial are fast)\ndata_pd = data_pd.sample(frac=0.02, random_state=1234)\nprint(data_pd.shape)\n\nWe can have a look at our current dataset.\n\nprint('Final descriptive stats of our dataset.')\nprint('\\t- %d items'   % data_pd['mid'].nunique())\nprint('\\t- %d users'   % data_pd['uid'].nunique())\nprint('\\t- %d ratings' % data_pd.shape[0])\n\nNotice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags).\n\nprint(data_pd.shape)\ndisplay(data_pd[:10])\n\n\ncols = ['gender','occupation','zip','mid','uid']\ndata_pd_dum = pd.get_dummies(data_pd, columns=cols)\nprint(data_pd_dum.shape)\ndisplay(data_pd_dum.head(10))\n\nWhile we used pandas to create dummies, scikit-learn has similar capacities. The preprocessing module is detailed here. You can also checkout the section on Categorical features.\nWe are ready to construct our first dataset. We will first use a subset of the columns (not including tags).\nBelow you will also note that we split our data into train and test using train_test_split from scikit-learn.\n\nattributes = \"mid_*|uid_*|gender_*|age|zip_*|occupation_*\"\nX = data_pd_dum.filter(regex=('('+attributes+')'))\nprint(X.shape)\n\nrating = data_pd_dum['rating']\nprint(rating.shape)\n\n# Split Train/Test\n# Keep 20% of the data for testing.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, rating, test_size=0.2, random_state=1234, shuffle=False)\n\nRecommender Systems note: We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items).\nRecommender Systems note #2: Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings).\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n # Section 2: Modelling\nRatings (likert scale) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n\\[ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2\\]\nThe MSE can be understood as the average square distance between the predictor \\(f(x_i)\\) and the target \\(y_i\\). MSE returns a non-negative quantity and the perfect predictor has an MSE of \\(0\\). If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\nTrain/Test: Recall that while we estimate the parameters of the model using the train data, we evaluate the quality of the model (its performance) using test data.\n\n\n\nIt is often helpful to use a very simple benchmark to compare against the performance of our models.\nOur initial benchmark is a model which simply predicts the mean (train) rating.\nRecommender Systems note: We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean.\n\n# Check accuracy of constant predictor\n\nprint(\"Constant predictor\")\n\nprint(\"\\tTrain mean-squared error: %.3f\"\n      % mean_squared_error(y_train,\n                           np.full_like(y_train, y_train.mean())))\nprint(\"\\tTest mean-squared error: %.3f\"\n      % mean_squared_error(y_test,\n                           np.full_like(y_test, y_train.mean())))\n\nThe train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance).\nIn terms of absolute values these indicate that, on average, our predictions are 1.3 units (\\(\\sqrt{1.6}\\)) away from the true rating. This indicates that you shouldn’t be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods.\n\n\n\nFor our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users’ gender, age, zip, and occupation. We fit this model \\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\\(\\theta_{1:6}\\) are the parameters, \\(\\text{gender}_u\\) stands for the gender of user \\(u\\) and similarly for other covariates. Also, \\(x_{\\text{uid}_u}\\) represents the identity of the user and similarly for \\(x_{\\text{mid}_i}\\) and movies.\nNote that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so \\(\\theta_{\\text{zip}}\\) has 100 dimensions.\nTraining this model involves minimizing the train MSE, this is exactly what the LinearRegression class does. (This is a least-squares problem and it can be solved in closed form.)\n\n# Create linear regression object\nreg = linear_model.LinearRegression()\n\n# Train the model using the training sets\nreg.fit(X_train, y_train)\n\nprint(\"Number of parameters: \", reg.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = reg.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = reg.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nWe note that train error \\(&lt;&lt;\\) test error (\\(&lt;&lt;\\) stands for “much smaller”). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it’s a low bias and high variance model).\nDifferent methods can help prevent the model from overfitting, this is often referred to as regularizing the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: \\[  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 \\]\nInstead of the previous $ := $.\nNote: - \\(||\\cdot||_2\\) stands for the 2-Norm. That is, the square root of the sum of the operand’s squared elements. - \\(\\alpha\\) is a hyper-parameter which denotes the strength of the regularizer (if \\(\\alpha=0\\) the regularizer vanishes and if \\(\\alpha=\\infty\\) all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning \\(\\alpha\\) along with the \\(\\theta\\)s would lead to a \\(\\alpha=0\\)).\nDuring learning the model must then tradeoff performance (MSE) and complexity (high \\(\\theta\\)s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the Ridge class from the linear_model package to fit regularized linear regression models.\n\n# Create linear regression object\nregr = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr.fit(X_train, y_train)\nfit_time = time.time() - start\n\nprint(\"Fitting time: %.2f seconds\" % fit_time)\n\nprint(\"Number of parameters:\", regr.coef_.shape[0]+1)\n\nQuestion 3 Explain why there are 7,184 parameters.\nHint: Don’t forget the intercept/bias term\n\n# Make train predictions\ny_train_pred = regr.predict(X_train)\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr.predict(X_test)\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nCompared to above, we see that with \\(\\alpha=10\\) the train and test errors are much closer (i.e., there’s less overfitting). Presumably different values of \\(\\alpha\\) would yield different generalizations.\nQuestion 4 How do you find the “best” value of \\(\\alpha\\) for a given model and dataset?\nHint: Have a look at the RidgeCV, a cross-validation enabled version of Ridge.\nAnswer: See below.\n\n# Create linear regression object\nregRCV = ...\n\n# Train the model using the training sets\n\n\nprint(\"Number of parameters: %d, estimated alpha: %d\" % (regRCV.coef_.shape[0], regRCV.alpha_))\n\nTechnical remark: since the optimization is often done in log space, it’s typical for the set of \\(\\alpha\\)’s to be powers of 10.\n\n# Make train predictions\ny_train_pred = ...\n\nprint(\"Train Mean squared error: %.3f\"\n      % mean_squared_error(y_train, y_train_pred))\n\n# Make test predictions\ny_test_pred = ...\n\nprint(\"Test Mean squared error: %.3f\"\n      % mean_squared_error(y_test, y_test_pred))\n\nThe advantage of doing cross validation (for example using RidgeCV) is clear. It automatically searches for the best values of hyperparameters (here \\(\\alpha\\)) from a given set (here \\(\\{ 1, 10, 100 \\}\\)).\nA validation set (or cross validation) should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement validation. In such cases you will likely need to split a separate validation set from your training data–in sklearn you can use the train_test_split function. It is typical for the validation set to be the same size as the test set.\nYou should also remember to never select model hyperparameters based on performance on test set, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models.\n\nArmed with a good model we can explore the learned model including some of its predictions\n\n# helper function to return non-zero columns\ndef non_zero(row, columns):\n    col_name = list(columns[~(row == 0)])[0]\n    #r = re.sub('mid_','',l)\n    return col_name\n\n\n# get number of ratings per movie (popularity)\nmids = X_test.filter(regex=('mid_*'))\ny_mid_cols = mids.apply(lambda x: non_zero(x, mids.columns), axis=1)\nmovie_popularity = X_train.filter(regex=('mid_*')).sum(axis=0)[ y_mid_cols ]\n\n# get number of ratings per user (activity)\nuids = X_test.filter(regex=('uid_*'))\ny_uid_cols = uids.apply(lambda x: non_zero(x, uids.columns), axis=1)\nuser_activity = X_train.filter(regex=('uid_*')).sum(axis=0)[ y_uid_cols ]\n\nerr = (y_test_pred-y_test)\n\n\n# only plot a subsample for higher readability\nsubn = 500\nfig, (ax0, ax1) = plt.subplots(ncols=2)\nfig.set_figwidth(15)\nax0.scatter(movie_popularity[:subn], err[:subn])\nax0.set_ylabel('Prediction error')\nax0.set_xlabel('Movie Popularity')\n\nax1.scatter(user_activity[:subn], err[:subn])\nax1.set_ylabel('Prediction error')\nax1.set_xlabel('User Activity');\n\nAbove we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n\nThis empirical distribution looks symmetrical so there doesn’t seem to be a bias toward lower or higher predictions\nThe prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a “triangle” pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters (\\(\\theta_{\\text{mid}}\\)). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process.\n\n\n\n\nWe use a linear regression model as above but also model movie tags:\n\\[\nf(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n\\]\nThe last term on the right-hand side (bolded) is the only difference wrt to our previous model.\nQuestion 5: How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance?\nHint: One model is a special case of another.\n\n# This is very similar to how we constructed our dataset above except that we add the tags columns\nX_tags = data_pd_dum.filter(regex=('('+attributes+\"|tid_*\"+')'))\nprint(X_tags.shape)\n\n# Split Train/Test. Notice that we use the same seed as above to replicate that split.\nX_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(\n    X_tags, rating, test_size=0.2, random_state=1234, shuffle=False)\nprint(X_train_tags.shape)\n\n\n# Create linear regression object\nregr_tags = linear_model.Ridge(alpha=10)\n\n# Train the model using the training sets\nstart = time.time()\nregr_tags.fit(X_train_tags, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", regr_tags.coef_.shape[0]+1)\n\n\n# Make train predictions\ny_train_pred = regr_tags.predict(X_train_tags)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_tags.predict(X_test_tags)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n\nRemarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n\n\n\n\nSo far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant.\nHere we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\nNeural network basics: (We will discuss these models in some depth over the next two weeks.) - A neural network is made up of interconnected neurons. Each neuron computes a few simple operations. - In a feed-forward network, neurons are organized into sets called layers. - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer. - The first layer is called the input layer it provides data to the next layer. The last layer is the output layer it provides a prediction \\(\\hat{y}\\). - Layers in between the input layer and the output layer are called hidden layers. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (activation function): \\(f(x) = \\sum_i x_i \\theta_i\\). - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions). - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers.\nMathematically for a regression task (with a single output), a one-hidden layer neural net is: \\[\nf(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) )\n\\] where - \\(\\theta_{ij}\\) are the parameters of input \\(i\\) and neuron \\(j\\) in the hidden layer. - \\(f_h\\) is the activation function of the hidden layer - \\(\\theta'_{j}\\) are the parameters that connect the neuron \\(j\\) in the hidden layer to the output layer. - \\(f_o\\) is the activation function of the output layer\nAn intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:\n\n# a 1 hidden layer neural net, where the input has 10 dimensions (p=10) and the output 1\ninput_dims = 10 # p\nhidden_layers_size = [4] # number of hidden neurons for each hidden layer (adding a dimension adds a layer)\noutput_dims = 1 # number of outputs\n\nnetwork = DrawNN( [input_dims] + hidden_layers_size + [output_dims] )\nnetwork.draw()\n\n\n# Fit a neural network on this data.\nregr_nn = neural_network.MLPRegressor(alpha=0.1, # l2-regularization (weight decay)\n                                      hidden_layer_sizes=tuple(hidden_layers_size),\n                                      early_stopping=True, # stop if validation performance decreases\n                                      verbose=True,\n                                      random_state=1234)\nstart = time.time()\nregr_nn.fit(X_train_tags.values, y_train_tags)\nfit_time = time.time() - start\n\nprint(\"fitting time: %.2f seconds\" % fit_time)\nprint(\"number of parameters:\", reduce(lambda x,y: x+y,\n                                       list(map(lambda x: x.size, regr_nn.coefs_+regr_nn.intercepts_)) ))\n\nMuch like previous models we can regularize a neural net to combat overfitting: - Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by \\(\\alpha\\). - In addition, we use a second regularizer called early-stopping. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In scikit-learn, the MLPRegressor class with early_stopping=True automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters.\nQuestion 6: Why does this model come with the possibility to set the random seed (i.e., random_state) while linear regression did not?\n\n# Make train predictions\ny_train_pred = regr_nn.predict(X_train_tags.values)\n\nprint(\"Train Mean squared error: %.4f\"\n      % mean_squared_error(y_train_tags, y_train_pred))\n\n# Make test predictions\ny_test_pred = regr_nn.predict(X_test_tags.values)\n\nprint(\"Test Mean squared error: %.4f\"\n      % mean_squared_error(y_test_tags, y_test_pred))\n#Train Mean squared error: 0.6623\n#Test Mean squared error: 1.0465\n\nHere is our updated table of results\n\n\n\nModel\nTest MSE\n\n\n\n\n2.2 (Linear Reg. w. basic features)\n1.031\n\n\n2.3 (2.2 + movie tags)\n0.991\n\n\n2.4 (Neural Network w. features from 2.3)\n1.029\n\n\n\nAlthough neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better.\n ### Section 3. Concluding Remarks\nThe goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it’s coming from and how to model it.\nHere are a few more parting thoughts:\n\n\nAs you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n\n\n\nscikit-learn is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\nModel Selection, i.e., which model should I use for a particular dataset/task can be daunting. This page provides some tips particular to scikit-learn. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\nNote that scikit-learn does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n\n\n\nSoftware is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. scikit-learn is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more.\n\n\n\n\nIn our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don’t have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be:\n\n\\[\nf(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n\\]\n\nQuestion 7: What’s wrong with the above model? Try to think about it for a minute or two before looking at the answer.\nAs we will see during week 11 (on recommender systems), many models take ratings as output and as input. For example, one could take a user’s previous ratings and try to predict one’s future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist)."
  },
  {
    "objectID": "Introduction_to_ML.html#references",
    "href": "Introduction_to_ML.html#references",
    "title": "MATH60629A Fall 2022",
    "section": "",
    "text": "Scikit-learn - Documentation - Tutorials - Help for model selection"
  },
  {
    "objectID": "Neural_Networks_questions.html",
    "href": "Neural_Networks_questions.html",
    "title": "MATH60629A",
    "section": "",
    "text": "This tutorial explores neural networks.\n\nimport numpy as np\n\n# Code to obtain utils.py\n!wget https://raw.githubusercontent.com/lcharlin/80-629/master/week5-NeuralNetworks/utils.py -O utilities.py\n\n--2021-10-01 15:19:56--  https://raw.githubusercontent.com/lcharlin/80-629/master/week5-NeuralNetworks/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13166 (13K) [text/plain]\nSaving to: ‘utilities.py’\n\nutilities.py        100%[===================&gt;]  12.86K  --.-KB/s    in 0.002s  \n\n2021-10-01 15:19:56 (7.38 MB/s) - ‘utilities.py’ saved [13166/13166]\n\n\n\n\n\nIn order to classify the examples, we will use the following simple neural network:\n\nwhere \\(\\sigma\\) is the sigmoid function defined as:\n\\[\n    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n\\]\n\n\nAssume that the parameters of the neural network are as follows:\n\\[\\begin{aligned}\n& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n& b_1 = 25 & b_2 = 40 && b_3 = -30\n\\end{aligned}\\]\nWhat would be the predicted label for the following data points:\n\n\n\nx1\nx2\no\nlabel\n\n\n\n\n4\n-4\n\n\n\n\n-4\n4\n\n\n\n\n-4\n-4\n\n\n\n\n4\n4\n\n\n\n\n\nYou can use the following piece of code to evaluate the output of the network:\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef nn1(x1, x2, w1, w2, w3, w4, w5, w6, b1, b2, b3):\n    h1 = sigmoid(w1*x1 + w3*x2 + b1)\n    h2 = sigmoid(w2*x1 + w4*x2 + b2)\n    o = sigmoid(w5*h1 + w6*h2 + b3)\n    return o\n\n\n\n\n\nLet’s move to a slightly more realistic example. Here we focus on the task of (binary) classification. As always, we first load the data that we want to classify:\n\nfrom utilities import load_data, plot_boundaries, plot_data # we wrote some helper functions\nX_train, y_train, X_test, y_test = load_data()          # to help with data loading\n\nYou can plot the data using the helper function plot_data:\n\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nAs you can see, this data is not linearly separable. In other words, the positive and negative examples cannot be separated using a linear classifier. Our goal for the rest of this notebook-session is to learn the parameters of a neural-network model which can separate the positives from the negative examples.\nWhat do we mean by learning the parameters? Remember that our neural network has 9 parameters including three biases (\\(w_1, \\ldots, w_6, b_1, b_2, b_3\\)). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best.\nLet’s see how different choices of parameters changes the classifier. For a given set of parameters, the function plot_boundaries shows the regions of positive prediction (coloured blue) and negative prediction (coloured red):\n\nw1 = 1; w2 = 1; w3 = 1; w4 = 1; w5 = 1; w6 = 1\nb1 = 0; b2 = 0; b3 = -1\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n\n\n\n\nNow let’s project the plot of data on these decision boundaries:\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nIt appears that the classifier obtained using the above set of parameters does not match our data. (Of course, this is to be expected. This classifier with fixed weights a priori has a high bias and a low variance.)\n\n\nTry the alternatives below and see which one is a better match for our data:\n\nw1 = -1; w2 = -1; w3 = -1; w4 = -1; w5 = 4; w6 = -3\nb1 = -4; b2 = 4; b3 = 1\n\n\nw1 = 1; w2 = -1; w3 = -1; w4 = -1; w5 = -4; w6 = 3\nb1 = 4; b2 = -4; b3 = 2\n\n\nw1 = -1; w2 = 2; w3 = 1; w4 = -2; w5 = 4; w6 = 4\nb1 = 5; b2 = 8; b3 = -6\n\nObviously, we need a better way than trial and error to find the best parameters. The way that we do this is by minimizing a loss function.\n\n\n\n\nA loss function evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the binary cross-entropy loss. Let’s represent our training data by the set \\(\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}\\) and our neural network function by \\(f\\). Then the binary cross-entropy loss function will be defined as:\n\\[\\begin{equation}\n    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n\\end{equation}\\]\nThe binary cross-entropy relates to the Bernoulli distribution (maximizing the Bernoulli likelihood is equivalent to minimizing the binary cross-entropy). It is the loss function that should be used for binary classification problems. It can be generalized to multiclass classification problems, see cross entropy.\n\n\nLet’s see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of \\(X, f(X), y\\) of those four examples are as follows:\n\n\n\nX\nf(X)\ny\n\n\n\n\n(5.4, 1.6)\n1\n1\n\n\n(1.4, -0.5)\n0.3679\n1\n\n\n(3.5, -3)\n0.8647\n0\n\n\n(-3.5, 1.1)\n0\n0\n\n\n\nCalculate the loss function using the equation above. You can calculate the log using this function:\n\nnp.log(0.5)\n\n-0.6931471805599453\n\n\nIt is important to remember that the loss function \\(l\\) is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n\\[\\begin{equation}\n    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n\\end{equation}\\]\nIn principle, we want to find the set of parameters \\(\\mathbf{w}, \\mathbf{b}\\) for which \\(\\ell(\\mathbf{w}, \\mathbf{b})\\) has the smallest value. We will use gradient descent to find these values.\n\n\n\nThe plot below shows the function \\(f(x_1, x_2) = x_1^2 + x_2^2\\):\n\n\n\n\nPoint A on the plot has coordinates \\((1, 1, 3)\\). The blue vector AB shows the direction \\((-1, -1)\\), and the green vector AC shows the direction \\((0, -1)\\). Assume that we are at initial point \\((1, 1)\\) and we want to move in a direction that minimizes the function \\(f\\). Which of these two directions moves faster towards the minimum: \\((-1, -1)\\) or \\((0, -1)\\)?\n\n\n\nCalculate the gradient of function \\(f\\) in the point \\((1, 1)\\). How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?\n\n\n\n\nWe now understand the theory of training neural networks. But how do we do this in practice? We will now develop our practical skills using the scikit-learn library to train our tiny network. Let’s first define the network:\n\nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(2,),\n                    activation='logistic',\n                    solver='lbfgs',\n                    random_state=0,\n                    max_iter=500,\n                    tol=1e-7)\n\nThe argument hidden_layer_sizes=(2,) states that we only have one hidden layer with two neurons, and the argument activation='logistic' shows that we use the sigmoid activation function (Let’s ignore the other arguments for now).\nWe will now train the network using our training data:\n\nclf.fit(X_train, y_train)\n\nMLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(2,), learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=0, shuffle=True, solver='lbfgs',\n              tol=1e-07, validation_fraction=0.1, verbose=False,\n              warm_start=False)\n\n\nOnce the network is trained, use the helper function tiny_net_parameters to get the parameters of the trained network (tiny_net_parameters is a wrapper around clf.coefs_ and clf.intercepts_):\n\nfrom utils import tiny_net_parameters\nw1, w2, w3, w4, w5, w6, b1, b2, b3 = tiny_net_parameters(clf)\n\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nThe learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data).\n\nIn addition to the decision boundaries in the original data space, we can also visualize how the data are transformed through the neural networks. Since we use a hidden layer with two neurons, we can visualize its “output” in two dimensions.\nEn plus des frontières de décisions dans l’espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions.\n(for better visibility, we changed the color of the yellow class to blue.)\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_train, y_train, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')\n\n\n\n\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_test, y_test, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')\n\n\n\n\n\n\n\nWe will now investigate a few properties of neural networks using tensorflow playground. Take a few minutes to familiarize yourself with the playground:\n\nChange the number of hidden layers to one\nChange the data distribution to exclusive OR\nPush the run button and see how the network is trained\nStop training after epoch 500 (each epoch involves doing gradient descent using the complete dataset)\nHover over the neurons in the hidden layer and see the vizualization of their outputs.\n\n\n\nOpen this example on tensorflow playground.\n\nPush the run button and see the learning process for 500 epochs. What do you observe?\nStop training and press the restart button. Change the learning rate from 3 to 0.1, and press the run button again. What is different from the previous run?\nTry these steps using three learning rates: 0.3, 0.03, and 0.003:\nPress the reset button\nChange the learning rate\nPress the step button (located at the right of run button) a few times, and observe how the training/test loss changes in each step.\n\nWhich of those three rates would you use?\n\n\n\nOpen this example on playground.\nLet’s first observe a few things about this example. Check the box titled Show test data. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting. - Press the run button and let the training proceed for 500 epochs, then pause the training. - What do you think about the decision boundary of the classifier? - What causes the difference between the training error and test error? (Check the Show test data box again) - Write down the test error\nWe will now see how we can avoid overfitting using \\(L_2\\) regularization. - Press the restart button - Change regularization from None to L2 - Change Regularization rate from 0 to 0.3 - Press the run button and run the model for 500 epochs - What is different from the previous setting? - Write down the test error\nJust like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003: - Press the restart button - Change Regularization rate - Press the run button and run the model for 500 epochs - Write down the test error\nWhich of these regularization rates would you use?"
  },
  {
    "objectID": "Neural_Networks_questions.html#a-tiny-neural-network-classifier",
    "href": "Neural_Networks_questions.html#a-tiny-neural-network-classifier",
    "title": "MATH60629A",
    "section": "",
    "text": "In order to classify the examples, we will use the following simple neural network:\n\nwhere \\(\\sigma\\) is the sigmoid function defined as:\n\\[\n    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n\\]\n\n\nAssume that the parameters of the neural network are as follows:\n\\[\\begin{aligned}\n& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n& b_1 = 25 & b_2 = 40 && b_3 = -30\n\\end{aligned}\\]\nWhat would be the predicted label for the following data points:\n\n\n\nx1\nx2\no\nlabel\n\n\n\n\n4\n-4\n\n\n\n\n-4\n4\n\n\n\n\n-4\n-4\n\n\n\n\n4\n4\n\n\n\n\n\nYou can use the following piece of code to evaluate the output of the network:\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef nn1(x1, x2, w1, w2, w3, w4, w5, w6, b1, b2, b3):\n    h1 = sigmoid(w1*x1 + w3*x2 + b1)\n    h2 = sigmoid(w2*x1 + w4*x2 + b2)\n    o = sigmoid(w5*h1 + w6*h2 + b3)\n    return o"
  },
  {
    "objectID": "Neural_Networks_questions.html#finding-good-parameters-for-our-network",
    "href": "Neural_Networks_questions.html#finding-good-parameters-for-our-network",
    "title": "MATH60629A",
    "section": "",
    "text": "Let’s move to a slightly more realistic example. Here we focus on the task of (binary) classification. As always, we first load the data that we want to classify:\n\nfrom utilities import load_data, plot_boundaries, plot_data # we wrote some helper functions\nX_train, y_train, X_test, y_test = load_data()          # to help with data loading\n\nYou can plot the data using the helper function plot_data:\n\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nAs you can see, this data is not linearly separable. In other words, the positive and negative examples cannot be separated using a linear classifier. Our goal for the rest of this notebook-session is to learn the parameters of a neural-network model which can separate the positives from the negative examples.\nWhat do we mean by learning the parameters? Remember that our neural network has 9 parameters including three biases (\\(w_1, \\ldots, w_6, b_1, b_2, b_3\\)). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best.\nLet’s see how different choices of parameters changes the classifier. For a given set of parameters, the function plot_boundaries shows the regions of positive prediction (coloured blue) and negative prediction (coloured red):\n\nw1 = 1; w2 = 1; w3 = 1; w4 = 1; w5 = 1; w6 = 1\nb1 = 0; b2 = 0; b3 = -1\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n\n\n\n\nNow let’s project the plot of data on these decision boundaries:\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nIt appears that the classifier obtained using the above set of parameters does not match our data. (Of course, this is to be expected. This classifier with fixed weights a priori has a high bias and a low variance.)\n\n\nTry the alternatives below and see which one is a better match for our data:\n\nw1 = -1; w2 = -1; w3 = -1; w4 = -1; w5 = 4; w6 = -3\nb1 = -4; b2 = 4; b3 = 1\n\n\nw1 = 1; w2 = -1; w3 = -1; w4 = -1; w5 = -4; w6 = 3\nb1 = 4; b2 = -4; b3 = 2\n\n\nw1 = -1; w2 = 2; w3 = 1; w4 = -2; w5 = 4; w6 = 4\nb1 = 5; b2 = 8; b3 = -6\n\nObviously, we need a better way than trial and error to find the best parameters. The way that we do this is by minimizing a loss function."
  },
  {
    "objectID": "Neural_Networks_questions.html#loss-function",
    "href": "Neural_Networks_questions.html#loss-function",
    "title": "MATH60629A",
    "section": "",
    "text": "A loss function evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the binary cross-entropy loss. Let’s represent our training data by the set \\(\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}\\) and our neural network function by \\(f\\). Then the binary cross-entropy loss function will be defined as:\n\\[\\begin{equation}\n    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n\\end{equation}\\]\nThe binary cross-entropy relates to the Bernoulli distribution (maximizing the Bernoulli likelihood is equivalent to minimizing the binary cross-entropy). It is the loss function that should be used for binary classification problems. It can be generalized to multiclass classification problems, see cross entropy.\n\n\nLet’s see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of \\(X, f(X), y\\) of those four examples are as follows:\n\n\n\nX\nf(X)\ny\n\n\n\n\n(5.4, 1.6)\n1\n1\n\n\n(1.4, -0.5)\n0.3679\n1\n\n\n(3.5, -3)\n0.8647\n0\n\n\n(-3.5, 1.1)\n0\n0\n\n\n\nCalculate the loss function using the equation above. You can calculate the log using this function:\n\nnp.log(0.5)\n\n-0.6931471805599453\n\n\nIt is important to remember that the loss function \\(l\\) is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n\\[\\begin{equation}\n    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n\\end{equation}\\]\nIn principle, we want to find the set of parameters \\(\\mathbf{w}, \\mathbf{b}\\) for which \\(\\ell(\\mathbf{w}, \\mathbf{b})\\) has the smallest value. We will use gradient descent to find these values.\n\n\n\nThe plot below shows the function \\(f(x_1, x_2) = x_1^2 + x_2^2\\):\n\n\n\n\nPoint A on the plot has coordinates \\((1, 1, 3)\\). The blue vector AB shows the direction \\((-1, -1)\\), and the green vector AC shows the direction \\((0, -1)\\). Assume that we are at initial point \\((1, 1)\\) and we want to move in a direction that minimizes the function \\(f\\). Which of these two directions moves faster towards the minimum: \\((-1, -1)\\) or \\((0, -1)\\)?\n\n\n\nCalculate the gradient of function \\(f\\) in the point \\((1, 1)\\). How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?"
  },
  {
    "objectID": "Neural_Networks_questions.html#training-the-neural-network",
    "href": "Neural_Networks_questions.html#training-the-neural-network",
    "title": "MATH60629A",
    "section": "",
    "text": "We now understand the theory of training neural networks. But how do we do this in practice? We will now develop our practical skills using the scikit-learn library to train our tiny network. Let’s first define the network:\n\nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(2,),\n                    activation='logistic',\n                    solver='lbfgs',\n                    random_state=0,\n                    max_iter=500,\n                    tol=1e-7)\n\nThe argument hidden_layer_sizes=(2,) states that we only have one hidden layer with two neurons, and the argument activation='logistic' shows that we use the sigmoid activation function (Let’s ignore the other arguments for now).\nWe will now train the network using our training data:\n\nclf.fit(X_train, y_train)\n\nMLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(2,), learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=0, shuffle=True, solver='lbfgs',\n              tol=1e-07, validation_fraction=0.1, verbose=False,\n              warm_start=False)\n\n\nOnce the network is trained, use the helper function tiny_net_parameters to get the parameters of the trained network (tiny_net_parameters is a wrapper around clf.coefs_ and clf.intercepts_):\n\nfrom utils import tiny_net_parameters\nw1, w2, w3, w4, w5, w6, b1, b2, b3 = tiny_net_parameters(clf)\n\n\nplot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\nplot_data(X_train, y_train, X_test, y_test)\n\n\n\n\nThe learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data).\n\nIn addition to the decision boundaries in the original data space, we can also visualize how the data are transformed through the neural networks. Since we use a hidden layer with two neurons, we can visualize its “output” in two dimensions.\nEn plus des frontières de décisions dans l’espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions.\n(for better visibility, we changed the color of the yellow class to blue.)\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_train, y_train, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')\n\n\n\n\n\nfrom utilities import plot_data_transformations\nplot_data_transformations(X_test, y_test, w1, w2, w3, w4, w5, w6, b1, b2, b3, \\\n                          language='English')"
  },
  {
    "objectID": "Neural_Networks_questions.html#tensorflow-playground-questions",
    "href": "Neural_Networks_questions.html#tensorflow-playground-questions",
    "title": "MATH60629A",
    "section": "",
    "text": "We will now investigate a few properties of neural networks using tensorflow playground. Take a few minutes to familiarize yourself with the playground:\n\nChange the number of hidden layers to one\nChange the data distribution to exclusive OR\nPush the run button and see how the network is trained\nStop training after epoch 500 (each epoch involves doing gradient descent using the complete dataset)\nHover over the neurons in the hidden layer and see the vizualization of their outputs.\n\n\n\nOpen this example on tensorflow playground.\n\nPush the run button and see the learning process for 500 epochs. What do you observe?\nStop training and press the restart button. Change the learning rate from 3 to 0.1, and press the run button again. What is different from the previous run?\nTry these steps using three learning rates: 0.3, 0.03, and 0.003:\nPress the reset button\nChange the learning rate\nPress the step button (located at the right of run button) a few times, and observe how the training/test loss changes in each step.\n\nWhich of those three rates would you use?\n\n\n\nOpen this example on playground.\nLet’s first observe a few things about this example. Check the box titled Show test data. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting. - Press the run button and let the training proceed for 500 epochs, then pause the training. - What do you think about the decision boundary of the classifier? - What causes the difference between the training error and test error? (Check the Show test data box again) - Write down the test error\nWe will now see how we can avoid overfitting using \\(L_2\\) regularization. - Press the restart button - Change regularization from None to L2 - Change Regularization rate from 0 to 0.3 - Press the run button and run the model for 500 epochs - What is different from the previous setting? - Write down the test error\nJust like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003: - Press the restart button - Change Regularization rate - Press the run button and run the model for 500 epochs - Write down the test error\nWhich of these regularization rates would you use?"
  }
]