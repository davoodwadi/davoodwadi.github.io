[
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\nWeek 4\n\nIntroduction to PyTorch (Part 1) (colab)\n\nWeek 5\n\nIntroduction to PyTorch (Part 2)\nDataset class in PyTorch\nBatching and Dataloader in PyTorch\nBackpropagation with PyTorch\nLinear regression with gradient descent\nClassification using MLP with Pytorch\n\nWeek 6\n\nMinibatch training in Pytorch\nBatch Normalization\nHigh accuracy: CIFAR-10\nIntroduction to RNNs\nTime dependence\nLanguage modeling (RNN and Transformer)\n\nWeek 7\n\nK-Means implementation (session 7 - in-class code)\n\nWeek 10\n\nMulti-GPU training with DataParallel\nMulti-Node training with DistributedDataParallel\n\nWeek 11\n\nSingular Value Decomposition with PyTorch\nSingular Value Decomposition for MovieLens100k\n\nWeek 12\n\nValue iteration\nPolicy iteration\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "courses.html#course-information",
    "href": "courses.html#course-information",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Code Tutorials | Contact Information | Bibliographic Sources | Class Information | Evaluations | Session Materials\n\n\n\nWeek 1\n\nPython basics\n\nWeek 2\n\nModel capacity\nOverfitting\nLinear regression - Part 1\nLinear regression - Part 2\nBias/variance tradeoff\nL2 regularization\n\nWeek 3\n\nLinear least squares for classification\nSVM for classification\nGaussian Naive Bayes\nMultinomial Naive Bayes\nPrecision, Recall, F1-Score\nPrecision-Recall Curve\n\nWeek 4\n\nIntroduction to PyTorch (Part 1) (colab)\n\nWeek 5\n\nIntroduction to PyTorch (Part 2)\nDataset class in PyTorch\nBatching and Dataloader in PyTorch\nBackpropagation with PyTorch\nLinear regression with gradient descent\nClassification using MLP with Pytorch\n\nWeek 6\n\nMinibatch training in Pytorch\nBatch Normalization\nHigh accuracy: CIFAR-10\nIntroduction to RNNs\nTime dependence\nLanguage modeling (RNN and Transformer)\n\nWeek 7\n\nK-Means implementation (session 7 - in-class code)\n\nWeek 10\n\nMulti-GPU training with DataParallel\nMulti-Node training with DistributedDataParallel\n\nWeek 11\n\nSingular Value Decomposition with PyTorch\nSingular Value Decomposition for MovieLens100k\n\nWeek 12\n\nValue iteration\nPolicy iteration\n\n\n\n\n\n\n\n\n\n\n\n\nDavood Wadi\n\nPart-time Faculty Lecturer\ndavood.wadi@hec.ca\nFor office hours:\n\nBook meeting\n\n\nValérie Boucher\n\nAssistant to Academic Activities\nvalerie.boucher@hec.ca\n514-340-5670\nOffice: 4.632\nAvailability:\n\nMonday to Friday\n8:30 a.m. to 12 p.m. and 1 to 4:00 p.m.\n\n\nAntonietta Florio\n\nAssistant to Academic Activities\nantonietta.florio@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\nJennifer Caron\n\nExecutive Assistant\njennifer.caron@hec.ca\n514-340-6473\nOffice: 4.632\nAvailability:\n\nFrom Monday to Friday\nFrom 8 am to 12 pm and from 1 pm to 4:30 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n\nAvailable online\n\nGoodfellow, Ian . Deep learning , MIT Press ISBN: 9780262035613\n\nAvailable online\n\nRichard S. Sutton, Andrew G. Barto. A Bradford Book (2017 (in progress)). Reinforcement Learning: An Introduction Hardcover, 2nd edition ed., The MIT Press\n\nAvailable online\n\nMurphy, Kevin (2012). Machine Learning: A Probabilistic Perspective, MIT Press ISBN: 9780262018029\n\nAvailable at the library\n\nRicci, Francesco. Recommender systems handbook, Springer ISBN: 148997637X\n\nAvailable at the library\n\nParsian, Mahmoud (2015). Data algorithms: recipes for scaling up with Hadoop and Spark, O’Reilly Media ISBN: 9781491906156\n\nAvailable at the library\n\nMcKinney, Wes. Python for data analysis: data wrangling with Pandas, NumPy, and IPython, O’Reilly Media\n\nAvailable at the library\n\nBishop, Christopher (2006). Pattern Recognition and Machine Learning, Springer-Verlag New York ISBN: 9780387310732\n\nAvailable at the library\n\nLeskovec, Jurij. Mining of massive datasets, Cambridge University Press\n\nAvailable at the library\n\nGrus, Joel. Data science from scratch, O’Reilly Media\n\nAvailable at the library\n\nSandy Ryza, Uri Laserson, Sean Owen, Josh Wills (2017). Advanced analytics with Spark: patterns from learning from data at scale, 2nd edition ed., O’Reilly Media\n\nAvailable at the library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\nBuilding C-Ste-Cath, Room Groupe Cholette\n\nTime\n\nTuesdays, 3:30 PM - 6:30 PM EDT"
  },
  {
    "objectID": "courses.html#evaluations",
    "href": "courses.html#evaluations",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "Evaluations",
    "text": "Evaluations\n\n[Bonus] Class participation (5%)\nTo receive this 5% bonus point:\n\nPrepare for each session’s code before the class starts.\nLog in to deepnote with your Full Name.\nDuring the coding exercises in class, add your answer to the questions as a comment.\n\n\n\nHomework (20%)\n\nDue on 24 October 2023\nHomework assignments are counted for 20% of your final grade.\nThe homework should be done in jupyter notebook here or Google Colab here.\nInstructions:\n\nThe homework is due by 11:59PM EST on the due date.\nPlease upload a PDF version of your assignment on ZoneCours and insert the shareable link to your notebook in the same interface.\nHomeworks must be completed individually.\n\nDiscussion with others is okay, but you must write solutions yourself.\n\nAll code used to arrive at answers is submitted along with answers.\nNotes:\n\nPlease provide your code answers in the code block under each question and verbal answers in text boxes assigned in the notebook (where applicable).\nPlease run the notebook before the submission so that the outputs are displayed.\nPlease make sure that your results are reproducible. You may use random seeds from random and numpy packages. For scikit-learn modules, you may use the random_state argument.\n\n\n\n\n\nProject (30%)\nThe aim of this project is to allow you to learn about machine learning by trying to solve a task with it.\nFirst, select a question that can be answered using machine learning. I expect that your question will be about a model/algorithm or about an application. Then design a study that will try to answer your question. Your study must have an element of novelty. For example the novelty could be an extension or a variation of an existing algorithm or results of an existing method on a new dataset.\nYour study should involve reading and understanding some background material. Your study must involve running some experiments. You are free to use (or not) any of the tools or models we have seen in class.\nStudy plan: (1 upload per group) Please submit a one-page summary of your proposed research question and study to ZoneCours. I will meet with each group to discuss study plans during the lecture on Week 9. I will send you a schedule the day before. We will probably only have about 15 minutes so please make sure that your study plan is clear and precise. You may also include questions that you would like us to discuss at the end of the document.\nThe group report: (1 upload per group) Your report must contain a description of the question you are trying to answer, a clear description of the model/algorithm you are studying, a survey of related work which proper references, an empirical section that reports your results, and a conclusion that summarizes your findings and (if pertinent) highlights possible future directions of investigation. Your report should be no longer than 10 pages in length (plus references) for pairs or 13 pages (plus references) for teams of three.\nThe individual report: (1 upload per student) You will also submit a brief individual report (at most one page), which will: (1) Describe the parts of the project you worked on (which machine learning methods you applied, which preprocessing steps you performed on the data, which parts of the term paper you wrote, who you worked with on what parts, etc.) and what parts of the project your teammates worked on. (2) What you learned from the project. The purpose of the individual report is to facilitate fair grading and to allow the instructor to understand well what you learned from the project.\nProject Report (30%)\n\nClarity/Relevance of problem statement and description of approach: 10%\nDiscussion of relationship to previous work and references: 4%\nDesign and execution of experiments: 10%\nFigures/Tables/Writing: easily readable, properly labeled,\ninformative: 5%\nIndiviual report: 1%\n\n\nProject Topic\nConcretely, your project could take one of the following forms:\n\nA competition on kaggle.com that you try multiple ML methods on and can achieve a validation set performance in the top 100 on the leaderboard.\nA benchmark task on paperswithcode (e.g. Speech Recognition) where you try multiple methods and compare their performance.\nA task in your own field whereby you apply multiple ML methods to solve the task.\nAn ML method that you apply to a new task (e.g. Multi-head attention applied to time-series prediction)\n\nNote. The list above is not exhaustive. You are encouraged to be creative about the topic. Pick a topic that you find interesting and is relevant. The appropriateness of the topic will be discussed and resolved in the session on Study plan.\n\n\nGroup Report Structure\nYour group report should read like a published paper. Similarly, your poster presentation should have a similar structure to conference posters. You might want to check out similar papers to your topic on the structure of your report (e.g. A Comparison of Deep Learning Approach for Underwater Object Detection).\nRegardless of your topic, as a general rule of thumb, your Group Report should be structured as follows:\n\nIntroduction: Use this section to introduce the topic, its relevance, and a summary of the whole paper. This section should address the question, “Why the target audience of the paper should care about your work?”\nRelated Work: This section discusses what has been done, in prior research, on your topic. While most of the cited papers should be from academic publications, it is not uncommon to see non-academic references in papers (e.g. stackoverflow). You might want to use academic search engines (e.g. SemancticScholar) to find relavant papers.\nExperiments This section should cover the steps you take to answer the question you posed in the Introduction. This includes data preparation, model and hyper-parameter selection, and evaluation steps.\nResults In this section you present the resutls of your experiments with appropriate tables and figures.\nConclusion In your final section, you would summarize what you did in the paper, acknowledge the limitations of your work (All papers have this; no work is perfect.), suggest future research directions to address your limitations.\n\n\n\nTimeline\n\nTeam Registration, due: October 1. Fill this form.\nStudy plan, due: October 28 (by the end of the day EDT).\nHanding in: Through ZoneCours\nProject meeting, October 31\nProject Presentation, due: December 1. Upload the PDF of your poster/slides to ZoneCours.\nIn-class Presentation, on December 5.\nFinal individual report, due: December 15, 2023 (by the end of the day EDT).\n\nHanding in: Through ZoneCours (per each team member).\n\n\n\n\n\nProject Presentation (10%)\nMake a poster that describes your project [You do not need to print your poster]. You can think of a poster as supporting material for your oral presentation (in that way it is similar to slides). It could also follow a similar structure: begin by motivating your work, then (quickly) highlight related work, talk in depth about your solution, then go into results (pictures and tables are good tools for that), finally conclude and perhaps mention one or two ideas for future work.\n\nProject Presentation (10%)\n\nClarity of presentation: 3%\nSlide or Poster quality: 2%\nCorrectness: 2%\nAnswers to questions: 3%\n\n\n\n\nFinal Exam (30%)\n\nDec 14, 2023\n9:00 am - 12:00 pm EDT\nLocation to be communicated\n\nPast exam - Fall 2018\n\n\nCapsule Quizzes (10%)\n\nIn-class quizzes of the capsules\nCapsule quizzes are counted for 10% of your final grade.\nTime to complete each quiz is 10 minutes.\n\nQuiz 1, Sep 20, 2023 11:30 AM to Sep 22, 2023 11:30 AM.\nQuiz 2, Sep 27, 2023 11:30 AM to Sep 29, 2023 11:30 AM.\nQuiz 3 - CNNs and RNNs, Nov 6, 2023 12:00 AM to Nov 8, 2023 12:00 AM.\nQuiz 4 - Unsupervised Learning, Nov 13, 2023 12:00 AM to Nov 15, 2023 12:00 AM.\nQuiz 5 - Parallel Computing, Nov 20, 2023 12:00 AM to Nov 22, 2023 12:00 AM.\nQuiz 6 - Reinforcement Learning, Nov 27, 2023 12:00 AM to Nov 29, 2023 12:00 AM."
  },
  {
    "objectID": "courses.html#in-class-tools",
    "href": "courses.html#in-class-tools",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "In-class Tools",
    "text": "In-class Tools\n\nCups pacing webapp"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\nUseful links\n\nImage Kernels\nVisualizing what ConvNets learn\nCNN excel example (fast.ai)\nConvolution to matrix (page 7)\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\nPapers: ResNet | Loss landscape | ULMFiT\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides - main\nClass slides - case study\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  },
  {
    "objectID": "sessions.html#session-materials",
    "href": "sessions.html#session-materials",
    "title": "MATH60629A - Machine Learning I (Fall 2023)",
    "section": "",
    "text": "Week 1 - Class introduction and math review [slides]\n\n\n\n\n\n\nCode tutorial\nRequired reading: Prologue to The Master Algorithm\nSuggested reading:\n\nChapter 1 of ESL\nTo explain or to predict\n\nMath review (if needed): have a look at the resources page.\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Machine learning fundamentals\n\n\n\n\n\n\nRequired readings: Chapter 5 of Deep Learning (the book).\n\nYou can skim 5.4 (except 5.4.4) to 5.10.\n\nCapsules: [slides]\n\nLearning Problem [14:40]\nTypes of Experiences [13:15]\nA first Supervised Model [8:03]\nModel Evaluation [15:26]\nRegularization [4:09]\nModel Validation [3:08]\nBias / Variance tradeoff [11:50]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Fundamentals_questions.ipynb AND 2) utilities.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 - Supervised learning algorithms\n\n\n\n\n\n\nReferences:\nSections 4.1-4.3, 4.5 of The Elements of Statistical Learning (available online),\nSections 3.5 and 4.2 of Machine Learning (K. Murphy)\nCapsules: [slides]\n\nNearest Neighbor [19:05]\nLinear Classification [15:26]\nIntroduction to Probabilistic Models (for Classification) [11:55]\nThe Naive Bayes Model [24:28]\nNaive Bayes Example [9:26]\n\nIn-class material:\n\nSummary\nExercises (colab)\nIf you do not want to use colab, here are the two files you need to download: 1a) Supervised_questions.ipynb AND 2) utils.py\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 - Python for scientific computations and machine learning [Practical Session]\n\n\n\n\n\n\nThe tutorial that you will follow is here (on colab).\nI encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.\n\n\n\n\n\n\n\n\n\n\nWeek 5 - Neural networks and deep learning\n\n\n\n\n\n\nRequired readings:\nSections 6.1–6.3 and 6.5 (stop at 6.5.4) of Deep Learning (the book).\nOther reference:\nChapter 11 of the Elements of Statistical Learning (available online).\nCapsules: [slides]\n\nFrom linear classification to neural networks [19:28]\nTraining neural networks [20:14]\nLearning representations [13:40]\nNeural networks hyperparameters [25:20]\nNeural networks takeaways [7:00]\n\nIn-class exercises:\n\nSummary\nExercises (colab)\n\nUseful links\n\nImage Kernels\nVisualizing what ConvNets learn\nCNN excel example (fast.ai)\nConvolution to matrix (page 7)\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]\n\n\n\n\n\n\nRequired readings:\nSections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. Sections 9, 9.1, 9.2, 9.3 (9.11 for fun).\nBoth from Deep Learning (the book).\nCapsules: [slides]\n\nModelling Sequential Data [8:42]\nPractical Overview of RNNs [29:32]\nRNNs for language modelling [15:13]\nOverview of CNNs [13:30]\nConvolutions and Pooling [26:00]\nConclusions and Practical remarks [9:17]\n\nIn-class material:\n\nSummary\nExercises RNNs (colab)\nExercises CNNs (colab)\nPapers: ResNet | Loss landscape | ULMFiT\n\n\nAttention and Transformers [Optional]\n\nCapsules: [Slides]\n\nTransformers: Application [5:57]\nHistory of Transformers and Attention Layers [6:01]\nWord and Position Encoding [11:46]\nSelf-Attention Layers [9:07]\nMulti Head Attention and Visual Transformers [4:59]\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 - Unsupervised learning\n\n\n\n\n\n\nRequired reading:\nSection 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.\nCapsules: [slides]\n\nIntroduction to unsupervised learning [8:17]\nK-means clustering [41:58] (there’s a natural break at 22:28)\nGMMs for clustering [17:52]\nBeyond Clustering [14:42]\n\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 - Reading week (no class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 - Project team meetings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Parallel computational paradigms for large-scale data processing\n\n\n\n\n\n\nCapsules: [Slides]\n\nIntro. to Distributed Computing for ML [19:35]\nMapReduce [17:41]\nSpark [17:37]\n\nSummary\n\n\n\n\n\n\n\n\n\n\nWeek 11 - Recommender systems\n\n\n\n\n\n\nRequired preparation for the case:\nCase Presentation and class execution (answer to Question 1 must be submitted by the 13 at the latest)\nClass slides - main\nClass slides - case study\n\n\n\n\n\n\n\n\n\n\nWeek 12 - Sequential decision making I\n\n\n\n\n\n\nCapsules: [slides]\n\nMotivating RL [8:22]\nPlanning with MDPs [12:16]\nMDP objective [14:16]\nAlgorithms for solving MDPs [17:51]\n\nNote: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not π(s)), the slides have been corrected (see slides 47 and 48)\nOptional: Demo of the policy iteration algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 - Sequential decision making II\n\n\n\n\n\n\nCapsules: [slides]\n\nIntroduction to RL [13:31]\nA first RL algorithm [17:13]\nRL Algorithms for Control [21:10]\n\nRequired reading: Sections 1 through 4 from this Survey\nOther reading: Chapters 1,3,4, and 6 from Reinforcement Learning: An Introduction\nOptional: Demo of the TD algorithm (from Andrej Karpathy)\nIn-class material:\n\nSummary\nExercises (colab)\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 - Class project presentations"
  }
]