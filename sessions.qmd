---
title: "MATH60629A - Machine Learning I (Fall 2023)"
resources:
    - "*.pdf"
format:
  html:
      toc: true
      toc-depth: 4

---

## Session Materials

::: {.callout-note appearance="minimal" collapse="false"}
### Week 1 - Class introduction and math review [[slides](w1/slides_intro.pdf)]


- **[Code tutorial](code/Session1.ipynb)**

- Required reading: [Prologue to The Master Algorithm](http://homes.cs.washington.edu/~pedrod/Prologue.pdf)

- Suggested reading: 
    + Chapter 1 of [ESL](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
    + [To explain or to predict](w1/to%20explain%20or%20to%20predict_shmueli.pdf)


- Math review (if needed): have a look at the [resources page](https://www.cs.toronto.edu/~lcharlin/courses/60629/math_resources.html).
:::


::: {.callout-note appearance="minimal" collapse="false"}

### Week 2 - Machine learning fundamentals


- Required readings: Chapter 5 of Deep Learning ([the book](http://www.deeplearningbook.org/contents/ml.html)). 
    + You can skim 5.4 (except 5.4.4) to 5.10.
- Capsules: [[slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_ml-fundamentals.pdf)]
    1. [Learning Problem [14:40]](https://youtu.be/XHjYLAooCQI)
    2. [Types of Experiences [13:15]](https://youtu.be/bUrw6MWiI7E)
    
    3. [A first Supervised Model [8:03]](https://www.youtube.com/watch?v=fu8IBbPREBg)
    4. [Model Evaluation [15:26]](https://youtu.be/jB69v09vrn8)
    5. [Regularization [4:09]](https://www.youtube.com/watch?v=SFzhFrWOTEI)
    6. [Model Validation [3:08]](https://www.youtube.com/watch?v=WoFGyFvyoeo)
    7. [Bias / Variance tradeoff [11:50]](https://www.youtube.com/watch?v=L5Hehy9s8SI)

- In-class material:

    - [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_ml-fundamentals_summary.pdf)
    - Exercises [(colab)](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week2-Fundamentals/Fundamentals_questions.ipynb)
    - If you do not want to use colab, here are the two files you need to download: 1a) [Fundamentals_questions.ipynb](https://raw.githubusercontent.com/lcharlin/80-629/master/week2-Fundamentals/Fundamentals_questions.ipynb) AND 2) [utilities.py](https://raw.githubusercontent.com/lcharlin/80-629/master/week2-Fundamentals/utilities.py)
:::

::: {.callout-note appearance="minimal" collapse="false"}

### Week 3 - Supervised learning algorithms
- References:

    Sections 4.1-4.3, 4.5 of The Elements of Statistical Learning ([available online](https://web.stanford.edu/~hastie/ElemStatLearn/)),

    Sections 3.5 and 4.2 of Machine Learning (K. Murphy)
- Capsules: [[slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_supervised.pdf)]

    1. [Nearest Neighbor [19:05]](https://youtu.be/wrpB9mxmhJc)
    
    2. [Linear Classification [15:26]](https://youtu.be/Kv8Ab2I_7CM)
    
    3. [Introduction to Probabilistic Models (for Classification) [11:55]](https://youtu.be/CnJTkeJpJLY)
    
    4. [The Naive Bayes Model [24:28]](https://youtu.be/8L2ZM20BdoA)
    
    5. [Naive Bayes Example [9:26]](https://youtu.be/xg8wZOr6zrY)
    
- In-class material:

    - [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_supervised_summary.pdf)
    - Exercises [(colab)](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week3-Supervised/Supervised_questions.ipynb) 
    - If you do not want to use colab, here are the two files you need to download: 1a) [Supervised_questions.ipynb](https://raw.githubusercontent.com/lcharlin/80-629/master/week3-Supervised/Supervised_questions.ipynb) AND 2) [utils.py](https://raw.githubusercontent.com/lcharlin/80-629/master/week3-Supervised/utils.py)
:::

::: {.callout-note appearance="minimal" collapse="false"}

### Week 4 - Python for scientific computations and machine learning [Practical Session]

- The tutorial that you will follow is [here](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week4-PracticalSession/Introduction_to_ML.ipynb) (on colab).
- I encourage you to start the tutorial ahead of time and to finish it during our 180 minutes together.

:::
::: {.callout-note appearance="minimal" collapse="false"}
### Week 5 - Neural networks and deep learning

* Required readings: 

    Sections 6.1--6.3 and 6.5 (stop at 6.5.4) of Deep Learning ([the book](http://www.deeplearningbook.org/contents/mlp.html)).

* Other reference: 

    Chapter 11 of the Elements of Statistical Learning ([available online](https://web.stanford.edu/~hastie/ElemStatLearn/)).

* Capsules: [[slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_nn.pdf)]

    1. [From linear classification to neural networks [19:28]](https://youtu.be/Bs6NA2gGz78)
    2. [Training neural networks [20:14]](https://youtu.be/c47a3YxIG7k)
    3. [Learning representations [13:40]](https://youtu.be/N_JU7egyGGA)
    4. [Neural networks hyperparameters [25:20]](https://youtu.be/5axp1O299qM)
    5. [Neural networks takeaways [7:00]](https://youtu.be/Nqs-C7wBVQo)

- In-class exercises:
    + [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_nn_summary.pdf)
    + Exercises [(colab)](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week5-NeuralNetworks/Neural_Networks_questions.ipynb)

:::

::: {.callout-note appearance="minimal" collapse="false"}

### Week 6 - Recurrent Neural networks and Convolutional neural networks [Optional: Transformers]
* Required readings: 
    
    Sections 10, 10.1, 10.2 (skim 10.2.2, skip 10.2.3), and 10.7. 
    Sections 9, 9.1, 9.2, 9.3 (9.11 for fun). 
    
    Both from Deep Learning ([the book](http://www.deeplearningbook.org/)).

* Capsules: [slides]
    1. [Modelling Sequential Data [8:42]](https://youtu.be/Ra_n9vJ89wM)
    2. [Practical Overview of RNNs [29:32]](https://youtu.be/2euWyjhO0GM)
    3. [RNNs for language modelling [15:13]](https://youtu.be/K-l8zCBuJbM)
    4. [Overview of CNNs [13:30]](https://youtu.be/EVZOThR2q1I)
    5. [Convolutions and Pooling [26:00]](https://youtu.be/L8tbxFKKoVw)
    6. [Conclusions and Practical remarks [9:17]](https://youtu.be/mA71uUtkcXw)
* In-class material:
    - [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_rnn-cnnSummary.pdf)
    - Exercises RNNs ([colab](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week6-RNNs%2BCNNs/RNNs_Questions.ipynb))
    - Exercises CNNs ([colab](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week6-RNNs%2BCNNs/CNNs_Questions.ipynb))

**Attention and Transformers [Optional]**

* Capsules: [[Slides](w6/slides_transformers.pdf)]
    1. [Transformers: Application [5:57]](https://youtu.be/K71RYvVwjSw) 
    2. [History of Transformers and Attention Layers [6:01]](https://youtu.be/ef07hT1Nm_0)
    3. [Word and Position Encoding [11:46]](https://youtu.be/hWyf21Dgujo)
    4. [Self-Attention Layers [9:07]](https://youtu.be/hR0V4KETjh0)
    5. [Multi Head Attention and Visual Transformers [4:59]](https://youtu.be/3ROYpukle44)
:::
::: {.callout-note appearance="minimal" collapse="false"}

### Week 7 - Unsupervised learning
* Required reading: 

    Section 14.3 (skip 14.3.5 and 14.3.12) of the Elements of Statistical Learning.

* Capsules: [[slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_unsupervised.pdf)]

    1. [Introduction to unsupervised learning [8:17]](https://youtu.be/z_PcTBDHvOs)
    2. [K-means clustering [41:58]](https://youtu.be/9EFWKAQ3TSs) (there's a natural break at 22:28)
    3. [GMMs for clustering [17:52]](https://youtu.be/OyK4tX2hjMc)
    4. [Beyond Clustering [14:42]](https://youtu.be/zVoi--FTiYk)
* In-class material:
    - [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slidesUnsupervised-summary.pdf)
    - Exercises ([colab](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week7-Unsupervised/Unsupervised_questions.ipynb))
:::

::: {.callout-note appearance="minimal" collapse="false"}
### Week 8 - Reading week (no class) 
:::

::: {.callout-note appearance="minimal" collapse="false"}
### Week 9 - Project team meetings
:::

::: {.callout-note appearance="minimal" collapse="false"}
### Week 10 - Parallel computational paradigms for large-scale data processing
- Capsules: [[Slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_largeScale22.pdf)]
    1. [Intro. to Distributed Computing for ML [19:35]](https://youtu.be/CtYOBS9pDvg)
    2. [MapReduce [17:41]](https://youtu.be/U3FLRYH3R5Q)
    3. [Spark [17:37]](https://www.youtube.com/watch?v=4gOdejqyHng)

- [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/summary-midterm.pdf)
:::
::: {.callout-note appearance="minimal" collapse="false"}
### Week 11 - Recommender systems
* **Required** preparation for the case: 

    [Case Presentation and class execution](https://www.cs.toronto.edu/~lcharlin/courses/60629/case_Decathlon-preparation.pdf) (answer to Question 1 must be submitted by the 13 at the latest)

* [Class slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/case_Decathlon-diapos.pdf)
:::
::: {.callout-note appearance="minimal" collapse="false"}
### Week 12 - Sequential decision making I
* Capsules: [[slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_rl.pdf)]
    1. [Motivating RL [8:22]](https://youtu.be/V2WrKWyiPoQ)
    2. [Planning with MDPs [12:16]](https://youtu.be/FwQQCSL5I_Y)
    3. [MDP objective [14:16]](https://youtu.be/3vX-J61A8NQ)
    4. [Algorithms for solving MDPs [17:51]](https://youtu.be/HBTyOjt4QBk)
    
    *Note*: In this capsule, there is a mistake in the second equation of the policy iteration algorithm (the transition should be given a and not Ï€(s)), the slides have been corrected (see slides 47 and 48)

* *Optional*: [Demo of the policy iteration algorithm](https://www.cs.toronto.edu/~lcharlin/courses/60629/reinforcejs/gridworld_dp.html) (from Andrej Karpathy)

* In-class material:
    - [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_rl-summary.pdf)
    - Exercises ([colab](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week12-MDPs/value_and_policy_iteration_question.ipynb))
:::
::: {.callout-note appearance="minimal" collapse="false"}
### Week 13 - Sequential decision making II
* Capsules: [[slides](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_rl2.pdf)]
    1. [Introduction to RL [13:31]](https://www.youtube.com/watch?v=VnZ4558bXys)
    2. [A first RL algorithm [17:13]](https://www.youtube.com/watch?v=EYeACgMxHVk)
    3. [RL Algorithms for Control [21:10]](https://www.youtube.com/watch?v=PeGnFc5S-f4)

* **Required** reading: 
        Sections 1 through 4 from [this Survey](https://www.jair.org/index.php/jair/article/download/10166/24110/)

* Other reading: 
        Chapters 1,3,4, and 6 from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)
* *Optional*: [Demo of the TD algorithm](https://www.cs.toronto.edu/~lcharlin/courses/60629/reinforcejs/gridworld_td.html) (from Andrej Karpathy)

* In-class material:
    - [Summary](https://www.cs.toronto.edu/~lcharlin/courses/60629/slides_rl2-summary.pdf)
    - Exercises ([colab](https://colab.research.google.com/github/lcharlin/80-629/blob/master/week13-RL/Monte_Carlo_Question.ipynb))
:::

::: {.callout-note appearance="minimal" collapse="false"}
### Week 14 - Class project presentations
:::